# Source: vertex-ai/docs/workbench/instances/add-environment.txt
Add a conda environment to a Vertex AI Workbench instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Add a conda environment
This page describes how to add a conda environment to your
Vertex AI Workbench instance.
Overview
When you add a conda environment to
your Vertex AI Workbench instance, it appears as a
kernel
in your instance's JupyterLab interface.
You might add a conda environment to your Vertex AI Workbench instance
to use kernels that aren't available in Vertex AI Workbench instances.
For example, you can add conda environments for R and Apache Beam. Or you
can add conda environments for specific older versions of the available
frameworks, such as TensorFlow, PyTorch, or Python.
Before you begin
If you haven't already,
create
a Vertex AI Workbench instance
.
Open JupyterLab
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Next to your Vertex AI Workbench instance's name,
click
Open JupyterLab
.
Your Vertex AI Workbench instance opens JupyterLab.
Add a conda environment
You can add a conda environment by entering commands in your instance's
JupyterLab terminal.
In JupyterLab,
select
File
>
New
>
Terminal
.
In the
Terminal
window, enter the following commands:
#
Creates
a
conda
environment.
conda
create
-n
CONDA_ENVIRONMENT_NAME
-y
conda
activate
CONDA_ENVIRONMENT_NAME
#
Install
packages
using
a
pip
local
to
the
conda
environment.
conda
install
pip
pip
install
PACKAGE
pip
install
ipykernel

#
Adds
the
conda
kernel.
DL_ANACONDA_ENV_HOME="
${
DL_ANACONDA_HOME
}
/envs/
CONDA_ENVIRONMENT_NAME
"
python
-m
ipykernel
install
--prefix
"
${
DL_ANACONDA_ENV_HOME
}
"
--name
CONDA_ENVIRONMENT_NAME
--display-name
KERNEL_DISPLAY_NAME
Replace the following:
CONDA_ENVIRONMENT_NAME
: your choice of
name for the environment
PACKAGE
: the package that you want to install
KERNEL_DISPLAY_NAME
: the display name for
the tile of the kernel in the JupyterLab interface
A default kernel can be created when installing to a given
conda environment. You can remove the default kernel with the
following command:
rm -rf "/opt/micromamba/envs/
CONDA_ENVIRONMENT_NAME
/share/jupyter/kernels/python3
To see your new kernel, do the following:
Refresh the page.
Select
File
>
New Launcher
.
The kernel is listed among the others in the
Launcher
window.
By default, conda might use pip packages in the system
pip
folder
(for example,
/usr/bin/pip
). Running
conda install pip
ensures that
the setup uses a pip local to the environment.
Example installation: R Essentials
The following example installs R Essentials in a conda environment named
r
.
conda create -n r
conda activate r
conda install -c r r-essentials
Example installation: pip package
The following example installs pip packages from a
requirements.txt
file.
conda
create
-n
myenv
conda
activate
myenv
conda
install
pip
pip
install
-r
requirements.txt
pip
install
ipykernel
DL_ANACONDA_ENV_HOME="
${
DL_ANACONDA_HOME
}
/envs/myenv"
python
-m
ipykernel
install
--prefix
"
${
DL_ANACONDA_ENV_HOME
}
"
--name
myenv
--display-name
myenv
Troubleshoot
To diagnose and resolve issues related to adding a conda environment,
see
Troubleshooting
Vertex AI Workbench
.
What's next
Learn more about
conda
.
To modify your conda environment, see
Manage your conda
environment
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/bigquery.txt
Query data in BigQuery from within JupyterLab  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Query data in BigQuery from within JupyterLab
This page shows you how to query data that is stored
in BigQuery from within the JupyterLab interface
of your Vertex AI Workbench instance.
Methods for querying BigQuery data in notebook (IPYNB) files
To query BigQuery data from within a JupyterLab notebook file,
you can use the
%%bigquery
magic command and
the BigQuery client library for Python.
Vertex AI Workbench instances also
include a BigQuery integration that lets you
browse and query data from within the JupyterLab interface.
This page describes how to use each of these methods.
Before you begin
If you haven't already,
create
a Vertex AI Workbench instance
.
Required roles
To ensure that your instance's service account has the necessary
      permissions to query data in BigQuery,
    
      ask your administrator to grant your instance's service account the
    
  
  Service Usage Consumer (
roles/serviceusage.serviceUsageConsumer
)
   IAM role on the project.
Important:
You must grant this role
      to your instance's service account,
not
to your user account. Failure to grant the role to the correct principal might result in permission errors.
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
Your administrator might also be able to give your instance's service account
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Open JupyterLab
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Next to your Vertex AI Workbench instance's name,
click
Open JupyterLab
.
Your Vertex AI Workbench instance opens JupyterLab.
Browse BigQuery resources
The BigQuery integration provides a pane for browsing
the BigQuery resources that you have access to.
In the JupyterLab navigation menu, click
BigQuery in Notebooks
.
The
BigQuery
pane lists available projects and datasets, where you
can perform tasks as follows:
To view a description of a dataset, double-click the dataset name.
To show a dataset's tables, views, and models, expand the dataset.
To open a summary description as a tab in JupyterLab, double-click a
table, view, or model.
Note:
On the summary description for a table, click the
Preview
tab to preview a table's data. The following image shows a preview of the
international_top_terms
table
found in the
google_trends
dataset in the
bigquery-public-data
project:
Query data by using the %%bigquery magic command
In this section, you write SQL directly in notebook cells and read data from
BigQuery into the Python notebook.
Magic commands that use a single or double percentage character (
%
or
%%
)
let you use minimal syntax to interact with BigQuery within the
notebook. The BigQuery client library for Python is automatically
installed in a Vertex AI Workbench instance. Behind the scenes, the
%%bigquery
magic
command uses the BigQuery client library for Python to run the
given query, convert the results to a pandas DataFrame, optionally save the
results to a variable, and then display the results.
Note
: As of version 1.26.0 of the
google-cloud-bigquery
Python package,
the
BigQuery Storage API
is used by default to download results from the
%%bigquery
magics.
To open a notebook file, select
File
>
New
>
Notebook
.
In the
Select Kernel
dialog, select
Python 3
, and then click
Select
.
Your new IPYNB file opens.
To get the number of regions by country in the
international_top_terms
dataset, enter the following statement:
%%
bigquery
SELECT
country_code
,
country_name
,
COUNT
(
DISTINCT
region_code
)
AS
num_regions
FROM
`
bigquery
-
public
-
data
.
google_trends
.
international_top_terms
`
WHERE
refresh_date
=
DATE_SUB
(
CURRENT_DATE
,
INTERVAL
1
DAY
)
GROUP
BY
country_code
,
country_name
ORDER
BY
num_regions
DESC
;
Click
play_circle_filled
Run cell
.
The output is similar to the following:
Query complete after 0.07s: 100%|██████████| 4/4 [00:00<00:00, 1440.60query/s]
Downloading: 100%|██████████| 41/41 [00:02<00:00, 20.21rows/s]
country_code      country_name    num_regions
0   TR  Turkey         81
1   TH  Thailand       77
2   VN  Vietnam        63
3   JP  Japan          47
4   RO  Romania        42
5   NG  Nigeria        37
6   IN  India          36
7   ID  Indonesia      34
8   CO  Colombia       33
9   MX  Mexico         32
10  BR  Brazil         27
11  EG  Egypt          27
12  UA  Ukraine        27
13  CH  Switzerland    26
14  AR  Argentina      24
15  FR  France         22
16  SE  Sweden         21
17  HU  Hungary        20
18  IT  Italy          20
19  PT  Portugal       20
20  NO  Norway         19
21  FI  Finland        18
22  NZ  New Zealand    17
23  PH  Philippines    17
...
Note:
Your results might differ from what is above as the
google_trends
dataset being queried is refreshed with new data on an ongoing basis.
In the next cell (below the output from the previous cell), enter the
following command to run the same query, but this time save the results to
a new pandas DataFrame that's named
regions_by_country
. You provide that
name by using an argument with the
%%bigquery
magic command.
%%
bigquery
regions_by_country
SELECT
country_code
,
country_name
,
COUNT
(
DISTINCT
region_code
)
AS
num_regions
FROM
`
bigquery
-
public
-
data
.
google_trends
.
international_top_terms
`
WHERE
refresh_date
=
DATE_SUB
(
CURRENT_DATE
,
INTERVAL
1
DAY
)
GROUP
BY
country_code
,
country_name
ORDER
BY
num_regions
DESC
;
Note:
For more information about available arguments for the
%%bigquery
command, see the
client library magics documentation
.
Click
play_circle_filled
Run cell
.
In the next cell, enter the following command to look at the first few
rows of the query results that you just read in:
regions_by_country
.
head
()
Click
play_circle_filled
Run cell
.
The pandas DataFrame
regions_by_country
is ready to plot.
Query data by using the BigQuery client library directly
In this section, you use the BigQuery client library for Python
directly to read data into the Python notebook.
The client library gives you more control over your queries and lets you use
more complex configurations for queries and jobs. The library's integrations
with pandas enable you to combine the power of declarative SQL with imperative
code (Python) to help you analyze, visualize, and transform your data.
Note:
You can use a number of Python data analysis, data wrangling, and
visualization libraries, such as
numpy
,
pandas
,
matplotlib
, and many
others. Several of these libraries are built on top of a DataFrame object.
In the next cell, enter the following Python code to import the
BigQuery client library for Python and initialize a client:
from
google.cloud
import
bigquery
client
=
bigquery
.
Client
()
The BigQuery client is used to send and receive messages
from the BigQuery API.
Click
play_circle_filled
Run cell
.
In the next cell, enter the following code to retrieve the percentage of
daily top terms in the US
top_terms
that overlap across time by number of days apart. The idea here is to look
at each day's top terms and see what percentage of them overlap with the
top terms from the day before, 2 days prior, 3 days prior, and so on (for
all pairs of dates over about a month span).
sql
=
"""
WITH
TopTermsByDate AS (
SELECT DISTINCT refresh_date AS date, term
FROM `bigquery-public-data.google_trends.top_terms`
),
DistinctDates AS (
SELECT DISTINCT date
FROM TopTermsByDate
)
SELECT
DATE_DIFF(Dates2.date, Date1Terms.date, DAY)
AS days_apart,
COUNT(DISTINCT (Dates2.date || Date1Terms.date))
AS num_date_pairs,
COUNT(Date1Terms.term) AS num_date1_terms,
SUM(IF(Date2Terms.term IS NOT NULL, 1, 0))
AS overlap_terms,
SAFE_DIVIDE(
SUM(IF(Date2Terms.term IS NOT NULL, 1, 0)),
COUNT(Date1Terms.term)
) AS pct_overlap_terms
FROM
TopTermsByDate AS Date1Terms
CROSS JOIN
DistinctDates AS Dates2
LEFT JOIN
TopTermsByDate AS Date2Terms
ON
Dates2.date = Date2Terms.date
AND Date1Terms.term = Date2Terms.term
WHERE
Date1Terms.date <= Dates2.date
GROUP BY
days_apart
ORDER BY
days_apart;
"""
pct_overlap_terms_by_days_apart
=
client
.
query
(
sql
).
to_dataframe
()
pct_overlap_terms_by_days_apart
.
head
()
The SQL being used is encapsulated in a Python string and then passed to the
query()
method
to run a query. The
to_dataframe
method
waits for the query to finish and downloads the results to a pandas
DataFrame by using the BigQuery Storage API.
Click
play_circle_filled
Run
cell
.
The first few rows of query results appear below the code cell.
days_apart   num_date_pairs  num_date1_terms overlap_terms   pct_overlap_terms
 0          0             32               800            800            1.000000
 1          1             31               775            203            0.261935
 2          2             30               750             73            0.097333
 3          3             29               725             31            0.042759
 4          4             28               700             23            0.032857
Note:
Your results might differ from what is above as the
google_trends
dataset being queried is refreshed with new data on an ongoing basis.
For more information about using BigQuery client libraries, see
the quickstart
Using client libraries
.
Query data by using the BigQuery integration in Vertex AI Workbench
The BigQuery integration provides two additional methods
for querying data. These methods are different from using
the
%%bigquery
magic command.
The
In-cell query editor
is a cell type that you can use within
your notebook files.
The
Stand-alone query editor
opens as a separate tab in JupyterLab.
In-cell
To use the in-cell query editor to query data
in a BigQuery table, complete the following steps:
In JupyterLab, open a notebook (IPYNB) file or
create a new
one
.
To create an in-cell query editor, click the cell,
and then to the right of the cell, click the
BigQuery Integration
button.
Or in a markdown cell, enter
#@BigQuery
.
The BigQuery integration converts the cell into
an in-cell query editor.
On a new line below
#@BigQuery
, write your query using
the
supported statements and SQL dialects of
BigQuery
.
If errors are detected in your query, an error message appears
in the top right corner of the query editor. If the query is valid,
the estimated number of bytes to be processed appears.
Click
Submit Query
. Your query results appear.
By default, query results are paginated at 100 rows per page
and limited to 1,000 rows total, but you can change
these settings at the bottom of the results table. In the
query editor, keep the query limited to only the data you need
to verify your query. You'll run this query again in a notebook cell,
where you can adjust the limit to retrieve the
full results set if you want.
Note:
Query text and results persist after closing and reopening
the notebook file.
You can click
Query and load as DataFrame
to automatically
add a new cell that contains a code segment
that imports the BigQuery client library for Python,
runs your query in a notebook cell, and stores the results
in a pandas dataframe named
df
.
Stand-alone
To use the stand-alone query editor to query data
in a BigQuery table, complete the following steps:
In JupyterLab, in the
BigQuery in Notebooks
pane, right-click
a table, and select
Query table
,
or double-click a table to open a description
in a separate tab, and then click the
Query table
link.
Write your query using the
supported statements and SQL dialects of
BigQuery
.
If errors are detected in your query, an error message appears
in the top right corner of the query editor. If the query is valid,
the estimated number of bytes to be processed appears.
Click
Submit Query
. Your query results appear.
By default, query results are paginated at 100 rows per page
and limited to 1,000 rows total, but you can change
these settings at the bottom of the results table. In the
query editor, keep the query limited to only the data you need
to verify your query. You'll run this query again in a notebook cell,
where you can adjust the limit to retrieve the
full results set if you want.
You can click
Copy code for DataFrame
to copy a code segment
that imports the BigQuery client library for Python,
runs your query in a notebook cell, and stores the results
in a pandas dataframe named
df
. Paste this code into a notebook cell
where you want to run it.
View query history and reuse queries
To view your query history as a tab in JupyterLab, perform the following steps:
In the JupyterLab navigation menu, click
BigQuery in Notebooks
to open the
BigQuery
pane.
In the
BigQuery
pane, scroll down and click
Query history
.
A list of your queries opens in a new tab, where you can perform tasks such
as the following:
To view the details of a query such as its Job ID, when the query was
run, and how long it took, click the query.
To revise the query, run it again, or copy it into your notebook for
future use, click
Open query in editor
.
Note:
If you run queries after opening this tab, click
refresh
Refresh
to show
the most recent queries.
What's next
To see examples of how to visualize the data from
your BigQuery tables, see
Explore and visualize data
in BigQuery from
within JupyterLab
.
To learn more about writing queries for BigQuery, see
Running interactive and batch query jobs
.
Learn how to
control access to
BigQuery datasets
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/change-machine-type.txt
Change machine type and configure GPUs of a Vertex AI Workbench instance  |  Google Cloud Documentation
Change machine type and configure GPUs of a Vertex AI Workbench instance
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Change machine type and configure GPUs of a Vertex AI Workbench instance
This page shows you how to change the machine type and
GPU configuration of a Vertex AI Workbench instance.
Overview
The machine type determines some specifications of
your Vertex AI Workbench instance,
such as the amount of memory, virtual cores, and persistent disk limits.
Changing the machine type can improve performance or help avoid errors
caused by high memory utilization.
GPUs provide hardware acceleration that can improve the performance
of your notebook code. You may want to add or increase the number of GPUs
for greater performance, or, to reduce the cost of running
your Vertex AI Workbench instance,
you may want to remove GPUs.
Before you change a machine type or GPU configuration
Consider the following before you make any changes to
your Vertex AI Workbench instance's machine types
or GPU configuration.
Billing implications
Each machine type and GPU configuration is billed at a different rate.
Make sure you understand
the
pricing
implications of making a change.
For example, an
e2-highmem-2
machine type costs more than an
e2-standard-2
machine type.
Changing between G2 and non-G2 machine types not supported
Changing between a
G2 machine type
and
a non-G2 machine type isn't supported. If your instance has a G2 machine type,
you can only change the machine type to a different G2 machine type.
If your instance has a non-G2 machine type, you can change the machine type
to a different non-G2 machine type, such as an E2 or N2 machine type.
Moving to a smaller machine type
If you move from a machine type with more resources to a machine type with fewer
resources, such as moving from an
e2-standard-8
machine type to an
e2-standard-2
, you might run into hardware resource issues or performance
limitations because smaller machine types are less powerful than larger machine
types.
Make a backup
It's good practice to make regular backups of
your instance's persistent disk data using snapshots. Consider
taking a
snapshot
of your
persistent disk data before you change the machine type. If you want to make
sure the new machine type is able to support the data on the existing instance,
you can take a persistent disk snapshot and use it to start a second instance
with the new machine type to confirm that the instance starts up successfully.
Change the machine type and configure GPUs
Note:
To change the machine type or GPUs for
    a Vertex AI Workbench instance,
    you must
shut down the Vertex AI Workbench
    instance
.
To change the machine type or configure the GPUs on
a Vertex AI Workbench instance, complete the following steps.
In the Google Cloud console, go to the
Instances
page.
Go to Instances
In the
Notebook name
column, click the name of the
instance that you want to modify.
The
Notebook details
page opens.
Click the
Hardware
tab.
In the
Modify hardware configuration
section,
select the
Machine type
that you want to use.
If there are GPUs available for your instance's combination of
zone, environment, and machine type, you can configure the GPUs.
In the
GPUs
section, select the
GPU type
and
Number of GPUs
that you want to use.
Learn more about GPU regions and zone
availability
.
If you haven't already installed the required GPU drivers on your instance,
select
Install NVIDIA GPU driver automatically for me
to install the drivers automatically on next startup.
After Vertex AI Workbench has finished updating the machine type
and GPU configuration, you can start
your Vertex AI Workbench instance.
What's next
Learn more about the available
GPU platforms
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/cloud-storage.txt
Access Cloud Storage buckets and files in JupyterLab in a Vertex AI Workbench instance.  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Access Cloud Storage buckets and files in JupyterLab
This page shows you how to mount a Cloud Storage bucket to the
JupyterLab interface of your Vertex AI Workbench instance so that you can
browse files that are stored in Cloud Storage. You can also open
and edit files that are compatible with JupyterLab, such as text files and
notebook (IPYNB) files.
Overview
Vertex AI Workbench instances include a Cloud Storage integration
that lets you mount a Cloud Storage bucket. This means you can
browse the contents of the bucket and work with compatible files from within
the JupyterLab interface.
You can access any of the Cloud Storage buckets and files that
your instance has access to within the same project as
your Vertex AI Workbench instance.
Note:
Your Vertex AI Workbench instance's access to
Cloud Storage is determined by the single user or service account
that you used to grant access to your instance. For example,
if you granted a specific service account access to your instance,
you must also grant that service account access to the
Cloud Storage buckets that you want to use in JupyterLab.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Required roles
To get the permissions that
      you need to mount a Cloud Storage bucket to a Vertex AI Workbench instance,
    
      ask your administrator to grant you the
    following IAM roles:
Notebooks Runner
(
roles/notebooks.runner
)
    
              on the project
Storage Object User
(
roles/storage.objectUser
)
    
              on the Vertex AI Workbench instance's service account
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Required permission for enabling shared storage mounting
To enable shared storage mounting in your Vertex AI Workbench instance,
ask your administrator to grant your Vertex AI Workbench instance's
service account the
storage.buckets.list
permission on the project.
The
storage.buckets.list
permission is required for the
Mount shared storage
button to appear in the JupyterLab interface of your
Vertex AI Workbench instance.
Create a bucket and a Vertex AI Workbench instance
You must have access to at least one Cloud Storage bucket in the
same project as your Vertex AI Workbench instance.
If you need to create a Cloud Storage bucket,
    see
Create a bucket
.
If you haven't already,
create a Vertex AI Workbench instance
in the same project as your Cloud Storage bucket.
Open JupyterLab
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Next to your Vertex AI Workbench instance's name,
      click
Open JupyterLab
.
Your Vertex AI Workbench instance opens JupyterLab.
Mount the Cloud Storage bucket
To mount and then access a Cloud Storage bucket, do the following:
In JupyterLab, make sure the
folder
File Browser
tab
      is selected.
In the left sidebar, click the
Mount
      shared storage
button. If you don't see the button, drag the right side
      of the sidebar to expand the sidebar until you see the button.
In the
Bucket name
field, enter the Cloud Storage
      bucket name that you want to mount.
Click
Mount
.
Your Cloud Storage bucket appears as a folder in the
File browser
tab of the left sidebar. Double-click the folder
      to open it and browse the contents.
Troubleshoot
To find methods for diagnosing and resolving issues with mounting a
Cloud Storage bucket to your instance, see
Troubleshooting
Vertex AI Workbench
.
What's next
Learn more about
Cloud Storage
.
Learn how to
query data in BigQuery
from within JupyterLab
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/cmek.txt
Customer-managed encryption keys  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Customer-managed encryption keys (CMEK)
By default, Vertex AI Workbench encrypts customer content at
    rest. Vertex AI Workbench handles encryption for you without any
  additional actions on your part. This option is called
Google default encryption
.
If you want to control your encryption keys, then you can use customer-managed encryption keys
  (CMEKs) in
Cloud KMS
with CMEK-integrated services including
  Vertex AI Workbench. Using Cloud KMS keys gives you control over their protection
  level, location, rotation schedule, usage and access permissions, and cryptographic boundaries.
  
    Using Cloud KMS also lets
you
track key usage
, view audit logs, and
control key lifecycles.
    
  
  Instead of Google owning and managing the symmetric
key encryption keys (KEKs)
that protect your data, you control and
  manage these keys in Cloud KMS.
After you set up your resources with CMEKs, the experience of accessing your
  Vertex AI Workbench resources is similar to using Google default encryption.
  For more information about your encryption
  options, see
Customer-managed encryption keys (CMEK)
.
This page describes some specific benefits and limitations of using CMEK with
Vertex AI Workbench and shows
how to configure a new Vertex AI Workbench instance
to use CMEK.
For more information about how to use CMEK for Vertex AI, see the
Vertex AI CMEK page
.
Benefits of CMEK
In general, CMEK is most useful if you need full control over the keys used to
encrypt your data. With CMEK, you can manage your keys within
Cloud Key Management Service. For example, you can rotate or disable a key or you can set
up a rotation schedule by using the Cloud KMS API.
When you run a Vertex AI Workbench instance,
your instance runs
on a virtual machine (VM) managed by Vertex AI Workbench.
When you enable
CMEK for a Vertex AI Workbench instance,
Vertex AI Workbench uses the key that you designate,
rather than a key managed by Google, to encrypt
data on the boot disks of the VM.
The CMEK key
doesn't
encrypt metadata, like the instance's name and region,
associated with your Vertex AI Workbench instance.
Metadata associated with
Vertex AI Workbench instances is always
encrypted using Google's default encryption mechanism.
Limitations of CMEK
To decrease latency and to prevent cases where resources depend on
services that are spread across multiple failure domains, Google recommends
that you protect regional
Vertex AI Workbench instances with keys in the same location.
You can encrypt regional Vertex AI Workbench instances
by using keys in the same location or in the global location. For example,
you can encrypt data in a disk in zone
us-west1-a
by using
a key in
us-west1
or
global
.
You can encrypt global instances by using keys in any location.
Configuring CMEK for
Vertex AI Workbench
doesn't
automatically configure CMEK
for other Google Cloud products that you use. To use CMEK to encrypt
data in other Google Cloud products, you must complete additional
configuration.
Configure CMEK for your Vertex AI Workbench instance
The following sections describe how to create a
key ring and key in Cloud Key Management Service,
grant the service account encrypter and decrypter permissions for your key,
and create a Vertex AI Workbench instance that uses CMEK.
Before you begin
We recommend using a setup that supports a
separation of
duties
. To configure CMEK
for Vertex AI Workbench, you can use
two separate Google Cloud projects:
A Cloud KMS project: a project for managing your encryption key
A Vertex AI Workbench project: a project for accessing
Vertex AI Workbench instances and interacting with any
other Google Cloud products that you need for your use case
Alternatively, you can use a single Google Cloud project. To do so,
use the same project for all of the following tasks.
Set up the Cloud KMS project
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Cloud KMS API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Cloud KMS API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Set up the Vertex AI Workbench project
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Set up the Google Cloud CLI
The gcloud CLI is required for some steps on this page and optional
for others.
Install
the Google Cloud CLI.
        
          After installation,
initialize
the Google Cloud CLI by running the following command:
gcloud
init
If you're using an external identity provider (IdP), you must first
sign in to the gcloud CLI with your federated identity
.
Note:
You can run the gcloud CLI in
      the Google Cloud console without installing the Google Cloud CLI. To run the
      gcloud CLI in the Google Cloud console,
use
        Cloud Shell
.
Create a key ring and key
When you create a key ring and key, keep the following requirements in mind:
When you choose your key ring's location, use either
global
or the location where your Vertex AI Workbench instance
will be.
Make sure to create your key ring and key in your Cloud KMS project.
To create a key ring and a key, see
Create symmetric encryption keys
.
Grant Vertex AI Workbench permissions
To use CMEK for your Vertex AI Workbench instance,
you must grant your Vertex AI Workbench instance permission to
encrypt and decrypt data using your key. You grant this permission to
your project's
service agent
and the
Compute Engine service account.
To find the specific accounts for your Vertex AI Workbench project,
use the Google Cloud console.
In the Google Cloud console, go to the
IAM
page
Go to IAM
Select
Include Google-provided role grants
.
Find the members that match the following email address formats. Make
note of the email addresses, and use them in the following steps.
Your project's service agent's email address looks like the following:
service
-
NOTEBOOKS_PROJECT_NUMBER
@gcp
-
sa
-
notebooks
.
iam.gserviceaccount.com
The Compute Engine service account's email address looks like
the following:
service
-
NOTEBOOKS_PROJECT_NUMBER
@compute
-
system
.
iam.gserviceaccount.com
Replace
NOTEBOOKS_PROJECT_NUMBER
with the
project
number
for your Vertex AI Workbench project.
To grant these accounts permission to encrypt and decrypt data using
your key, you can use the Google Cloud console or the Google Cloud CLI.
Console
In the Google Cloud console, go to the
Key management
page.
Go to Key management
Select your Cloud KMS project.
Click the name of the key ring that you created in
Create a key ring and key
. The
Key ring details
page opens.
Select the checkbox for the key that you created in
Create a key ring and key
.
If an info panel labeled with the name of your key isn't already
open, click
Show info panel
.
In the info panel, click
person_add
Add member
.
The
Add members to "
KEY_NAME
"
dialog opens. In this
dialog, do the following:
In the
New members
field, enter your project's service agent's
email address:
service-
NOTEBOOKS_PROJECT_NUMBER
@gcp-sa-notebooks.
iam.gserviceaccount.com
In the
Select a role
list, click
Cloud KMS
and then select the
Cloud KMS CryptoKey Encrypter/Decrypter
role.
Click
Save
.
Repeat these steps for the Compute Engine service agent:
service-
NOTEBOOKS_PROJECT_NUMBER
@compute-system.
iam.gserviceaccount.com
gcloud
To grant your project's service agent permission to encrypt and
decrypt data using your key, run the following command:
gcloud
kms
keys
add-iam-policy-binding
KEY_NAME
\
--keyring
=
KEY_RING_NAME
\
--location
=
REGION
\
--project
=
KMS_PROJECT_ID
\
--member
=
serviceAccount:service-
NOTEBOOKS_PROJECT_NUMBER
@gcp-sa-notebooks.iam.gserviceaccount.com
\
--role
=
roles/cloudkms.cryptoKeyEncrypterDecrypter
Replace the following:
KEY_NAME
: the name of the key that you
created in
Create a key ring and key
KEY_RING_NAME
: the key ring that you
created in
Create a key ring and key
REGION
: the region where you created your
key ring
KMS_PROJECT_ID
: the ID of your
Cloud KMS project
NOTEBOOKS_PROJECT_NUMBER
: the project number
of your Vertex AI Workbench project, which you noted in
the preceding section as part of a service account email address.
To grant the Compute Engine service account permission
to encrypt and decrypt data using your key, run the following command:
gcloud
kms
keys
add-iam-policy-binding
KEY_NAME
\
--keyring
=
KEY_RING_NAME
\
--location
=
REGION
\
--project
=
KMS_PROJECT_ID
\
--member
=
serviceAccount:service-
NOTEBOOKS_PROJECT_NUMBER
@compute-system.iam.gserviceaccount.com
\
--role
=
roles/cloudkms.cryptoKeyEncrypterDecrypter
Create a Vertex AI Workbench instance with CMEK
After you have granted your Vertex AI Workbench instance
permission
to encrypt and decrypt data
using your key, you can create a Vertex AI Workbench instance
that encrypts data using this key.
The following example shows how to encrypt and decrypt data
using your key by using the Google Cloud console.
To create a Vertex AI Workbench instance with a
customer-managed encryption key:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
In the
New instance
dialog, click
Advanced options
.
In the
Create instance
dialog,
in the
Details
section,
provide the following information for your new instance:
Name
: a name for your new instance
Region
: the region that your key and key ring are in
Zone
: a zone within the region that you selected
In the
Disks
section, in
Encryption
,
select
Customer-managed encryption key (CMEK)
.
Click
Select a customer-managed key
.
If the customer-managed key that you want to use is in the list,
select it.
If the customer-managed key that you want to use isn't in the list,
enter the resource ID for your customer-managed key. The resource
ID for your customer-managed key looks like the following:
projects/
NOTEBOOKS_PROJECT_NUMBER
/locations/global/keyRings/
KEY_RING_NAME
/cryptoKeys/
KEY_NAME
Replace the following:
NOTEBOOKS_PROJECT_NUMBER
: the ID of your
Vertex AI Workbench project
KEY_RING_NAME
: the key ring that you created
in
Create a key ring and key
KEY_NAME
: the name of the key that you
created in
Create a key ring and key
Complete the rest of the instance creation dialog,
and then click
Create
.
What's next
Learn more about
CMEK on Google Cloud
.
Learn
how to use CMEK with other Google Cloud
products
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/create-confidential-computing.txt
Create an instance with Confidential Computing  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create an instance with Confidential Computing
Preview
This feature is
        
        subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the
Service Specific
        Terms
.
        
        Pre-GA features are available "as is" and might have limited support.
      
      For more information, see the
launch stage descriptions
.
This document describes how to create a Vertex AI Workbench instance with
Confidential Computing enabled.
Overview
Confidential Computing is the protection of data in-use with
hardware-based Trusted Execution Environment (TEE). TEEs are secure and
isolated environments that prevent unauthorized access or modification
of applications and data while they are in use. This security standard is
defined by the
Confidential Computing
Consortium
.
When you create a Vertex AI Workbench instance with
Confidential Computing enabled, your new Vertex AI Workbench instance
is a Confidential VM instance. To learn more about
Confidential VM instances, see the
Confidential VM
overview
.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Compute Engine and Notebooks APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Compute Engine and Notebooks APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Required roles
To get the permissions that
      you need to create a Vertex AI Workbench instance,
    
      ask your administrator to grant you the
Notebooks Runner
(
roles/notebooks.runner
)
     IAM role on the project.
  

  

  
  
  For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create an instance
You can create an instance with Confidential Computing enabled
by using the gcloud CLI or the REST API:
gcloud
To create a Vertex AI Workbench instance with
Confidential Computing enabled, use the
gcloud workbench
instances create
command and set
--confidential-compute-type
to
SEV
.
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: the name of your Vertex AI Workbench instance;
    must start with a letter followed by up to 62 lowercase letters,
    numbers, or hyphens (-), and cannot end with a hyphen
PROJECT_ID
: your project ID
LOCATION
: the zone where you want your instance to be located
MACHINE_TYPE
: the
machine type
of your instance's VM, for example:
n2d-standard-2
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
\
--project
=
PROJECT_ID
\
--location
=
LOCATION
\
--machine-type
=
MACHINE_TYPE
\
--confidential-compute-type
=
SEV
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
`
--project
=
PROJECT_ID
`
--location
=
LOCATION
`
--machine-type
=
MACHINE_TYPE
`
--confidential-compute-type
=
SEV
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
^
--project
=
PROJECT_ID
^
--location
=
LOCATION
^
--machine-type
=
MACHINE_TYPE
^
--confidential-compute-type
=
SEV
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link in the Google Cloud console.
REST
To create a Vertex AI Workbench instance with
Confidential Computing enabled, use the
projects.locations.instances.create
method and include a
confidentialInstanceConfig
in your
GceSetup
.
Before using any of the request data,
  make the following replacements:
PROJECT_ID
: your project ID
LOCATION
: the zone where you want your instance to be located
MACHINE_TYPE
: the
machine type
of your instance's VM, for example:
n2d-standard-2
HTTP method and URL:
POST https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances
Request JSON body:
{
  "gce_setup": {
    "machine_type": "
MACHINE_TYPE
",
    "confidentialInstanceConfig": {
      "confidentialInstanceType": SEV
    }
  }
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances" | Select-Object -Expand Content
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link in the Google Cloud console.
Confirm whether an instance has Confidential Computing enabled
To confirm whether a Vertex AI Workbench instance has
Confidential Computing enabled, do the following:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
In the
Instance name
column, click the name of the instance that
you want to check.
The
Instance details
page opens.
Next to
VM details
, click
View in Compute Engine
.
On the Compute Engine details page, the value for
Confidential VM service
shows either
Enabled
or
Disabled
.
Limitations
When you create or use a Vertex AI Workbench instance with
Confidential Computing enabled, the following limitations apply:
Only N2D machine types are supported. See
N2D machine
types
.
Confidential Computing can't be enabled or turned off after you
create the Vertex AI Workbench instance.
Billing
While this feature is in
Preview
,
charges for using Vertex AI Workbench instances with
Confidential Computing are the same as using instances
without Confidential Computing. See
Pricing
.
What's next
To use a notebook to help you get started using Vertex AI and
  other Google Cloud services, see
Vertex AI
  notebook tutorials
.
To check on the health status of your Vertex AI Workbench instance,
    see
Monitor health status
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/create-console-quickstart.txt
Quickstart: Create a Vertex AI Workbench instance by using the Google Cloud console  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create an instance by using the Google Cloud console
Learn how to create a Vertex AI Workbench instance
and open JupyterLab by using the Google Cloud console.
This page also describes how to stop, start, reset, or delete
an instance.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Note:
The
Notebooks API
lets you
      manage Vertex AI Workbench resources. For managing
      Vertex AI resources, see the
Vertex AI API
.
Create an instance
In the Google Cloud console,
go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
For
Name
, enter
my-instance
.
Click
Create
.
When you finish the tasks that are described in this document, you can avoid
   continued billing by deleting the resources that you created. For more information, see
Clean up
.
Open JupyterLab
After you create your instance, Vertex AI Workbench automatically starts
the instance. When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link.
Next to your instance's name,
click
Open JupyterLab
.
Your Vertex AI Workbench instance opens JupyterLab.
Open a new notebook file
In JupyterLab,
select
File
>
New
>
Notebook
.
In the
Select kernel
dialog, select
Python 3
,
and then click
Select
.
Your new notebook file opens.
Stop your instance
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Select the instance that you want to stop.
Click
square
Stop
.
Start your instance
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Select the instance that you want to start.
Click
arrow_right
Start
.
Reset your instance
Resetting an instance forcibly wipes the memory contents of your instance and
resets the instance to its initial state. To learn more about how resetting an
instance works, see
Reset operation
.
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Select the instance that you want to reset.
Click
Reset
, and then click
Reset
to confirm.
Clean up
To avoid incurring charges to your Google Cloud account for
          the resources used on this page, follow these steps.
If you created a new project to learn about Vertex AI Workbench instances
and you no longer need the project, then
delete the project
.
If you used an existing Google Cloud project, then delete the resources
you created to avoid incurring charges to your account:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Select the row containing the instance that you want to delete.
Click
delete
Delete
.
(Depending on the size of your window,
the
Delete
button might be in
the
more_vert
options menu.)
To confirm, click
Confirm
.
What's next
Read the
Introduction to
    Vertex AI Workbench instances
.
To use a notebook to help you get started using Vertex AI and
  other Google Cloud services, see
Vertex AI
  notebook tutorials
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/create-custom-container.txt
Create a Vertex AI Workbench instance using a custom container  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create an instance using a custom container
This page describes how to create a Vertex AI Workbench instance based on
a custom container.
Overview
Vertex AI Workbench instances support using a custom container derived
from one of the Google-provided base containers. You can modify these
base containers to make a custom container image and use these
custom containers to create a Vertex AI Workbench instance.
The base containers are configured with a
Container-Optimized
OS
in the host
virtual machine (VM). The host image is built from the
cos-stable
image family
.
Limitations
Consider the following limitations when planning your project:
The custom container must be derived from a
Google-provided base container
.
Using a container that isn't derived from a base container increases the
risk of compatibility issues and limits our ability to support your
usage of Vertex AI Workbench instances.
Use of more than one container with a Vertex AI Workbench instance
isn't supported.
Supported metadata for custom containers from
user-managed notebooks and managed notebooks can have
different behavior when used with Vertex AI Workbench instances.
The VM hosting the custom container is running off of a
Container-Optimized OS
,
which restricts how you can interact with the host machine. For example,
Container-Optimized OS doesn't include a package manager. This
means that packages acting on the host must be performed on a container
with mounts. This affects the post-startup scripts that are migrated from
managed notebooks instances and
user-managed notebooks instances, where the
host machine contains significantly more tooling than
Container-Optimized OS.
Vertex AI Workbench instances uses
nerdctl
(a containerd CLI) for running the custom container. This is required
for compatibility with the Image streaming service. Any container
parameters that are added using a metadata value need to
adhere to what is supported by
nerdctl
.
Vertex AI Workbench instances are configured to pull either
from Artifact Registry or a public container repository. To configure
an instance to pull from a private repository, you must manually
configure the credentials used by the containerd.
Base containers
Standard base container
The standard base container supports all Vertex AI Workbench features
and includes the following:
Pre-installed
data science
packages
.
Cuda libraries
similar to
Deep Learning Containers
.
Google Cloud JupyterLab integrations such as the
Dataproc
and
BigQuery
integrations.
Common system packages such as
curl
or
git
.
Metadata-based JupyterLab configuration.
Micromamba-based kernel management.
Specifications
The standard base container has the following specifications:
Base image:
nvidia/cuda:12.6.1-cudnn-devel-ubuntu24.04
Image size: Approximately 22 GB
URI:
us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-container:latest
Slim base container
The slim base container provides a minimal set of configurations
that permit a proxy connection to the instance. Standard
Vertex AI Workbench features and packages aren't included,
except for the following:
JupyterLab
Metadata-based JupyterLab configuration
Micromamba-based kernel management
Additional packages or JupyterLab extensions must be installed and
managed independently.
Specifications
The slim base container has the following specifications:
Base image:
marketplace.gcr.io/google/ubuntu24.04
Image size: Approximately 2 GB
URI:
us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-container-slim:latest
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Required roles
To get the permissions that
      you need to create a Vertex AI Workbench instance with a custom container,
    
      ask your administrator to grant you the
    following IAM roles:
Notebooks Runner
(
roles/notebooks.runner
)
    
              on the user account
To pull images from the Artifact Registry repository:
Artifact Registry Reader
(
roles/artifactregistry.reader
)
    
              on the service account
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create a custom container
To create a custom container for use with Vertex AI Workbench instances:
Create a derivative container derived from a
Google-provided base container image
.
Build and push the container to Artifact Registry. You'll use
the container's URI when you create your Vertex AI Workbench instance.
For example, the URI might look like this:
gcr.io/
PROJECT_ID
/
IMAGE_NAME
.
Create the instance
You can create a Vertex AI Workbench instance based on a
custom container by using the Google Cloud console or the Google Cloud CLI.
Console
To create a Vertex AI Workbench instance based on a custom container,
do the following:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
In the
New instance
dialog, click
Advanced options
.
In the
Create instance
dialog, in the
Environment
section,
select
Use custom container
.
For
Docker container image
, click
Select
.
In the
Select container image
dialog, navigate to
the container image that you want to use, and then click
Select
.
Optional. For
Post-startup script
, enter a path to a post-startup
script that you want to use.
Optional. Add metadata for your instance. To learn more, see
Custom container metadata
.
Optional. In the
Networking section
, customize your network settings.
To learn more, see
Network configuration options
.
Complete the rest of the instance creation dialog, and then
click
Create
.
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link.
gcloud
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: the name of your Vertex AI Workbench instance;
    must start with a letter followed by up to 62 lowercase letters,
    numbers, or hyphens (-), and cannot end with a hyphen
PROJECT_ID
: your project ID
LOCATION
: the zone where you want your instance to be located
CUSTOM_CONTAINER_PATH
: the path to the container image repository,
    for example:
gcr.io/
PROJECT_ID
/
IMAGE_NAME
METADATA
: custom metadata to apply to this instance;
    for example, to specify a post-startup-script,
    you can use the
post-startup-script
metadata tag, in the format:
"--metadata=post-startup-script=gs://
BUCKET_NAME
/hello.sh"
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
\
--project
=
PROJECT_ID
\
--location
=
LOCATION
\
--container-repository
=
CUSTOM_CONTAINER_URL
\
--container-tag
=
latest
\
--metadata
=
METADATA
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
`
--project
=
PROJECT_ID
`
--location
=
LOCATION
`
--container-repository
=
CUSTOM_CONTAINER_URL
`
--container-tag
=
latest
`
--metadata
=
METADATA
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
^
--project
=
PROJECT_ID
^
--location
=
LOCATION
^
--container-repository
=
CUSTOM_CONTAINER_URL
^
--container-tag
=
latest
^
--metadata
=
METADATA
For more information about the command for creating an
instance from the command line, see the
gcloud CLI
documentation
.
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link in the Google Cloud console.
Network configuration options
In addition to the
general network options
,
a Vertex AI Workbench instance with a custom container must have access to
the Artifact Registry service.
If you have turned off public IP access for your VPC,
ensure that you have enabled Private Google Access.
Enable Image streaming
The custom container host is provisioned to interact with
Image streaming in Google Kubernetes Engine (GKE),
which pulls containers faster and reduces initialization time for
large containers once they are cached in the GKE
remote file system.
To view the requirements for enabling Image streaming, see
Requirements
.
Often, Image streaming can be used with Vertex AI Workbench instances
by enabling the Container File System API.
Enable
Container File System API
How the host VM runs the custom container
Instead of using Docker to run the custom container, the host VM uses
nerdctl
under the Kubernetes namespace to load and run the
container. This lets Vertex AI Workbench use
Image streaming for custom containers.
# Runs the custom container.
sudo
/
var
/
lib
/
google
/
nerdctl
/
nerdctl
--
snapshotter
=
gcfs
-
n
k8s
.
io
run
--
name
payload
-
container
Example installation: custom container with a custom default kernel
The following example shows how to create a new kernel with a pip package
pre-installed.
Create a new custom container:
FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-container:latest

ENV MAMBA_ROOT_PREFIX=/opt/micromamba

RUN micromamba create -n
ENVIRONMENT_NAME
-c conda-forge python=
PYTHON_VERSION
-y

SHELL ["micromamba", "run", "-n", "
ENVIRONMENT_NAME
", "/bin/bash", "-c"]

RUN micromamba install -c conda-forge pip -y
RUN pip install
PACKAGE
RUN pip install ipykernel
RUN python -m ipykernel install --prefix /opt/micromamba/envs/
ENVIRONMENT_NAME
--name
ENVIRONMENT_NAME
--display-name
KERNEL_NAME
#
Creation of a micromamba kernel automatically creates a python3 kernel
#
that must be removed if it's in conflict with the new kernel.
RUN rm -rf "/opt/micromamba/envs/
ENVIRONMENT_NAME
/share/jupyter/kernels/python3"
Add the new container to Artifact Registry:
gcloud auth configure-docker
REGION
-docker.pkg.dev
docker build -t
REGION
-docker.pkg.dev/
PROJECT_ID
/
REPOSITORY_NAME
/
IMAGE_NAME
.
docker push
REGION
-docker.pkg.dev/
PROJECT_ID
/
REPOSITORY_NAME
/
IMAGE_NAME
:latest
Create an instance:
gcloud workbench instances create
INSTANCE_NAME
\
    --project=
PROJECT_ID
\
    --location=
ZONE
\
    --container-repository=
REGION
-docker.pkg.dev/
PROJECT_ID
/
REPOSITORY_NAME
/
IMAGE_NAME
\
    --container-tag=latest
Persistent kernels for custom containers
Vertex AI Workbench custom containers only mount a data disk to the
/home/
USER
directory within each container,
where
jupyter
is the default user. This means that
any change outside of
/home/
USER
is ephemeral and
won't persist after a restart. If you need installed packages to persist
for a specific kernel, you can create a kernel in the
/home/
USER
directory.
Note:
Base containers are configured with a mount point on
/home/
USER
. To create a persistent kernel
when building a custom image, you must enable Docker BuildKit.
See
Notes about specifying
volumes
.
To learn more about BuildKit, see the
BuildKit
documentation
.
To create a kernel in the
/home/
USER
directory:
Create a micromamba environment:
micromamba create -p /home/
USER
/
ENVIRONMENT_NAME
-c conda-forge python=3.11 -y
micromamba activate /home/
USER
/
ENVIRONMENT_NAME
pip install ipykernel
pip install -r ~/requirement.txt
python -m ipykernel install --prefix "/home/
USER
/
ENVIRONMENT_NAME
" --display-name "Example Kernel"
Replace the following:
USER
: the user directory name, which is
jupyter
by default
ENVIRONMENT_NAME
: the name of the environment
PYTHON_VERSION
: the Python version, for example
3.11
Wait 30 seconds to 1 minute for the kernels to refresh.
Updating the startup of the base container
The base container for a Vertex AI Workbench instance
(
us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-container:latest
)
starts JupyterLab by running
/run_jupyter.sh
.
If you modify the container's startup in a derivative container, you must
append
/run_jupyter.sh
to run the default configuration of JupyterLab.
The following is an example of how the Dockerfile might be modified:
#
DockerFile
FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-container:latest

CP startup_file.sh /
#
Ensure that you have the correct permissions and startup is executable.
RUN chmod 755 /startup_file.sh && \
    chown jupyter:jupyter /startup_file.sh
#
Override the existing CMD directive from the base container.
CMD ["/startup_file.sh"]
#
/startup_file.sh

echo "Running startup scripts"
...

/run_jupyter.sh
Updating the JupyterLab Configuration within the base container
If you need to modify the JupyterLab configuration on the base
container you must do the following:
Ensure that JupyterLab is configured to port 8080. Our proxy agent is
configured to forward any request to port 8080 and if the jupyter server
isn't listening to the correct port, the instance encounters
provisioning issues.
Modify JupyterLab packages under the
jupyterlab
micromamba environment.
We provide a separate package environment to run JupyterLab and its
plugin to ensure that there aren't any dependency conflicts with the kernel
environment. If you want to install an additional JupyterLab extension,
you must install it within the
jupyterlab
environment. For example:
# DockerFile
FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-container:latest
RUN micromamba activate jupyterlab && \
  jupyter nbextension install nbdime
Custom Container Metadata
In addition to the standard list of
metadata
that can be applied to a Vertex AI Workbench instance, instances with
custom containers include the following metadata for managing the
instantiation of the payload container:
Feature
Description
Metadata key
Accepted values and defaults
Enables Cloud Storage FUSE on a container image
Mounts
/dev/fuse
onto the container and enables
gcsfuse
for use on the container.
container-allow-fuse
true
: Enables Cloud Storage FUSE.
false
(default): Doesn't enable Cloud Storage FUSE.
Additional container run parameters
Appends additional container parameters to
nerdctl run
,
          where
nerdctl
is the Containerd CLI.
container-custom-params
A string of container run parameters. Example:
--v /mnt/disk1:/mnt/disk1
.
Additional container environment flags
Stores Environment variables into a flag under
/mnt/stateful_partition/workbench/container_env
and appends it to
nerdctl run
.
container-env-file
A string of container environment variables. Example:
CONTAINER_NAME=derivative-container
.
Upgrade a Custom Container
When your instance starts for the first time, it pulls the container image
from a URI stored in the
custom-container-payload
metadata.
If you use the
:latest
tag, the container is updated at
every restart. The
custom-container-payload
metadata value
can't be modified directly because it's a
protected metadata key
.
To update your instance's custom container image, you can use the following
methods supported by the Google Cloud CLI, Terraform, or
the Notebooks API.
gcloud
You can update the custom container image metadata on
a Vertex AI Workbench instance by using the following command:
gcloud
workbench
instances
update
INSTANCE_NAME
\
--container-repository
=
CONTAINER_URI
\
--container-tag
=
CONTAINER_TAG
Terraform
You can change the
container_image
field in the terraform configuration
to update the container payload.
To learn how to apply or remove a Terraform configuration, see
Basic Terraform commands
.
resource
"google_workbench_instance"
"default"
{
name
=
"workbench-instance-example"
location
=
"us-central1-a"
gce_setup
{
machine_type
=
"n1-standard-1"
container_image
{
repository
=
"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-container"
family
=
"latest"
}
}
}
Notebooks API
Use the
instances.patch
method with changes to
gce_setup.container_image.repository
and
gce_setup.container_image.tag
in the
updateMask
.
Run the diagnostic tool
The diagnostic tool checks and verifies the status of various
Vertex AI Workbench services. To learn more,
see
Tasks performed by the
diagnostic tool
.
When you create a Vertex AI Workbench instance using a custom container,
the diagnostic tool isn't available as a script in the host environment
that users can run. Instead, it is compiled into a binary and loaded onto
a Google runtime container that is built to run diagnostic services in
a Container-Optimized OS environment. See
Container-Optimized OS
Overview
.
To run the diagnostic tool, complete the following steps:
Use ssh to connect to your Vertex AI Workbench
instance
.
In the SSH terminal, run the following command:
sudo
docker
exec
diagnostic
-
service
./
diagnostic_tool
To view additional command options, run the following command:
sudo
docker
exec
diagnostic
-
service
./
diagnostic_tool
--
help
For more information about the diagnostic tool's options, see the
monitoring
health status
documentation
.
To run the diagnostic tool by using the REST API, see the
REST API
documentation
.
Access your instance
You can access your instance through a proxy URL.
After your instance has been created and is active, you can get the
proxy URL by using the gcloud CLI.
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: the name of your Vertex AI Workbench instance
PROJECT_ID
: your project ID
LOCATION
: the zone where your instance is located
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
describe
INSTANCE_NAME
\
--project
=
PROJECT_ID
\
--location
=
LOCATION
|
grep
proxy-url
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
describe
INSTANCE_NAME
`
--project
=
PROJECT_ID
`
--location
=
LOCATION
|
grep
proxy-url
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
describe
INSTANCE_NAME
^
--project
=
PROJECT_ID
^
--location
=
LOCATION
|
grep
proxy-url
proxy-url: 7109d1b0d5f850f-dot-datalab-vm-staging.googleusercontent.com
The
describe
command returns your proxy URL. To access your
instance, open the proxy URL in a web browser.
For more information about the command for describing an
instance from the command line, see the
gcloud CLI
documentation
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/create-dataproc-enabled.txt
Create a Dataproc Spark-enabled Vertex AI Workbench instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create a Dataproc Spark-enabled instance
Dataproc Serverless
is now
Google Cloud Serverless for Apache Spark
.
Until updated, the Google Cloud console and JupyterLab pages
will refer to the previous name.
This page describes how to create a Dataproc Spark-enabled
Vertex AI Workbench instance. This page also describes the benefits
of the Dataproc JupyterLab extension and provides an overview
on how to use the extension with Serverless for Apache Spark and
Dataproc on Compute Engine.
Overview of the Dataproc JupyterLab extension
Vertex AI Workbench instances have the Dataproc
JupyterLab extension preinstalled, as of version
M113
and later.
Note:
You can also
install and use the
      Dataproc JupyterLab extension on your local machine or
      a Compute Engine VM
      instance
.
The Dataproc JupyterLab extension provides two ways to run
Apache Spark notebook jobs: Dataproc clusters and
Google Cloud Serverless for Apache Spark.
Dataproc clusters
include a rich set
    of features with control over the infrastructure that Spark runs on.
    You choose the size and configuration of your Spark cluster, allowing
    for customization and control over your environment. This approach is
    ideal for complex workloads, long-running jobs, and
    fine-grained resource management.
Serverless for Apache Spark
eliminates infrastructure concerns. You submit your Spark jobs, and
    Google handles the provisioning, scaling, and optimization of resources
    behind the scenes. This serverless approach offers a
    cost-efficient option for data science and ML workloads.
With both options, you can use Spark for data processing
  and analysis. The choice between Dataproc clusters and
  Serverless for Apache Spark depends on your specific
  workload requirements, required level of control, and resource usage patterns.
Benefits of using Serverless for Apache Spark for data science and
  ML workloads include:
No cluster management
: You don't need to worry
    about provisioning, configuring, or managing Spark clusters. This saves
    you time and resources.
Autoscaling
: Serverless for Apache Spark
    automatically scales up and down based on the workload, so you only pay
    for the resources you use.
High performance
: Serverless for Apache Spark
    is optimized for performance and takes advantage of Google Cloud's
    infrastructure.
Integration with other Google Cloud technologies
:
    Serverless for Apache Spark integrates with other
    Google Cloud products, such as BigQuery and
    Dataplex Universal Catalog.
For more information, see the
Google Cloud Serverless for Apache Spark documentation
.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Enable the Cloud Resource Manager, Dataproc, and Notebooks APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Enable the Cloud Resource Manager, Dataproc, and Notebooks APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Required roles
To ensure that the service account has the necessary
      permissions to run a notebook file on a Serverless for Apache Spark cluster or a Dataproc cluster,
    
      ask your administrator to grant the service account the
    following IAM roles:
Important:
You must grant these roles
      to the service account,
not
to your user account. Failure to grant the roles to the correct principal might result in permission errors.
Dataproc Worker
(
roles/dataproc.worker
)
    
              on your project
Dataproc Editor
(
roles/dataproc.editor
)
    
              on the cluster for the
dataproc.clusters.use
permission
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
These predefined roles contain
        
        the permissions required to run a notebook file on a Serverless for Apache Spark cluster or a Dataproc cluster. To see the exact permissions that are
        required, expand the
Required permissions
section:
Required permissions
The following permissions are required to run a notebook file on a Serverless for Apache Spark cluster or a Dataproc cluster:
dataproc.agents.create
dataproc.agents.delete
dataproc.agents.get
dataproc.agents.update
dataproc.tasks.lease
dataproc.tasks.listInvalidatedLeases
dataproc.tasks.reportStatus
dataproc.clusters.use
Your administrator might also be able to give the service account
          these permissions
        with
custom roles
or
        other
predefined roles
.
Create an instance with Dataproc enabled
To create a Vertex AI Workbench instance with
Dataproc enabled, do the following:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
In the
New instance
dialog, click
Advanced options
.
In the
Create instance
dialog, in the
Details
section, make sure
Enable Dataproc Serverless Interactive Sessions
is selected.
Make sure
Workbench type
is set to
Instance
.
In the
Environment
section, make sure you use the latest version
or a version numbered
M113
or higher.
Click
Create
.
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link.
Note:
Specific network configurations could affect your ability to use
      the Dataproc extension. For more information on how to
      ensure that your network configuration is compatible, see
Network configuration options
.
Open JupyterLab
Next to your instance's name, click
Open JupyterLab
.
The JupyterLab
Launcher
tab opens in your browser. By default it contains
sections for
Serverless for Apache Spark Notebooks
and
Dataproc Jobs and Sessions
. If there are
Jupyter-ready clusters in the selected project and region, there will be
a section called
Dataproc Cluster Notebooks
.
Use the extension with Serverless for Apache Spark
Serverless for Apache Spark runtime templates that are in the same region and project
as your Vertex AI Workbench instance appear in the
Serverless for Apache Spark Notebooks
section of the JupyterLab
Launcher
tab.
To create a runtime template, see
Create a Serverless for Apache Spark
runtime template
.
To open a new serverless Spark notebook, click a runtime template. It
takes about a minute for the remote Spark kernel to start. After the kernel
starts, you can start coding.
Use the extension with Dataproc on Compute Engine
If you created a
Dataproc on Compute Engine
Jupyter cluster
,
the
Launcher
tab has
a
Dataproc Cluster Notebooks
section.
Four cards appear for each Jupyter-ready Dataproc cluster
that you have access to in that region and project.
To change the region and project, do the following:
Select
Settings
>
Cloud Dataproc Settings
.
On the
Setup Config
tab, under
Project Info
, change the
Project ID
and
Region
, and then click
Save
.
These changes don't take effect until you restart JupyterLab.
To restart JupyterLab, select
File
>
Shut Down
, and
then click
Open JupyterLab
on the
Vertex AI Workbench instances
page.
To create a new notebook, click a card. After the remote kernel on the
Dataproc cluster starts, you can start writing your code
and then run it on your cluster.
Manage Dataproc on an instance using the gcloud CLI and the API
This section describes ways to manage Dataproc on a
Vertex AI Workbench instance.
Change the region of your Dataproc cluster
Your Vertex AI Workbench instance's default kernels, such as
Python and TensorFlow, are local kernels that run in the instance's VM.
On a Dataproc Spark-enabled Vertex AI Workbench instance,
your notebook runs on a Dataproc cluster through a remote kernel.
The remote kernel runs on a service outside of your instance's VM,
which lets you access any Dataproc cluster within the same
project.
By default Vertex AI Workbench uses Dataproc clusters
within the same region as your instance, but you can
change the Dataproc
region
as long as the
Component Gateway
and
the
optional Jupyter component
are enabled on the Dataproc cluster.
Test Access
The Dataproc JupyterLab extension is enabled by default for
Vertex AI Workbench instances. To test access to Dataproc,
you can check access to your instance's remote kernels by sending the following
curl request to the
kernels.googleusercontent.com
domain:
curl --verbose -H "Authorization: Bearer $(gcloud auth print-access-token)" https://
PROJECT_ID
-dot-
REGION
.kernels.googleusercontent.com/api/kernelspecs | jq .
If the curl command fails, check to make sure that:
Your DNS entries are configured correctly.
There is a cluster available in the same project (or you will need to create one if it doesn't exist).
Your cluster has both the
Component Gateway
and
the
optional Jupyter component
enabled.
Turn off Dataproc
Vertex AI Workbench instances are created with Dataproc
enabled by default. You can create a Vertex AI Workbench instance with
Dataproc turned off by setting the
disable-mixer
metadata
key to
true
.
gcloud
workbench
instances
create
INSTANCE_NAME
--metadata
=
disable-mixer
=
true
Enable Dataproc
You can enable Dataproc on a stopped Vertex AI Workbench instance by updating the metadata value.
gcloud
workbench
instances
update
INSTANCE_NAME
--metadata
=
disable-mixer
=
false
Manage Dataproc using Terraform
Dataproc for Vertex AI Workbench instances
on Terraform is managed using the
disable-mixer
key in the metadata field.
Turn on Dataproc by setting the
disable-mixer
metadata
key to
false
. Turn off Dataproc by setting
the
disable-mixer
metadata key to
true
.
To learn how to apply or remove a Terraform configuration, see
Basic Terraform commands
.
resource "google_workbench_instance" "default" {
  name     = "workbench-instance-example"
  location = "us-central1-a"

  gce_setup {
    machine_type = "n1-standard-1"
    vm_image {
      project = "cloud-notebooks-managed"
      family  = "workbench-instances"
    }
    metadata = {
      disable-mixer = "false"
    }
  }
}
Troubleshoot
To diagnose and resolve issues related to creating
a Dataproc Spark-enabled instance, see
Troubleshooting
Vertex AI Workbench
.
What's next
For more information about the Dataproc JupyterLab extension, see
Use the JupyterLab extension to develop serverless Spark workloads
.
To learn more about Serverless for Apache Spark, see the
Serverless for Apache Spark documentation
Learn how to run Serverless for Apache Spark workloads without provisioning and
managing clusters.
To learn more about using Spark with Google Cloud products and
services, see
Spark on Google Cloud
.
Browse the available
Dataproc templates on
GitHub
.
Learn about Serverless Spark through the
serverless-spark-workshop
on
GitHub
.
Read the
Apache Spark documentation
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/create-euc-instance.txt
Create a Vertex AI Workbench instance with user credential access  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create an instance with user credential access
Preview
This feature is
        
        subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the
Service Specific
        Terms
.
        
        Pre-GA features are available "as is" and might have limited support.
      
      For more information, see the
launch stage descriptions
.
This page describes how to create a Vertex AI Workbench instance that
accesses Google Cloud services and APIs through your user credentials.
Your user credentials are the credentials associated with your Google Account.
Your user credentials determine which Google Cloud services and APIs your
Google Account has access to.
By default, when you run code in a Vertex AI Workbench instance,
your instance can access Google Cloud services and APIs by using
the credentials associated with your instance's service account. This
means that your instance has the same access to Google Cloud as
the service account.
This page describes how to create and configure an instance so that it
has the same access to Google Cloud as your user credentials.
Overview
Vertex AI Workbench uses a global google-managed OAuth client
to manage user credential access, scoped for the Google Cloud resources
in the user's project. Users must grant consent to the OAuth Client to
manage their credentials for each Vertex AI Workbench instance.
This is done one time per instance through a dialog that opens when
you click the
Open JupyterLab
button in the Google Cloud console.
The service account used to create the Vertex AI Workbench instance is the
following service agent:
service-
PROJECT_NUMBER
@gcp-sa-notebooks-vm.
iam.
gserviceaccount.
com
.
This service agent provides limited permissions for essential services such
as exporting logs. Users can't specify a different service account
if the end user credentials feature is enabled.
Instances with end user credentials enabled have the
notebooks-managed-euc: true
Compute Engine label and the
euc-enabled: true
metadata key
attached to the VM resource to denote the feature enablement.
Limitations
Consider the following limitations when you plan your project:
Vertex AI Workbench uses a global google-managed OAuth client
to manage user credential access. Organizations can't
enact fine grain controls, access the OAuth client, or use logging
to check for use of the OAuth client.
To protect the security of Vertex AI Workbench instances with
managed user credentials,
users aren't able to
:
Use SSH to access the instance.
Run a Vertex AI Workbench post-startup script or a
Compute Engine startup script
.
Access the detailed VM page.
Use an image that isn't created by Google.
Using
third party
credentials
isn't supported because the OAuth client only supports Google-managed
OAuth credentials.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Required roles
To get the permissions that
      you need to create a Vertex AI Workbench instance,
    
      ask your administrator to grant you the
Notebooks Runner
(
roles/notebooks.runner
)
     IAM role on the project.
  

  

  
  
  For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create a single user instance
To create a Vertex AI Workbench instance by using
the Google Cloud console, do the following:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
In the
New instance
dialog, click
Advanced options
.
In the
Create instance
dialog, in the
Details
section,
provide the following information for your new instance:
Name
: Provide a name for your new instance. The name
must start with a letter followed by up to 62 lowercase letters,
numbers, or hyphens (-), and cannot end with a hyphen.
Region
and
Zone
: Select a region and zone for
the new instance. For best network performance,
select the region that is geographically closest to you.
See the available
Vertex AI Workbench
locations
.
In the
IAM and Security
section, select
Single user
.
In the
User email
field,
enter the user account that you want to grant access. If the
specified user is not the creator of the instance, you must grant
the specified user the
Service Account User
role
(
roles/iam.serviceAccountUser
) on the instance's service account.
Select
Enable managed end user credentials
.
Complete the rest of the instance creation dialog, and then
click
Create
.
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link in the Google Cloud console.
Users must grant consent to the OAuth client to manage their credentials
for each Vertex AI Workbench instance. This is done one time
per instance. To grant consent, click
Open JupyterLab
and complete
the dialog that appears.
If you try to access the instance without granting consent, JupyterLab
displays a message to authenticate by opening JupyterLab from the
Google Cloud console.
To verify that your end user credentials are available within JupyterLab,
open a Terminal in JupyterLab, and enter the following command:
gcloud
auth
list
Authenticate the instance with your user credentials
Vertex AI Workbench can use Application Default Credentials (ADC)
to authenticate your user credentials to Google Cloud services and APIs.
This section describes how to provide your user credentials to ADC if any of
the limitations prevent you from enabling managed credentials.
The authentication steps depend on whether you are using a Google Account
or third party credentials.
Google Account
After you can access JupyterLab on your instance, do the following:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Next to your instance's name, click
Open JupyterLab
.
In JupyterLab, select
File
>
New
>
Terminal
.
In the terminal window, run the following:
gcloud
auth
login
Enter
Y
.
Follow the instructions to copy a verification code and enter it into
the terminal.
Third party credentials
If you
created an instance with
third party credentials
,
then after the JupyterLab proxy is available, do the following:
Open JupyterLab by using the federated JupyterLab proxy.
In JupyterLab, select
File
>
New
>
Terminal
.
Create a Workforce Identity Federation
credential file
with headless sign-in.
In the terminal window, run the following:
gcloud
auth
login
--cred-file
=
"
CREDENTIAL_FILE
"
Replace
CREDENTIAL_FILE
with the path and name of the
credential file that you created.
Follow the instructions to authenticate through the
third party authentication portal.
Confirm that your credentials are accessible through your instance
by using the following command:
gcloud
auth
list
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/create-specific-version.txt
Create a specific version of a Vertex AI Workbench instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create a specific version of an instance
This page describes how to create a specific version of a
Vertex AI Workbench instance.
Why you might want to create a specific version
To ensure that your Vertex AI Workbench instance has software
that is compatible with your code or application, you might want to create
a specific version.
Vertex AI Workbench instance images are updated frequently, and
specific versions of preinstalled software and packages vary from version
to version.
To learn more about specific Vertex AI Workbench versions,
see the
Vertex AI release notes
.
After you create a specific version of
a Vertex AI Workbench instance, you can upgrade it.
Upgrading the instance updates the preinstalled software and packages.
For more information,
see
Upgrade an instance's environment
.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Create a specific version
You can create a specific version of a Vertex AI Workbench instance
by using the Google Cloud console or the Google Cloud CLI.
Console
To create a specific version of a Vertex AI Workbench instance,
do the following:
When you
create an instance
,
in the
Environment
section, select
Use a previous version
.
Click the
Version
list, and select a version. Versions are numbered
in the form of an
M
followed by the number of the release,
for example,
M123
.
Complete the rest of the instance-creation dialog, and then
click
Create
.
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link.
gcloud
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: the name of your Vertex AI Workbench instance;
    must start with a letter followed by up to 62 lowercase letters,
    numbers, or hyphens (-), and cannot end with a hyphen
PROJECT_ID
: your project ID
LOCATION
: the zone where you want your instance to be located
VM_IMAGE_NAME
: the image name; to get a list of the available
    image names, use the
get-config
command
MACHINE_TYPE
: the
machine type
of your instance's VM
METADATA
: custom metadata to apply to this instance;
      for example, to specify a post-startup-script,
      you can use the
post-startup-script
metadata tag, in the format:
--metadata=post-startup-script=gs://
BUCKET_NAME
/hello.sh
To enable the JupyterLab 4 preview, use
--metadata=enable-jupyterlab4-preview=true
. For more information, see
JupyterLab 4 preview
.
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
\
--project
=
PROJECT_ID
\
--location
=
LOCATION
\
--vm-image-project
=
"cloud-notebooks-managed"
\
--vm-image-name
=
VM_IMAGE_NAME
\
--machine-type
=
MACHINE_TYPE
\
--metadata
=
METADATA
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
`
--project
=
PROJECT_ID
`
--location
=
LOCATION
`
--vm-image-project
=
"cloud-notebooks-managed"
`
--vm-image-name
=
VM_IMAGE_NAME
`
--machine-type
=
MACHINE_TYPE
`
--metadata
=
METADATA
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
^
--project
=
PROJECT_ID
^
--location
=
LOCATION
^
--vm-image-project
=
"cloud-notebooks-managed"
^
--vm-image-name
=
VM_IMAGE_NAME
^
--machine-type
=
MACHINE_TYPE
^
--metadata
=
METADATA
For more information about the command for creating an
instance from the command line, see the
gcloud CLI
documentation
.
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link in the Google Cloud console.
What's next
Learn more about
upgrading
Vertex AI Workbench instances
to ensure that your instance upgrades only when you are ready.
Learn about
monitoring the health status
of
your Vertex AI Workbench instance.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/create-third-party-instance.txt
Create a Vertex AI Workbench instance with third party credentials  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create an instance with third party credentials
This page describes how to create a Vertex AI Workbench instance with
third party credentials.
Overview
You can create and manage Vertex AI Workbench instances with
third party credentials provided by Workforce Identity Federation.
Workforce Identity Federation uses your external identity provider (IdP)
to grant a group of users access to Vertex AI Workbench instances
through a proxy.
Access to a Vertex AI Workbench instance is granted by assigning a
workforce pool principal
to the Vertex AI Workbench instance's service account.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Configure your IdP with a
workforce
    identity pool
.
Required role for creating an instance
To ensure that your workforce pool principal has the necessary
      permissions to create a Vertex AI Workbench instance,
    
      ask your administrator to grant your workforce pool principal the
Notebooks Admin
(
roles/notebooks.admin
)
     IAM role on the project.
  

  

  
  
  For more information about granting roles, see
Manage access to projects, folders, and organizations
.
Your administrator might also be able to give your workforce pool principal
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Required roles for using third party credentials
Your workforce pool principal needs access to your
Vertex AI Workbench instance's service account, with specific permissions.
To ensure that the workforce pool principal has the necessary
      permissions to use a Vertex AI Workbench instance with third party credentials,
    
      ask your administrator to grant the workforce pool principal the
    following IAM roles on the service account that you'll specify when you create your instance:
Service Account Token Creator
(
roles/iam.serviceAccountTokenCreator
)
Service Account User
(
roles/iam.serviceAccountUser
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
Your administrator might also be able to give the workforce pool principal
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create the instance using third party credentials
To ensure that your Vertex AI Workbench instance contains a
byoid.googleusercontent.com
domain, you must do one of the following:
Create the instance by using the Google Cloud
Workforce Identity Federation console.
Use the
enable_third_party_identity
flag when you create your instance.
You can create a Vertex AI Workbench using third party credentials by using the
Google Cloud console or the gcloud CLI:
Console
Sign in to the Google Cloud console using a workforce pool provider.
Go to the Console
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
In the
New instance
dialog, click
Advanced options
.
In the
Create instance
dialog, in the
IAM and security
section,
do the following:
Make sure
Service account
is selected.
Clear
Use default Compute Engine service account
, and then,
in the
Service account email
field, enter the service account
email address that is associated with your workforce principal.
Click
Create
.
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link.
gcloud
Follow the
IAM guide
for authenticating the gcloud CLI with a workforce identity pool.
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: the name of your Vertex AI Workbench instance;
    must start with a letter followed by up to 62 lowercase letters,
    numbers, or hyphens (-), and cannot end with a hyphen
PROJECT_ID
: your project ID
LOCATION
: the zone where you want your instance to be located
VM_IMAGE_PROJECT
: the ID of the Google Cloud project that
    VM image belongs to, in the format:
projects/
IMAGE_PROJECT_ID
VM_IMAGE_NAME
: the full image name; to find the image name of a
    specific version, see
Find
    the specific version
MACHINE_TYPE
: the
machine type
of your instance's VM
METADATA
: custom metadata to apply to this instance;
    for example, to specify a post-startup-script,
    you can use the
post-startup-script
metadata tag, in the format:
"--metadata=post-startup-script=gs://
BUCKET_NAME
/hello.sh"
SERVICE_ACCOUNT_EMAIL
: the service account email address that
    is associated with your workforce principal
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
\
--project
=
PROJECT_ID
\
--location
=
LOCATION
\
--vm-image-project
=
VM_IMAGE_PROJECT
\
--vm-image-name
=
VM_IMAGE_NAME
\
--machine-type
=
MACHINE_TYPE
\
--metadata
=
METADATA
\
--service-account-email
=
SERVICE_ACCOUNT_EMAIL
\
--enable-third-party-identity
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
`
--project
=
PROJECT_ID
`
--location
=
LOCATION
`
--vm-image-project
=
VM_IMAGE_PROJECT
`
--vm-image-name
=
VM_IMAGE_NAME
`
--machine-type
=
MACHINE_TYPE
`
--metadata
=
METADATA
`
--service-account-email
=
SERVICE_ACCOUNT_EMAIL
`
--enable-third-party-identity
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
^
--project
=
PROJECT_ID
^
--location
=
LOCATION
^
--vm-image-project
=
VM_IMAGE_PROJECT
^
--vm-image-name
=
VM_IMAGE_NAME
^
--machine-type
=
MACHINE_TYPE
^
--metadata
=
METADATA
^
--service-account-email
=
SERVICE_ACCOUNT_EMAIL
^
--enable-third-party-identity
For more information about the command for creating an
instance from the command line, see the
gcloud CLI
documentation
.
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link in the Google Cloud console.
Access Jupyterlab with third party credentials
Your new Vertex AI Workbench instance creates two separate proxy URLs with the
following domains:
byoid.googleusercontent.com
: This domain can only be used by users
authenticating with a workforce identity pool. Its value is stored in your
instance's metadata field
proxy-byoid-url
. This metadata value activates
an
Open JupyterLab
link in the
Google Cloud
Workforce Identity Federation console
(
console.cloud.google/
).
googleusercontent.com
: This domain can only be used by users
authenticating with the default Google's First Party Authentication.
Its value is stored in your instance's metadata field
proxy-url
. This
metadata value activates an
Open JupyterLab
link in the
Google Cloud console (
console.cloud.google.com
).
What's next
To learn more about third party principals to use for provisioning notebooks,
see
Workforce Identity Federation
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/create.txt
Create a Vertex AI Workbench instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create a Vertex AI Workbench instance
This page shows you how to create a Vertex AI Workbench instance by using
the Google Cloud console or the Google Cloud CLI. While creating your instance,
you can configure your instance's hardware, encryption type, network,
and other details.
Before you begin
Before you create a Vertex AI Workbench instance, you must complete
the following steps:
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Note:
The
Notebooks API
lets you
      manage Vertex AI Workbench resources. For managing
      Vertex AI resources, see the
Vertex AI API
.
Create an instance
You can create a Vertex AI Workbench instance
by using the Google Cloud console, the gcloud CLI,
or Terraform:
Console
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
In the
New instance
dialog, click
Advanced options
.
In the
Create instance
dialog,
in the
Details
section,
provide the following information for your new instance:
Name
: Provide a name for your new instance. The name
must start with a letter followed by up to 62 lowercase letters,
numbers, or hyphens (-), and cannot end with a hyphen.
Region
and
Zone
: Select a region and zone for
the new instance. For best network performance,
select the region that is geographically closest to you.
See the available
Vertex AI Workbench
locations
.
Labels
: Optional. Provide custom key-value labels for the
instance.
Network tags
: Optional. Provide
network tags
for the instance.
In the
Environment
section, provide the following:
JupyterLab version
: To enable JupyterLab 4
(
Preview
), select
JupyterLab 4.x
. If unselected, your instance will use
JupyterLab 3. For more information,
see
JupyterLab 4 preview
.
Version
: Use the latest version or a previous version
of Vertex AI Workbench instances.
Post-startup script
: Optional. Click
Browse
to
select a script to run one time, after the instance is created.
The path must be a URL or Cloud Storage path,
for example:
gs://
PATH_TO_FILE
/
FILE_NAME
.
Metadata
: Optional. Provide custom metadata keys for the
instance.
In the
Machine type
section, provide the following:
Machine type
: Select the number of CPUs and amount of RAM for your
new instance. Vertex AI Workbench provides monthly cost
estimates for each machine type that you select.
GPU
: Optional. If you want GPUs, select the
GPU type
and
Number of GPUs
for your new instance. The accelerator type
that you want must be available in your instance's
zone. To learn about accelerator availability by zone, see
GPU regions and zones
availability
.
For information about the different GPUs, see
GPUs on Compute Engine
.
Select
Install NVIDIA GPU driver automatically for me
.
Note:
You can modify the machine type and GPU configuration for
your instance after it is created.
For more information, see
Change machine type and configure GPUs of
a Vertex AI Workbench instance
.
Shielded VM
: Optional. Select or clear the following checkboxes:
Secure Boot
Virtual Trusted Platform Module (vTPM)
Integrity monitoring
Idle shutdown
: Optional.
To change the number of minutes before shutdown,
in the
Time of inactivity before shutdown (Minutes)
field,
change the value to an integer from 10 through 1440.
To turn off idle shutdown, clear
Enable Idle Shutdown
.
In the
Disks
section, provide the following:
Disks
: Optional. To change the default data disk settings,
select a
Data disk type
and
Data disk size in GB
.
For more information about disk types, see
Storage options
.
Delete to trash
: Optional. Select this checkbox to use
the operating system's default trash behavior, If you use
the default trash behavior, files deleted by using the JupyterLab
user interface are recoverable but these deleted files
do use disk space.
Encryption
: Select
Google-managed encryption key
or
Customer-managed encryption key (CMEK)
.
To use CMEK, see
Customer-managed encryption keys
.
In the
Networking
section, provide the following:
Networking
: Adjust the network options to use a network in
your current project or a
Shared VPC network
from
a host project, if one is configured. If you are using a
Shared VPC
in the host project, you must also grant the
Compute Network User role
(
roles/compute.networkUser
) to the
Notebooks Service
Agent
from the service project.
In the
Network
field, select the network that you want. You
can select a VPC network, as long as the network
has
Private Google Access
enabled or can access the internet. For more information,
see
network configuration options
.
In the
Subnetwork
field, select the subnetwork that you want.
To turn off the external IP address,
clear the
Assign external IP address
checkbox.
To turn off proxy access, clear the
Allow proxy access
checkbox.
Note:
If you turn off proxy access, you must
use SSH to connect
to your instance's JupyterLab interface
.
In the
IAM and security
section, provide the following:
IAM and security
: To grant access to the instance's
JupyterLab interface, complete one of the following steps:
To grant access to JupyterLab through a service account,
select
Service account
.
To use the default Compute Engine service account,
select
Use default Compute Engine service account
.
To use a custom service account, clear
Use
default Compute Engine service account
, and then,
in the
Service account email
field, enter
your custom service account email address.
To grant a single user access to the JupyterLab interface,
do the following:
Select
Single user
, and then,
in the
User email
field,
enter the user account that you want to grant access. If the
specified user is not the creator of the instance, you must grant
the specified user the
Service Account User
role
(
roles/iam.serviceAccountUser
) on the
instance's service account.
Your instance uses a service account to interact with
Google Cloud services and APIs.
To use the
default Compute Engine service account,
select
Use default Compute Engine service account
.
To use a custom service account, clear
Use
default Compute Engine service account
, and then,
in the
Service account email
field, enter
your custom service account email address.
Note:
To grant access to the instance through the service account
    or single user option, you must use an individual's
    user account email address. Group access is not supported.
To learn more about granting access,
see
Manage access
.
Security options
: Select or clear the following checkboxes:
Root access to the instance
nbconvert
File downloading
Terminal access
In the
System health
section, provide the following:
Environment upgrade and system health
:
To automatically upgrade to newly released environment versions,
select
Environment auto-upgrade
and complete the
Upgrade schedule
.
In
Reporting
, select or clear the following checkboxes:
Report system health
Report custom metrics to Cloud Monitoring
Install Cloud Monitoring
Report DNS status for required Google domains
Click
Create
.
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link.
gcloud
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: the name of your Vertex AI Workbench instance;
    must start with a letter followed by up to 62 lowercase letters,
    numbers, or hyphens (-), and cannot end with a hyphen
PROJECT_ID
: your project ID
LOCATION
: the zone where you want your instance to be located
VM_IMAGE_PROJECT
: the ID of the Google Cloud project that
    VM image belongs to; the default Google Cloud project ID for supported images
    is
cloud-notebooks-managed
VM_IMAGE_NAME
: the image name; to find the image name of a
    specific version, see
Find
    the specific version
MACHINE_TYPE
: the
machine type
of your instance's VM
METADATA
: custom metadata to apply to this instance;
      for example, to specify a post-startup-script,
      you can use the
post-startup-script
metadata tag, in the format:
--metadata=post-startup-script=gs://
BUCKET_NAME
/hello.sh
To enable the JupyterLab 4 preview, use
--metadata=enable-jupyterlab4-preview=true
. For more information, see
JupyterLab 4 preview
.
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
\
--project
=
PROJECT_ID
\
--location
=
LOCATION
\
--vm-image-project
=
VM_IMAGE_PROJECT
\
--vm-image-name
=
VM_IMAGE_NAME
\
--machine-type
=
MACHINE_TYPE
\
--metadata
=
METADATA
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
`
--project
=
PROJECT_ID
`
--location
=
LOCATION
`
--vm-image-project
=
VM_IMAGE_PROJECT
`
--vm-image-name
=
VM_IMAGE_NAME
`
--machine-type
=
MACHINE_TYPE
`
--metadata
=
METADATA
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
create
INSTANCE_NAME
^
--project
=
PROJECT_ID
^
--location
=
LOCATION
^
--vm-image-project
=
VM_IMAGE_PROJECT
^
--vm-image-name
=
VM_IMAGE_NAME
^
--machine-type
=
MACHINE_TYPE
^
--metadata
=
METADATA
For more information about the command for creating an
instance from the command line, see the
gcloud CLI
documentation
.
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link in the Google Cloud console.
Terraform
The following sample uses the
google_workbench_instance
Terraform resource to create
a Vertex AI Workbench instance
named
workbench-instance-example
.
To learn how to apply or remove a Terraform configuration, see
Basic Terraform commands
.
resource "google_workbench_instance" "default" {
  name     = "workbench-instance-example"
  location = "us-central1-a"

  gce_setup {
    machine_type = "n1-standard-1"
    accelerator_configs {
      type       = "NVIDIA_TESLA_T4"
      core_count = 1
    }
    vm_image {
      project = "cloud-notebooks-managed"
      family  = "workbench-instances"
    }
  }
}
JupyterLab 4 preview
Preview
      
        — JupyterLab 4
This feature is
        
        subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the
Service Specific
        Terms
.
        
        Pre-GA features are available "as is" and might have limited support.
      
      For more information, see the
launch stage descriptions
.
This section describes how to change the JupyterLab version on your instance.
This section also includes limitations for you to consider when enabling
JupyterLab 4.
Change the version of JupyterLab on an existing instance
You can change your instance's JupyterLab version by using the
Google Cloud console or the gcloud CLI.
Console
To change the JupyterLab version on an existing instance,
do the following:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click the name of your instance to open the
Instance details
page.
On the
System
tab, do one of the following:
To enable the preview, select
Enable JupyterLab 4 preview
.
To turn off the preview and use JupyterLab 3, clear
Enable JupyterLab 4 preview
.
Click
Submit
.
Reset your
instance
.
gcloud
You can change the JupyterLab version on an existing instance by using
the following command:
gcloud
workbench
instances
update
INSTANCE_NAME
\
--project
=
"
PROJECT_ID
"
\
--location
=
"
LOCATION
"
\
--metadata
=
enable-jupyterlab4-preview
=
ENABLEMENT_BOOLEAN
Replace the following:
PROJECT_ID
: your project ID
LOCATION
: the zone where you want
    your instance to be located
INSTANCE_NAME
: the name of your
    Vertex AI Workbench instance
ENABLEMENT_BOOLEAN
: use one of the following:
true
: enables the JupyterLab 4 preview
false
: turns off the JupyterLab 4 preview and
        changes to JupyterLab 3
Limitations
Consider the following limitations when enabling the JupyterLab 4 preview:
Integrations with Google Cloud services such as BigQuery
and Cloud Storage aren't supported during preview.
Enabling JupyterLab 4 in a custom container-based instance is supported. See
the
limitations for using custom containers with
Vertex AI Workbench
.
Network configuration options
A Vertex AI Workbench instance must access service endpoints
that are outside your VPC network.
You can provide this access in one of the following ways:
Assign an external IP address to
the instance. This is done by default when you create
a new instance. Make sure your
environment meets the
requirements for accessing Google APIs and
services
.
Connect the instance to a subnet where
Private Google Access
is enabled.
Make sure your environment meets the
requirements for
Private Google Access
.
If you use the
private.googleapis.com
or
restricted.googleapis.com
VIP to
provide access to the service endpoints,
add DNS entries for each of the required service
endpoints
:
notebooks.googleapis.com
*.notebooks.cloud.google.com
*.notebooks.googleusercontent.com
*.kernels.googleusercontent.com
For an instance with
third party credentials
, add a DNS entry for the following:
*.byoid.googleusercontent.com
Network tags
Your new Vertex AI Workbench instance automatically has the
deeplearning-vm
and
notebook-instance
network tags
assigned.
These tags let you manage network access to and from
your Vertex AI Workbench instance by referencing the tags in your
VPC networking firewall rules. For more information about
network tags, see
Add network tags
.
To view the network tags for a Vertex AI Workbench instance,
do the following:
In the Google Cloud console, go to the
VM instances
page.
Go to VM instances
Click the name of the instance.
In the
Networking
section, find
Network tags
.
Troubleshooting
If you encounter a problem when you create an instance, see
Troubleshooting
Vertex AI Workbench
for help with common issues.
What's next
To use a notebook to help you get started using Vertex AI and
  other Google Cloud services, see
Vertex AI
  notebook tutorials
.
Create an
    instance with Confidential Computing enabled
.
To check on the health status of your Vertex AI Workbench instance,
    see
Monitor health status
.
For a Terraform solution for simplified Vertex AI networking setup, see
Simplified Cloud Networking Configuration Solutions
.
You can create a Vertex AI Workbench instance using a private IP. For a Terraform solution, see
Workbench
.
To learn more about granting access,
            see
Manage access
.
To use CMEK, see
Customer-managed encryption keys
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/iam.txt
Vertex AI Workbench instances access control  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Access control
This page describes how to use
Identity and Access Management (IAM)
and an access mode to manage access to
Vertex AI Workbench resources.
To manage access to
Vertex AI resources, see the
Vertex AI page on
access control
.
Vertex AI Workbench uses IAM to manage
access to instances and an access mode to manage access to
each instance's JupyterLab interface.
Control access to an instance with IAM
You can manage access to a Vertex AI Workbench instance at the
project level or per instance.
To grant access to resources at the project level, assign one or more
roles
to a principal (user, group, or
service account
).
To grant access to a specific instance, set an IAM policy
on that resource. The policy defines which roles are assigned
to which principals. To learn more, see
Manage access to
an instance
.
Access to an instance can include a broad range of abilities. For example,
you might grant a principal the ability to start, stop, and upgrade
an instance. However, even granting a principal full access to
a Vertex AI Workbench instance doesn't grant
the ability to use the instance's JupyterLab interface. See the following
section.
Control access to an instance's JupyterLab interface with the access mode
You control access to a Vertex AI Workbench instance's
JupyterLab interface through the instance's access mode.
You set a JupyterLab access mode when you create
a Vertex AI Workbench instance.
The access mode can't be changed after the notebook is created.
The JupyterLab access mode determines who can use
the instance's JupyterLab interface.
The access mode also determines which credentials are used when
your instance interacts with other Google Cloud services.
To learn more, see
Manage access to an instance's
JupyterLab interface
.
Types of IAM roles
There are different types of IAM roles that can be used in
Vertex AI Workbench:
Predefined roles
let you grant a set of related
permissions to your Vertex AI Workbench resources at the project level.
Basic roles
(Owner,
Editor, and Viewer) provide access control to your Vertex AI Workbench
resources at the project level, and are common to all Google Cloud
services.
Custom roles
enable you to choose a
specific set of permissions, create your own role with those permissions,
and grant the role to users in your organization.
To add, update, or remove these roles in your Vertex AI Workbench project,
see the documentation on
granting, changing, and
revoking access
.
Predefined Vertex AI Workbench IAM roles
Vertex AI Workbench resources are managed through the Notebooks API.
Therefore, Notebooks roles define permissions and access
to the use of Vertex AI Workbench.
Role
Permissions
Notebooks Admin
(
roles/
notebooks.admin
)
Full access to Notebooks, all resources.
Lowest-level resources where you can grant this role:
Instance
aiplatform.
notebookExecutionJobs.*
aiplatform.
notebookExecutionJobs.
create
aiplatform.
notebookExecutionJobs.
delete
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.operations.list
aiplatform.pipelineJobs.create
aiplatform.schedules.*
aiplatform.schedules.create
aiplatform.schedules.delete
aiplatform.schedules.get
aiplatform.schedules.list
aiplatform.schedules.update
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.*
notebooks.environments.create
notebooks.environments.delete
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.
environments.
setIamPolicy
notebooks.executions.create
notebooks.executions.delete
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
executions.
setIamPolicy
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.delete
notebooks.instances.diagnose
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.instances.reset
notebooks.
instances.
setAccelerator
notebooks.
instances.
setIamPolicy
notebooks.instances.setLabels
notebooks.
instances.
setMachineType
notebooks.instances.start
notebooks.instances.stop
notebooks.instances.update
notebooks.
instances.
updateConfig
notebooks.
instances.
updateShieldInstanceConfig
notebooks.instances.upgrade
notebooks.instances.use
notebooks.locations.get
notebooks.locations.list
notebooks.operations.cancel
notebooks.operations.delete
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.delete
notebooks.runtimes.diagnose
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.runtimes.reset
notebooks.
runtimes.
setIamPolicy
notebooks.runtimes.start
notebooks.runtimes.stop
notebooks.runtimes.switch
notebooks.runtimes.update
notebooks.runtimes.upgrade
notebooks.schedules.create
notebooks.schedules.delete
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
notebooks.
schedules.
setIamPolicy
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Legacy Admin
(
roles/
notebooks.legacyAdmin
)
Full access to Notebooks all resources through compute API.
backupdr.
backupPlanAssociations.
createForComputeDisk
backupdr.
backupPlanAssociations.
createForComputeInstance
backupdr.
backupPlanAssociations.
deleteForComputeDisk
backupdr.
backupPlanAssociations.
deleteForComputeInstance
backupdr.
backupPlanAssociations.
fetchForComputeDisk
backupdr.
backupPlanAssociations.
getForComputeDisk
backupdr.
backupPlanAssociations.
list
backupdr.
backupPlanAssociations.
triggerBackupForComputeDisk
backupdr.
backupPlanAssociations.
triggerBackupForComputeInstance
backupdr.
backupPlanAssociations.
updateForComputeDisk
backupdr.
backupPlanAssociations.
updateForComputeInstance
backupdr.backupPlans.get
backupdr.backupPlans.list
backupdr.
backupPlans.
useForComputeDisk
backupdr.
backupPlans.
useForComputeInstance
backupdr.backupVaults.get
backupdr.backupVaults.list
backupdr.locations.list
backupdr.operations.get
backupdr.operations.list
backupdr.
serviceConfig.
initialize
cloudkms.keyHandles.*
cloudkms.keyHandles.create
cloudkms.keyHandles.get
cloudkms.keyHandles.list
cloudkms.operations.get
cloudkms.
projects.
showEffectiveAutokeyConfig
compute.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.create
compute.
addresses.
createInternal
compute.
addresses.
createTagBinding
compute.addresses.delete
compute.
addresses.
deleteInternal
compute.
addresses.
deleteTagBinding
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.addresses.setLabels
compute.addresses.use
compute.addresses.useInternal
compute.advice.calendarMode
compute.autoscalers.create
compute.autoscalers.delete
compute.autoscalers.get
compute.autoscalers.list
compute.autoscalers.update
compute.
backendBuckets.
addSignedUrlKey
compute.backendBuckets.create
compute.
backendBuckets.
createTagBinding
compute.backendBuckets.delete
compute.
backendBuckets.
deleteSignedUrlKey
compute.
backendBuckets.
deleteTagBinding
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.
backendBuckets.
setIamPolicy
compute.
backendBuckets.
setSecurityPolicy
compute.backendBuckets.update
compute.backendBuckets.use
compute.
backendServices.
addSignedUrlKey
compute.backendServices.create
compute.
backendServices.
createTagBinding
compute.backendServices.delete
compute.
backendServices.
deleteSignedUrlKey
compute.
backendServices.
deleteTagBinding
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.
backendServices.
setIamPolicy
compute.
backendServices.
setSecurityPolicy
compute.backendServices.update
compute.backendServices.use
compute.commitments.create
compute.commitments.get
compute.commitments.list
compute.commitments.update
compute.
commitments.
updateReservations
compute.
crossSiteNetworks.
create
compute.
crossSiteNetworks.
delete
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.
crossSiteNetworks.
update
compute.diskSettings.get
compute.diskSettings.update
compute.diskTypes.get
compute.diskTypes.list
compute.
disks.
addResourcePolicies
compute.disks.create
compute.disks.createSnapshot
compute.disks.createTagBinding
compute.disks.delete
compute.disks.deleteTagBinding
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
disks.
removeResourcePolicies
compute.disks.resize
compute.disks.setIamPolicy
compute.disks.setLabels
compute.
disks.
startAsyncReplication
compute.
disks.
stopAsyncReplication
compute.
disks.
stopGroupAsyncReplication
compute.disks.update
compute.disks.updateKmsKey
compute.disks.use
compute.disks.useReadOnly
compute.
externalVpnGateways.
create
compute.
externalVpnGateways.
createTagBinding
compute.
externalVpnGateways.
delete
compute.
externalVpnGateways.
deleteTagBinding
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.
externalVpnGateways.
setLabels
compute.
externalVpnGateways.
use
compute.
firewallPolicies.
cloneRules
compute.
firewallPolicies.
copyRules
compute.
firewallPolicies.
create
compute.
firewallPolicies.
createTagBinding
compute.
firewallPolicies.
delete
compute.
firewallPolicies.
deleteTagBinding
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewallPolicies.move
compute.
firewallPolicies.
setIamPolicy
compute.
firewallPolicies.
update
compute.firewallPolicies.use
compute.firewalls.create
compute.
firewalls.
createTagBinding
compute.firewalls.delete
compute.
firewalls.
deleteTagBinding
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.firewalls.update
compute.forwardingRules.create
compute.
forwardingRules.
createTagBinding
compute.forwardingRules.delete
compute.
forwardingRules.
deleteTagBinding
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.
forwardingRules.
pscCreate
compute.
forwardingRules.
pscDelete
compute.
forwardingRules.
pscSetLabels
compute.
forwardingRules.
pscUpdate
compute.
forwardingRules.
setLabels
compute.
forwardingRules.
setTarget
compute.forwardingRules.update
compute.forwardingRules.use
compute.
futureReservations.
cancel
compute.
futureReservations.
create
compute.
futureReservations.
delete
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.
futureReservations.
setIamPolicy
compute.
futureReservations.
update
compute.globalAddresses.create
compute.
globalAddresses.
createInternal
compute.
globalAddresses.
createTagBinding
compute.globalAddresses.delete
compute.
globalAddresses.
deleteInternal
compute.
globalAddresses.
deleteTagBinding
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalAddresses.
setLabels
compute.globalAddresses.use
compute.
globalForwardingRules.
create
compute.
globalForwardingRules.
createTagBinding
compute.
globalForwardingRules.
delete
compute.
globalForwardingRules.
deleteTagBinding
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalForwardingRules.
pscCreate
compute.
globalForwardingRules.
pscDelete
compute.
globalForwardingRules.
pscSetLabels
compute.
globalForwardingRules.
pscUpdate
compute.
globalForwardingRules.
setLabels
compute.
globalForwardingRules.
setTarget
compute.
globalForwardingRules.
update
compute.
globalNetworkEndpointGroups.
attachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
create
compute.
globalNetworkEndpointGroups.
createTagBinding
compute.
globalNetworkEndpointGroups.
delete
compute.
globalNetworkEndpointGroups.
deleteTagBinding
compute.
globalNetworkEndpointGroups.
detachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.
globalNetworkEndpointGroups.
use
compute.
globalOperations.
delete
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalOperations.
setIamPolicy
compute.
globalPublicDelegatedPrefixes.
create
compute.
globalPublicDelegatedPrefixes.
delete
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.
globalPublicDelegatedPrefixes.
updatePolicy
compute.healthChecks.create
compute.
healthChecks.
createTagBinding
compute.healthChecks.delete
compute.
healthChecks.
deleteTagBinding
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.healthChecks.update
compute.healthChecks.use
compute.
healthChecks.
useReadOnly
compute.
httpHealthChecks.
create
compute.
httpHealthChecks.
createTagBinding
compute.
httpHealthChecks.
delete
compute.
httpHealthChecks.
deleteTagBinding
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.
httpHealthChecks.
update
compute.httpHealthChecks.use
compute.
httpHealthChecks.
useReadOnly
compute.
httpsHealthChecks.
create
compute.
httpsHealthChecks.
createTagBinding
compute.
httpsHealthChecks.
delete
compute.
httpsHealthChecks.
deleteTagBinding
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.
httpsHealthChecks.
update
compute.httpsHealthChecks.use
compute.
httpsHealthChecks.
useReadOnly
compute.images.create
compute.
images.
createTagBinding
compute.images.delete
compute.
images.
deleteTagBinding
compute.images.deprecate
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.images.setIamPolicy
compute.images.setLabels
compute.images.update
compute.images.useReadOnly
compute.
instanceGroupManagers.
create
compute.
instanceGroupManagers.
createTagBinding
compute.
instanceGroupManagers.
delete
compute.
instanceGroupManagers.
deleteTagBinding
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.
instanceGroupManagers.
update
compute.
instanceGroupManagers.
use
compute.instanceGroups.create
compute.
instanceGroups.
createTagBinding
compute.instanceGroups.delete
compute.
instanceGroups.
deleteTagBinding
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceGroups.update
compute.instanceGroups.use
compute.instanceSettings.get
compute.
instanceSettings.
update
compute.
instanceTemplates.
create
compute.
instanceTemplates.
delete
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.
instanceTemplates.
setIamPolicy
compute.
instanceTemplates.
useReadOnly
compute.
instances.
addAccessConfig
compute.
instances.
addNetworkInterface
compute.
instances.
addResourcePolicies
compute.instances.attachDisk
compute.instances.create
compute.
instances.
createTagBinding
compute.instances.delete
compute.
instances.
deleteAccessConfig
compute.
instances.
deleteNetworkInterface
compute.
instances.
deleteTagBinding
compute.instances.detachDisk
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instances.osAdminLogin
compute.instances.osLogin
compute.
instances.
pscInterfaceCreate
compute.
instances.
removeResourcePolicies
compute.instances.reset
compute.instances.resume
compute.
instances.
sendDiagnosticInterrupt
compute.
instances.
setDeletionProtection
compute.
instances.
setDiskAutoDelete
compute.instances.setIamPolicy
compute.instances.setLabels
compute.
instances.
setMachineResources
compute.
instances.
setMachineType
compute.instances.setMetadata
compute.
instances.
setMinCpuPlatform
compute.instances.setName
compute.
instances.
setScheduling
compute.
instances.
setSecurityPolicy
compute.
instances.
setServiceAccount
compute.
instances.
setShieldedInstanceIntegrityPolicy
compute.
instances.
setShieldedVmIntegrityPolicy
compute.instances.setTags
compute.
instances.
simulateMaintenanceEvent
compute.instances.start
compute.
instances.
startWithEncryptionKey
compute.instances.stop
compute.instances.suspend
compute.instances.update
compute.
instances.
updateAccessConfig
compute.
instances.
updateDisplayDevice
compute.
instances.
updateNetworkInterface
compute.
instances.
updateSecurity
compute.
instances.
updateShieldedInstanceConfig
compute.
instances.
updateShieldedVmConfig
compute.instances.use
compute.instances.useReadOnly
compute.
instantSnapshots.
create
compute.
instantSnapshots.
delete
compute.
instantSnapshots.
export
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
instantSnapshots.
setIamPolicy
compute.
instantSnapshots.
setLabels
compute.
instantSnapshots.
useReadOnly
compute.
interconnectAttachmentGroups.
create
compute.
interconnectAttachmentGroups.
delete
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachmentGroups.
patch
compute.
interconnectAttachments.
create
compute.
interconnectAttachments.
createTagBinding
compute.
interconnectAttachments.
delete
compute.
interconnectAttachments.
deleteTagBinding
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.
interconnectAttachments.
setLabels
compute.
interconnectAttachments.
update
compute.
interconnectAttachments.
use
compute.
interconnectGroups.
create
compute.
interconnectGroups.
delete
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectGroups.
patch
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.create
compute.
interconnects.
createTagBinding
compute.interconnects.delete
compute.
interconnects.
deleteTagBinding
compute.interconnects.get
compute.
interconnects.
getMacsecConfig
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.
interconnects.
setLabels
compute.interconnects.update
compute.interconnects.use
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.
licenseCodes.
setIamPolicy
compute.licenses.create
compute.licenses.delete
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.licenses.setIamPolicy
compute.licenses.update
compute.machineImages.create
compute.machineImages.delete
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.
machineImages.
setIamPolicy
compute.
machineImages.
setLabels
compute.
machineImages.
useReadOnly
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.create
compute.multiMig.delete
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.
networkAttachments.
create
compute.
networkAttachments.
createTagBinding
compute.
networkAttachments.
delete
compute.
networkAttachments.
deleteTagBinding
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkAttachments.
setIamPolicy
compute.
networkAttachments.
update
compute.networkAttachments.use
compute.
networkEdgeSecurityServices.
create
compute.
networkEdgeSecurityServices.
createTagBinding
compute.
networkEdgeSecurityServices.
delete
compute.
networkEdgeSecurityServices.
deleteTagBinding
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEdgeSecurityServices.
update
compute.
networkEndpointGroups.
attachNetworkEndpoints
compute.
networkEndpointGroups.
create
compute.
networkEndpointGroups.
createTagBinding
compute.
networkEndpointGroups.
delete
compute.
networkEndpointGroups.
deleteTagBinding
compute.
networkEndpointGroups.
detachNetworkEndpoints
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.
networkEndpointGroups.
use
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.access
compute.networks.addPeering
compute.networks.create
compute.
networks.
createTagBinding
compute.networks.delete
compute.
networks.
deleteTagBinding
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.networks.mirror
compute.networks.removePeering
compute.
networks.
setFirewallPolicy
compute.
networks.
switchToCustomMode
compute.networks.update
compute.networks.updatePeering
compute.networks.updatePolicy
compute.networks.use
compute.networks.useExternalIp
compute.nodeGroups.addNodes
compute.nodeGroups.create
compute.nodeGroups.delete
compute.nodeGroups.deleteNodes
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.
nodeGroups.
performMaintenance
compute.
nodeGroups.
setIamPolicy
compute.
nodeGroups.
setNodeTemplate
compute.
nodeGroups.
simulateMaintenanceEvent
compute.nodeGroups.update
compute.nodeTemplates.create
compute.nodeTemplates.delete
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.
nodeTemplates.
setIamPolicy
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
disableXpnHost
compute.
organizations.
disableXpnResource
compute.
organizations.
enableXpnHost
compute.
organizations.
enableXpnResource
compute.
organizations.
listAssociations
compute.
organizations.
setFirewallPolicy
compute.
organizations.
setSecurityPolicy
compute.
oslogin.
updateExternalUser
compute.
packetMirrorings.
create
compute.
packetMirrorings.
createTagBinding
compute.
packetMirrorings.
delete
compute.
packetMirrorings.
deleteTagBinding
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.
packetMirrorings.
update
compute.previewFeatures.get
compute.previewFeatures.list
compute.previewFeatures.update
compute.projects.get
compute.
projects.
setCloudArmorTier
compute.
projects.
setCommonInstanceMetadata
compute.
projects.
setDefaultNetworkTier
compute.
projects.
setDefaultServiceAccount
compute.
projects.
setManagedProtectionTier
compute.
projects.
setUsageExportBucket
compute.
publicAdvertisedPrefixes.
create
compute.
publicAdvertisedPrefixes.
delete
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicAdvertisedPrefixes.
update
compute.
publicAdvertisedPrefixes.
updatePolicy
compute.
publicDelegatedPrefixes.
announce
compute.
publicDelegatedPrefixes.
create
compute.
publicDelegatedPrefixes.
createTagBinding
compute.
publicDelegatedPrefixes.
delete
compute.
publicDelegatedPrefixes.
deleteTagBinding
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
publicDelegatedPrefixes.
update
compute.
publicDelegatedPrefixes.
updatePolicy
compute.
publicDelegatedPrefixes.
use
compute.
publicDelegatedPrefixes.
withdraw
compute.
regionBackendBuckets.
create
compute.
regionBackendBuckets.
createTagBinding
compute.
regionBackendBuckets.
delete
compute.
regionBackendBuckets.
deleteTagBinding
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendBuckets.
setIamPolicy
compute.
regionBackendBuckets.
update
compute.
regionBackendBuckets.
use
compute.
regionBackendServices.
create
compute.
regionBackendServices.
createTagBinding
compute.
regionBackendServices.
delete
compute.
regionBackendServices.
deleteTagBinding
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionBackendServices.
setIamPolicy
compute.
regionBackendServices.
setSecurityPolicy
compute.
regionBackendServices.
update
compute.
regionBackendServices.
use
compute.
regionCompositeHealthChecks.
create
compute.
regionCompositeHealthChecks.
delete
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionCompositeHealthChecks.
update
compute.
regionFirewallPolicies.
cloneRules
compute.
regionFirewallPolicies.
create
compute.
regionFirewallPolicies.
createTagBinding
compute.
regionFirewallPolicies.
delete
compute.
regionFirewallPolicies.
deleteTagBinding
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionFirewallPolicies.
setIamPolicy
compute.
regionFirewallPolicies.
update
compute.
regionFirewallPolicies.
use
compute.
regionHealthAggregationPolicies.
create
compute.
regionHealthAggregationPolicies.
delete
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthAggregationPolicies.
update
compute.
regionHealthCheckServices.
create
compute.
regionHealthCheckServices.
delete
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.
regionHealthCheckServices.
update
compute.
regionHealthCheckServices.
use
compute.
regionHealthChecks.
create
compute.
regionHealthChecks.
createTagBinding
compute.
regionHealthChecks.
delete
compute.
regionHealthChecks.
deleteTagBinding
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthChecks.
update
compute.regionHealthChecks.use
compute.
regionHealthChecks.
useReadOnly
compute.
regionHealthSources.
create
compute.
regionHealthSources.
delete
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionHealthSources.
update
compute.
regionNetworkEndpointGroups.
attachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
create
compute.
regionNetworkEndpointGroups.
createTagBinding
compute.
regionNetworkEndpointGroups.
delete
compute.
regionNetworkEndpointGroups.
deleteTagBinding
compute.
regionNetworkEndpointGroups.
detachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNetworkEndpointGroups.
use
compute.
regionNotificationEndpoints.
create
compute.
regionNotificationEndpoints.
delete
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.
regionNotificationEndpoints.
update
compute.
regionNotificationEndpoints.
use
compute.
regionOperations.
delete
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionOperations.
setIamPolicy
compute.
regionSecurityPolicies.
create
compute.
regionSecurityPolicies.
createTagBinding
compute.
regionSecurityPolicies.
delete
compute.
regionSecurityPolicies.
deleteTagBinding
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSecurityPolicies.
update
compute.
regionSecurityPolicies.
use
compute.
regionSslCertificates.
create
compute.
regionSslCertificates.
createTagBinding
compute.
regionSslCertificates.
delete
compute.
regionSslCertificates.
deleteTagBinding
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.
regionSslPolicies.
create
compute.
regionSslPolicies.
createTagBinding
compute.
regionSslPolicies.
delete
compute.
regionSslPolicies.
deleteTagBinding
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionSslPolicies.
update
compute.regionSslPolicies.use
compute.
regionTargetHttpProxies.
create
compute.
regionTargetHttpProxies.
createTagBinding
compute.
regionTargetHttpProxies.
delete
compute.
regionTargetHttpProxies.
deleteTagBinding
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpProxies.
setUrlMap
compute.
regionTargetHttpProxies.
use
compute.
regionTargetHttpsProxies.
create
compute.
regionTargetHttpsProxies.
createTagBinding
compute.
regionTargetHttpsProxies.
delete
compute.
regionTargetHttpsProxies.
deleteTagBinding
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
setSslCertificates
compute.
regionTargetHttpsProxies.
setUrlMap
compute.
regionTargetHttpsProxies.
update
compute.
regionTargetHttpsProxies.
use
compute.
regionTargetTcpProxies.
create
compute.
regionTargetTcpProxies.
createTagBinding
compute.
regionTargetTcpProxies.
delete
compute.
regionTargetTcpProxies.
deleteTagBinding
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.
regionTargetTcpProxies.
use
compute.regionUrlMaps.create
compute.
regionUrlMaps.
createTagBinding
compute.regionUrlMaps.delete
compute.
regionUrlMaps.
deleteTagBinding
compute.regionUrlMaps.get
compute.
regionUrlMaps.
invalidateCache
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.update
compute.regionUrlMaps.use
compute.regionUrlMaps.validate
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationBlocks.
performMaintenance
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.
reservationSubBlocks.
performMaintenance
compute.
reservationSubBlocks.
reportFaulty
compute.reservations.create
compute.reservations.delete
compute.reservations.get
compute.reservations.list
compute.
reservations.
performMaintenance
compute.reservations.resize
compute.reservations.update
compute.
resourcePolicies.
create
compute.
resourcePolicies.
delete
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.
resourcePolicies.
setIamPolicy
compute.
resourcePolicies.
update
compute.resourcePolicies.use
compute.
resourcePolicies.
useReadOnly
compute.rolloutPlans.create
compute.rolloutPlans.delete
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.cancel
compute.rollouts.delete
compute.rollouts.get
compute.rollouts.list
compute.routers.create
compute.
routers.
createTagBinding
compute.routers.delete
compute.
routers.
deleteRoutePolicy
compute.
routers.
deleteTagBinding
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routers.update
compute.
routers.
updateRoutePolicy
compute.routers.use
compute.routes.create
compute.
routes.
createTagBinding
compute.routes.delete
compute.
routes.
deleteTagBinding
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.
securityPolicies.
addAssociation
compute.
securityPolicies.
copyRules
compute.
securityPolicies.
create
compute.
securityPolicies.
createTagBinding
compute.
securityPolicies.
delete
compute.
securityPolicies.
deleteTagBinding
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.securityPolicies.move
compute.
securityPolicies.
removeAssociation
compute.
securityPolicies.
setLabels
compute.
securityPolicies.
update
compute.securityPolicies.use
compute.
serviceAttachments.
create
compute.
serviceAttachments.
createTagBinding
compute.
serviceAttachments.
delete
compute.
serviceAttachments.
deleteTagBinding
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.
serviceAttachments.
setIamPolicy
compute.
serviceAttachments.
update
compute.serviceAttachments.use
compute.snapshotSettings.get
compute.
snapshotSettings.
update
compute.snapshots.create
compute.
snapshots.
createTagBinding
compute.snapshots.delete
compute.
snapshots.
deleteTagBinding
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.snapshots.setIamPolicy
compute.snapshots.setLabels
compute.snapshots.updateKmsKey
compute.snapshots.useReadOnly
compute.spotAssistants.get
compute.sslCertificates.create
compute.
sslCertificates.
createTagBinding
compute.sslCertificates.delete
compute.
sslCertificates.
deleteTagBinding
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.create
compute.
sslPolicies.
createTagBinding
compute.sslPolicies.delete
compute.
sslPolicies.
deleteTagBinding
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.sslPolicies.update
compute.sslPolicies.use
compute.storagePools.create
compute.storagePools.delete
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.
storagePools.
setIamPolicy
compute.storagePools.update
compute.storagePools.use
compute.subnetworks.create
compute.
subnetworks.
createTagBinding
compute.subnetworks.delete
compute.
subnetworks.
deleteTagBinding
compute.
subnetworks.
expandIpCidrRange
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.subnetworks.mirror
compute.
subnetworks.
setIamPolicy
compute.
subnetworks.
setPrivateIpGoogleAccess
compute.subnetworks.update
compute.subnetworks.use
compute.
subnetworks.
useExternalIp
compute.
subnetworks.
usePeerMigration
compute.
targetGrpcProxies.
create
compute.
targetGrpcProxies.
createTagBinding
compute.
targetGrpcProxies.
delete
compute.
targetGrpcProxies.
deleteTagBinding
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.
targetGrpcProxies.
update
compute.targetGrpcProxies.use
compute.
targetHttpProxies.
create
compute.
targetHttpProxies.
createTagBinding
compute.
targetHttpProxies.
delete
compute.
targetHttpProxies.
deleteTagBinding
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.
targetHttpProxies.
setUrlMap
compute.
targetHttpProxies.
update
compute.targetHttpProxies.use
compute.
targetHttpsProxies.
create
compute.
targetHttpsProxies.
createTagBinding
compute.
targetHttpsProxies.
delete
compute.
targetHttpsProxies.
deleteTagBinding
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.
targetHttpsProxies.
setCertificateMap
compute.
targetHttpsProxies.
setQuicOverride
compute.
targetHttpsProxies.
setSslCertificates
compute.
targetHttpsProxies.
setSslPolicy
compute.
targetHttpsProxies.
setUrlMap
compute.
targetHttpsProxies.
update
compute.targetHttpsProxies.use
compute.targetInstances.create
compute.
targetInstances.
createTagBinding
compute.targetInstances.delete
compute.
targetInstances.
deleteTagBinding
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.
targetInstances.
setSecurityPolicy
compute.targetInstances.use
compute.
targetPools.
addHealthCheck
compute.
targetPools.
addInstance
compute.targetPools.create
compute.
targetPools.
createTagBinding
compute.targetPools.delete
compute.
targetPools.
deleteTagBinding
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.
targetPools.
removeHealthCheck
compute.
targetPools.
removeInstance
compute.
targetPools.
setSecurityPolicy
compute.targetPools.update
compute.targetPools.use
compute.
targetSslProxies.
create
compute.
targetSslProxies.
createTagBinding
compute.
targetSslProxies.
delete
compute.
targetSslProxies.
deleteTagBinding
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.
targetSslProxies.
setBackendService
compute.
targetSslProxies.
setCertificateMap
compute.
targetSslProxies.
setProxyHeader
compute.
targetSslProxies.
setSslCertificates
compute.
targetSslProxies.
setSslPolicy
compute.
targetSslProxies.
update
compute.targetSslProxies.use
compute.
targetTcpProxies.
create
compute.
targetTcpProxies.
createTagBinding
compute.
targetTcpProxies.
delete
compute.
targetTcpProxies.
deleteTagBinding
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.
targetTcpProxies.
update
compute.targetTcpProxies.use
compute.
targetVpnGateways.
create
compute.
targetVpnGateways.
createTagBinding
compute.
targetVpnGateways.
delete
compute.
targetVpnGateways.
deleteTagBinding
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.
targetVpnGateways.
setLabels
compute.targetVpnGateways.use
compute.urlMaps.create
compute.
urlMaps.
createTagBinding
compute.urlMaps.delete
compute.
urlMaps.
deleteTagBinding
compute.urlMaps.get
compute.
urlMaps.
invalidateCache
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.update
compute.urlMaps.use
compute.urlMaps.validate
compute.vpnGateways.create
compute.
vpnGateways.
createTagBinding
compute.vpnGateways.delete
compute.
vpnGateways.
deleteTagBinding
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnGateways.setLabels
compute.vpnGateways.use
compute.vpnTunnels.create
compute.
vpnTunnels.
createTagBinding
compute.vpnTunnels.delete
compute.
vpnTunnels.
deleteTagBinding
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.vpnTunnels.setLabels
compute.wireGroups.create
compute.wireGroups.delete
compute.wireGroups.get
compute.wireGroups.list
compute.wireGroups.update
compute.zoneOperations.delete
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.
zoneOperations.
setIamPolicy
compute.zones.get
compute.zones.list
notebooks.*
notebooks.environments.create
notebooks.environments.delete
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.
environments.
setIamPolicy
notebooks.executions.create
notebooks.executions.delete
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
executions.
setIamPolicy
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.delete
notebooks.instances.diagnose
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.instances.reset
notebooks.
instances.
setAccelerator
notebooks.
instances.
setIamPolicy
notebooks.instances.setLabels
notebooks.
instances.
setMachineType
notebooks.instances.start
notebooks.instances.stop
notebooks.instances.update
notebooks.
instances.
updateConfig
notebooks.
instances.
updateShieldInstanceConfig
notebooks.instances.upgrade
notebooks.instances.use
notebooks.locations.get
notebooks.locations.list
notebooks.operations.cancel
notebooks.operations.delete
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.delete
notebooks.runtimes.diagnose
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.runtimes.reset
notebooks.
runtimes.
setIamPolicy
notebooks.runtimes.start
notebooks.runtimes.stop
notebooks.runtimes.switch
notebooks.runtimes.update
notebooks.runtimes.upgrade
notebooks.schedules.create
notebooks.schedules.delete
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
notebooks.
schedules.
setIamPolicy
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Legacy Viewer
(
roles/
notebooks.legacyViewer
)
Read-only access to Notebooks all resources through compute API.
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
instances.
checkUpgradability
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.locations.*
notebooks.locations.get
notebooks.locations.list
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Runner
(
roles/
notebooks.runner
)
Restricted access for running scheduled Notebooks.
aiplatform.
notebookExecutionJobs.*
aiplatform.
notebookExecutionJobs.
create
aiplatform.
notebookExecutionJobs.
delete
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.operations.list
aiplatform.pipelineJobs.create
aiplatform.schedules.*
aiplatform.schedules.create
aiplatform.schedules.delete
aiplatform.schedules.get
aiplatform.schedules.list
aiplatform.schedules.update
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.executions.create
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.locations.*
notebooks.locations.get
notebooks.locations.list
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.schedules.create
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
AI Platform Notebooks Service Agent
(
roles/
notebooks.serviceAgent
)
Provide access for notebooks service agent to manage notebook instances in user projects
Warning:
Do not grant service agent roles to any principals except
service agents
.
aiplatform.customJobs.cancel
aiplatform.customJobs.create
aiplatform.customJobs.get
aiplatform.customJobs.list
aiplatform.
notebookExecutionJobs.*
aiplatform.
notebookExecutionJobs.
create
aiplatform.
notebookExecutionJobs.
delete
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.operations.list
aiplatform.pipelineJobs.create
aiplatform.schedules.*
aiplatform.schedules.create
aiplatform.schedules.delete
aiplatform.schedules.get
aiplatform.schedules.list
aiplatform.schedules.update
backupdr.
backupPlanAssociations.
createForComputeDisk
backupdr.
backupPlanAssociations.
createForComputeInstance
backupdr.
backupPlanAssociations.
deleteForComputeDisk
backupdr.
backupPlanAssociations.
deleteForComputeInstance
backupdr.
backupPlanAssociations.
fetchForComputeDisk
backupdr.
backupPlanAssociations.
getForComputeDisk
backupdr.
backupPlanAssociations.
list
backupdr.
backupPlanAssociations.
triggerBackupForComputeDisk
backupdr.
backupPlanAssociations.
triggerBackupForComputeInstance
backupdr.
backupPlanAssociations.
updateForComputeDisk
backupdr.
backupPlanAssociations.
updateForComputeInstance
backupdr.backupPlans.get
backupdr.backupPlans.list
backupdr.
backupPlans.
useForComputeDisk
backupdr.
backupPlans.
useForComputeInstance
backupdr.backupVaults.get
backupdr.backupVaults.list
backupdr.locations.list
backupdr.operations.get
backupdr.operations.list
backupdr.
serviceConfig.
initialize
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.
addresses.
createInternal
compute.
addresses.
deleteInternal
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.addresses.use
compute.addresses.useInternal
compute.autoscalers.*
compute.autoscalers.create
compute.autoscalers.delete
compute.autoscalers.get
compute.autoscalers.list
compute.autoscalers.update
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.*
compute.
disks.
addResourcePolicies
compute.disks.create
compute.disks.createSnapshot
compute.disks.createTagBinding
compute.disks.delete
compute.disks.deleteTagBinding
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
disks.
removeResourcePolicies
compute.disks.resize
compute.disks.setIamPolicy
compute.disks.setLabels
compute.
disks.
startAsyncReplication
compute.
disks.
stopAsyncReplication
compute.
disks.
stopGroupAsyncReplication
compute.disks.update
compute.disks.updateKmsKey
compute.disks.use
compute.disks.useReadOnly
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.globalAddresses.use
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.*
compute.
globalNetworkEndpointGroups.
attachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
create
compute.
globalNetworkEndpointGroups.
createTagBinding
compute.
globalNetworkEndpointGroups.
delete
compute.
globalNetworkEndpointGroups.
deleteTagBinding
compute.
globalNetworkEndpointGroups.
detachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.
globalNetworkEndpointGroups.
use
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.*
compute.images.create
compute.
images.
createTagBinding
compute.images.delete
compute.
images.
deleteTagBinding
compute.images.deprecate
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.images.setIamPolicy
compute.images.setLabels
compute.images.update
compute.images.useReadOnly
compute.
instanceGroupManagers.*
compute.
instanceGroupManagers.
create
compute.
instanceGroupManagers.
createTagBinding
compute.
instanceGroupManagers.
delete
compute.
instanceGroupManagers.
deleteTagBinding
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.
instanceGroupManagers.
update
compute.
instanceGroupManagers.
use
compute.instanceGroups.*
compute.instanceGroups.create
compute.
instanceGroups.
createTagBinding
compute.instanceGroups.delete
compute.
instanceGroups.
deleteTagBinding
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceGroups.update
compute.instanceGroups.use
compute.instanceSettings.*
compute.instanceSettings.get
compute.
instanceSettings.
update
compute.instanceTemplates.*
compute.
instanceTemplates.
create
compute.
instanceTemplates.
delete
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.
instanceTemplates.
setIamPolicy
compute.
instanceTemplates.
useReadOnly
compute.instances.*
compute.
instances.
addAccessConfig
compute.
instances.
addNetworkInterface
compute.
instances.
addResourcePolicies
compute.instances.attachDisk
compute.instances.create
compute.
instances.
createTagBinding
compute.instances.delete
compute.
instances.
deleteAccessConfig
compute.
instances.
deleteNetworkInterface
compute.
instances.
deleteTagBinding
compute.instances.detachDisk
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instances.osAdminLogin
compute.instances.osLogin
compute.
instances.
pscInterfaceCreate
compute.
instances.
removeResourcePolicies
compute.instances.reset
compute.instances.resume
compute.
instances.
sendDiagnosticInterrupt
compute.
instances.
setDeletionProtection
compute.
instances.
setDiskAutoDelete
compute.instances.setIamPolicy
compute.instances.setLabels
compute.
instances.
setMachineResources
compute.
instances.
setMachineType
compute.instances.setMetadata
compute.
instances.
setMinCpuPlatform
compute.instances.setName
compute.
instances.
setScheduling
compute.
instances.
setSecurityPolicy
compute.
instances.
setServiceAccount
compute.
instances.
setShieldedInstanceIntegrityPolicy
compute.
instances.
setShieldedVmIntegrityPolicy
compute.instances.setTags
compute.
instances.
simulateMaintenanceEvent
compute.instances.start
compute.
instances.
startWithEncryptionKey
compute.instances.stop
compute.instances.suspend
compute.instances.update
compute.
instances.
updateAccessConfig
compute.
instances.
updateDisplayDevice
compute.
instances.
updateNetworkInterface
compute.
instances.
updateSecurity
compute.
instances.
updateShieldedInstanceConfig
compute.
instances.
updateShieldedVmConfig
compute.instances.use
compute.instances.useReadOnly
compute.instantSnapshots.*
compute.
instantSnapshots.
create
compute.
instantSnapshots.
delete
compute.
instantSnapshots.
export
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
instantSnapshots.
setIamPolicy
compute.
instantSnapshots.
setLabels
compute.
instantSnapshots.
useReadOnly
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.*
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.
licenseCodes.
setIamPolicy
compute.licenses.*
compute.licenses.create
compute.licenses.delete
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.licenses.setIamPolicy
compute.licenses.update
compute.machineImages.*
compute.machineImages.create
compute.machineImages.delete
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.
machineImages.
setIamPolicy
compute.
machineImages.
setLabels
compute.
machineImages.
useReadOnly
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.*
compute.multiMig.create
compute.multiMig.delete
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.*
compute.
networkEndpointGroups.
attachNetworkEndpoints
compute.
networkEndpointGroups.
create
compute.
networkEndpointGroups.
createTagBinding
compute.
networkEndpointGroups.
delete
compute.
networkEndpointGroups.
deleteTagBinding
compute.
networkEndpointGroups.
detachNetworkEndpoints
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.
networkEndpointGroups.
use
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.networks.use
compute.networks.useExternalIp
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
projects.
setCommonInstanceMetadata
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.*
compute.
regionNetworkEndpointGroups.
attachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
create
compute.
regionNetworkEndpointGroups.
createTagBinding
compute.
regionNetworkEndpointGroups.
delete
compute.
regionNetworkEndpointGroups.
deleteTagBinding
compute.
regionNetworkEndpointGroups.
detachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNetworkEndpointGroups.
use
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.*
compute.
resourcePolicies.
create
compute.
resourcePolicies.
delete
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.
resourcePolicies.
setIamPolicy
compute.
resourcePolicies.
update
compute.resourcePolicies.use
compute.
resourcePolicies.
useReadOnly
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.*
compute.snapshots.create
compute.
snapshots.
createTagBinding
compute.snapshots.delete
compute.
snapshots.
deleteTagBinding
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.snapshots.setIamPolicy
compute.snapshots.setLabels
compute.snapshots.updateKmsKey
compute.snapshots.useReadOnly
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.storagePools.use
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.subnetworks.use
compute.
subnetworks.
useExternalIp
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
dataproc.clusters.get
dataproc.clusters.use
dataproc.jobs.cancel
dataproc.jobs.create
dataproc.jobs.delete
dataproc.jobs.get
dataproc.jobs.list
dataproc.jobs.update
iam.serviceAccounts.actAs
iam.serviceAccounts.get
iam.
serviceAccounts.
getAccessToken
iam.serviceAccounts.list
ml.jobs.create
ml.jobs.get
ml.jobs.list
notebooks.*
notebooks.environments.create
notebooks.environments.delete
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.
environments.
setIamPolicy
notebooks.executions.create
notebooks.executions.delete
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
executions.
setIamPolicy
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.delete
notebooks.instances.diagnose
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.instances.reset
notebooks.
instances.
setAccelerator
notebooks.
instances.
setIamPolicy
notebooks.instances.setLabels
notebooks.
instances.
setMachineType
notebooks.instances.start
notebooks.instances.stop
notebooks.instances.update
notebooks.
instances.
updateConfig
notebooks.
instances.
updateShieldInstanceConfig
notebooks.instances.upgrade
notebooks.instances.use
notebooks.locations.get
notebooks.locations.list
notebooks.operations.cancel
notebooks.operations.delete
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.delete
notebooks.runtimes.diagnose
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.runtimes.reset
notebooks.
runtimes.
setIamPolicy
notebooks.runtimes.start
notebooks.runtimes.stop
notebooks.runtimes.switch
notebooks.runtimes.update
notebooks.runtimes.upgrade
notebooks.schedules.create
notebooks.schedules.delete
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
notebooks.
schedules.
setIamPolicy
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Viewer
(
roles/
notebooks.viewer
)
Read-only access to Notebooks, all resources.
Lowest-level resources where you can grant this role:
Instance
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.schedules.get
aiplatform.schedules.list
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
instances.
checkUpgradability
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.locations.*
notebooks.locations.get
notebooks.locations.list
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Basic roles
The older Google Cloud
basic roles
are common to all Google Cloud services. These roles are Owner, Editor,
and Viewer.
The basic roles provide permissions across Google Cloud, not just for
Vertex AI Workbench. For this reason, you should
use Vertex AI Workbench roles whenever possible.
Custom roles
If the predefined IAM roles for Vertex AI Workbench
don't meet your needs, you can define custom roles. Custom roles enable you
to choose a specific set of permissions, create your own role with
those permissions, and grant the role to users in your organization.
For more information, see
Understanding
IAM custom roles
.
Project-level access versus resource-level policies
A resource inherits all policies from its
ancestry
.
A
policy
set at the resource level doesn't
affect project-level policies.  You can use project-level access and
resource-level policies to customize permissions.
For example, you can grant users
roles/notebooks.viewer
permissions
at the project level so that they can view all
Vertex AI Workbench resources in the project,
and then you can grant each user
roles/notebooks.admin
permissions
on a specific Vertex AI Workbench instance so that they
have all of the
admin
abilities to administer that instance.
Not all Vertex AI Workbench predefined roles and resources support
resource-level policies. To see which roles can be used on which resources,
view the descriptions
for each role.
Changes to the ability to access a resource take time to propagate. For more
information, see
Access change
propagation
.
What's next
Grant a principal access to
a Vertex AI Workbench instance.
Grant a principal access to
JupyterLab.
Learn more about
IAM
.
Learn how to
create and manage custom IAM
roles
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/idle-shutdown.txt
Idle shutdown  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Idle shutdown
Vertex AI Workbench instances
shut down after a specified period of inactivity by default.
This page describes the idle shutdown feature and how to
change the default idle shutdown settings during instance creation.
Overview
To help manage costs, Vertex AI Workbench instances
shut down after being idle for a specific time period by default.
You can change the amount of time or turn this feature off.
Requirements for running idle shutdown
For idle shutdown to run, your Vertex AI Workbench instance
must have guest attributes enabled. Guest attributes are enabled by default
but if you've turned off guest attributes, you can enable guest attributes
by setting the
enable-guest-attributes
metadata key to
true
. See
Update an instance's metadata
.
Billing
While your instance is shut down, there are no CPU or GPU
usage charges except for scheduled executions that run during
the shutdown. For more information about scheduled executions, see
Scheduled executions run while instance is shut down
on this page.
Disk storage charges still apply while
your instance is shut down. For more information,
see
Pricing
.
Turn off idle shutdown or change the default inactivity time period
Idle shutdown is enabled and set to shut down your instance after
180 inactive minutes by default.
You can change these settings when you
create an instance
.
To turn off idle shutdown or to change the inactivity time period
on an existing instance:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click the instance name.
On the
Instance details
page, click the
Software and security
tab.
In the
Modify software and security configuration
section:
To turn off idle shutdown, clear the check mark next to
Enable Idle Shutdown
.
To change the inactivity time period,
in
Time of inactivity before shutdown (Minutes)
, change
the number to the number of minutes of inactivity that you want.
In the Google Cloud console, this setting
can be set to any integer value from 10 to 1440.
Click
Submit
.
Manage idle shutdown using the gcloud CLI
Idle shutdown for Vertex AI Workbench instances is managed using
the
metadata
flag. To enable idle shutdown,
create an instance with the
idle-timeout-seconds
key in the metadata
with the value set to the number of seconds.
gcloud
workbench
instances
create
INSTANCE_NAME
--metadata
=
idle-timeout-seconds
=
86400
To change the idle shutdown time period, update the value for the
idle-timeout-seconds
key in the metadata.
gcloud
workbench
instances
update
INSTANCE_NAME
--metadata
=
idle-timeout-seconds
=
43200
To turn off idle shutdown, use the following command:
gcloud
workbench
instances
update
INSTANCE_NAME
--metadata
=
idle-timeout-seconds
=
Manage idle shutdown on Terraform
Idle shutdown for workbench instances on Terraform is managed using the
idle-timeout-seconds
key in the metadata field.
To learn how to apply or remove a Terraform configuration, see
Basic Terraform commands
.
resource "google_workbench_instance" "default" {
  name     = "workbench-instance-example"
  location = "us-central1-a"

  gce_setup {
    machine_type = "n1-standard-1"
    vm_image {
      project = "cloud-notebooks-managed"
      family  = "workbench-instances"
    }
    metadata = {
      idle-timeout-seconds = "10800"
    }
  }
}
How idle shutdown works
Your instance shuts down when there is no kernel activity for the
specified time period. For example, running a cell or new output printing
to a notebook is activity that resets the idle shutdown timer. CPU usage
doesn't reset the idle shutdown timer.
If you leave JupyterLab open and you don't interact with the window,
the instance will shut down after the idle shutdown period.
By default, idle shutdown looks for activity in kernels running in
the following addresses of the instance:
127.0.0.1:8080/api/sessions
127.0.0.1:8080/api/terminals
127.0.0.1:8080/api/kernels
Warning:
By default Jupyter runs on port 8080. If this port is changed,
  idle shutdown won't detect any kernel activity.
Scheduled executions run while instance is shut down
If you have scheduled an execution of a notebook file
in a Vertex AI Workbench instance that is shut down,
the execution still runs on schedule.
What's next
To run a notebook file on a schedule, even when your instance is shut down,
see
schedule a notebook
run
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/introduction.txt
Introduction to Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Introduction to Vertex AI Workbench
Vertex AI Workbench instances are Jupyter notebook-based development environments
for the entire data science workflow. You can interact with
Vertex AI and other Google Cloud services from within
a Vertex AI Workbench instance's Jupyter notebook.
Vertex AI Workbench integrations and features can make it easier
to access your data, process data faster, schedule notebook runs, and more.
Vertex AI Workbench instances are prepackaged with
JupyterLab
and have a preinstalled suite of deep learning packages,
including support for the TensorFlow and PyTorch
frameworks. You can configure either CPU-only or GPU-enabled instances.
Vertex AI Workbench instances support the ability to sync with a
GitHub
repository.
Vertex AI Workbench instances are protected
by Google Cloud authentication and authorization.
Access to data
You can access your data without leaving the JupyterLab user interface.
In JupyterLab's navigation menu on a Vertex AI Workbench instance,
you can use the Cloud Storage integration
to browse data and other files that you have access to.
See
Access Cloud Storage buckets and files
from within JupyterLab
.
You can also use the BigQuery integration
to browse tables that you have access to, write queries, preview results,
and load data into your notebook. See
Query data in BigQuery
tables from within JupyterLab
.
Execute notebook runs
Use the executor to run a notebook file as a one-time execution or
on a schedule. Choose the specific environment and hardware that you want
your execution to run on. Your notebook's code will run on
Vertex AI custom training, which can make it easier
to do distributed training, optimize hyperparameters, or
schedule continuous training jobs.
You can use parameters in your execution to make specific changes to each run.
For example, you might specify a different dataset to use,
change the learning rate on your model, or change the version of the model.
You can also set a notebook to run on a recurring schedule. Even while your
instance is shut down, Vertex AI Workbench will run your notebook file and
save the results for you to look at and share with others. See
Schedule a notebook run
.
Share insights
Executed notebook runs are stored in a Cloud Storage bucket,
so you can share your insights with others by granting access
to the results. See the
previous section on executing
notebook runs
.
Secure your instance
The following sections describe supported capabilities that can help you
secure your Vertex AI Workbench instance.
VPC
You can deploy your Vertex AI Workbench instance
with the default Google-managed network,
which uses a default VPC network and subnet.
Instead of the default network, you can specify a
VPC network to use with your instance.
To use Vertex AI Workbench within a service perimeter, see
Use a Vertex AI Workbench instance within a service
perimeter
.
Customer-managed encryption keys (CMEK)
By default, Google Cloud automatically
encrypts data when it is at
rest
using encryption keys
managed by Google. If you have specific compliance or regulatory requirements
related to the keys that protect your data, you can use customer-managed
encryption keys (CMEK) with your Vertex AI Workbench instances.
For more information,
see
Customer-managed encryption keys
.
Confidential Computing
Preview
This feature is
        
        subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the
Service Specific
        Terms
.
        
        Pre-GA features are available "as is" and might have limited support.
      
      For more information, see the
launch stage descriptions
.
You can encrypt your data-in-use by using Confidential Computing. To use
Confidential Computing, you enable the Confidential VM service
when you create a Vertex AI Workbench instance. To get started,
see
Create an instance with
Confidential Computing
.
Automated shutdown for idle instances
To help manage costs, Vertex AI Workbench instances shut down after
being idle for a specific time period by default. You can change the
amount of time or turn this feature off. For more information,
see
Idle shutdown
.
Add conda environments
Vertex AI Workbench instances use
kernels
based on conda environments. You can add a conda environment to
your Vertex AI Workbench instance, and the environment appears as
a kernel in your instance's JupyterLab interface.
Adding conda environments lets you use kernels that aren't available in the
default Vertex AI Workbench instance.
For example, you can add conda environments for R and Apache Beam. Or you
can add conda environments for specific earlier versions of the available
frameworks, such as TensorFlow, PyTorch, or Python.
For more information, see
Add a conda environment
.
Custom containers
You can create a Vertex AI Workbench instance based on a custom container.
Start with a Google-provided base container image, and modify it for
your needs. Then create an instance based on your custom container.
For more information, see
Create an instance using a
custom container
.
Dataproc integration
You can process data quickly by running a notebook on
a Dataproc cluster. After your cluster is set up, you can run
a notebook file on it without leaving the JupyterLab user interface.
For more information, see
Create a Dataproc-enabled
instance
.
Reserve VM resources
Use
Compute Engine reservations
to gain a high level of assurance that your Vertex AI Workbench instances
have enough virtual machine (VM) resources to run.
Reservations are a Compute Engine feature. They help make sure that
you have the resources available to create VMs with the same hardware
(memory and vCPUs) and optional resources (GPUs and Local SSD disks)
whenever you need them.
For more information, see
Use reservations
.
Create instances with third party credentials
You can create and manage Vertex AI Workbench instances with
third party credentials provided by Workforce Identity Federation.
Workforce Identity Federation uses your external identity provider (IdP)
to grant a group of users access to Vertex AI Workbench instances
through a proxy.
Access to a Vertex AI Workbench instance is granted by assigning a
workforce pool principal
to the Vertex AI Workbench instance's service account.
For more information, see
Create an instance with
third party credentials
.
Tags for Vertex AI Workbench instances
The underlying VM of a Vertex AI Workbench instance is a
Compute Engine VM. You can add and manage resource tags to your
Vertex AI Workbench instance through its Compute Engine VM.
When you create a Vertex AI Workbench instance,
Vertex AI Workbench attaches the Compute Engine resource tag
vertex-workbench-instances:prod=READ_ONLY
. This resource tag is
only used for internal purposes.
To learn more about managing tags for Compute Engine instances,
see
Manage tags for resources
.
Limitations
Consider the following limitations of
Vertex AI Workbench instances when planning your project:
Third party JupyterLab extensions aren't supported.
When you use
Access Context Manager
and
Chrome Enterprise Premium
to protect Vertex AI Workbench instances with context-aware
access controls, access is evaluated each time the user authenticates
to the instance. For example, access is evaluated the first time
the user accesses JupyterLab and whenever they access it thereafter
if their web browser's cookie has expired.
Using a custom container that isn't derived from the
Google-provided base container
(
gcr.io/deeplearning-platform-release/workbench-container:latest
)
increases the risks of compatibility issues with our services and
isn't supported. Instead, modify the base container to create a
custom container that meets your needs, and then
create an instance using
the custom container
.
Vertex AI Workbench instances expect images from the
cloud-notebooks-managed
project. The list of image names is available at
the creation page in the Google Cloud console. Although the use of custom
virtual machine (VM) images or
Deep Learning VM
images with Vertex AI Workbench instances can be possible,
Vertex AI Workbench doesn't provide any support for unexpected
behaviors or malfunctions when using those images.
The use of a user-managed notebooks image or
managed notebooks image to create a
Vertex AI Workbench instance isn't supported.
You can't edit the underlying VM of a Vertex AI Workbench instance
by using the Google Cloud console or the Compute Engine API. To edit a
Vertex AI Workbench instance's underlying VM, use the
projects.locations.instances.patch
method in the Notebooks API or the
gcloud workbench instances update
command in the Google Cloud SDK.
In instances that use VPC Service Controls, use of the
executor
isn't
supported.
To use accelerators with Vertex AI Workbench instances,
the accelerator type that you want must be available in your instance's
zone. To learn about accelerator availability by zone, see
GPU regions and zones availability
.
What's next
Create a Vertex AI Workbench instance
.
Compare Vertex AI's
notebook solutions
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/manage-access-jupyterlab.txt
Manage access to a Vertex AI Workbench instance's JupyterLab interface  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Manage access to an instance's JupyterLab interface
This page describes how to grant access to the JupyterLab interface
of a Vertex AI Workbench instance.
You control access to a Vertex AI Workbench instance's
JupyterLab interface through the instance's access mode.
You set a JupyterLab access mode when you create
a Vertex AI Workbench instance.
The access mode can't be changed after the notebook is created.
The JupyterLab access mode determines who can use
the instance's JupyterLab interface.
The access mode also determines which credentials are used when
your instance interacts with other Google Cloud services.
Access limitations
Granting a principal access to
a Vertex AI Workbench instance's JupyterLab interface
doesn't grant access to the instance itself. For example,
to start, stop, or reset an instance, you must grant the principal
access to perform those operations by setting an
IAM policy
on the instance.
To grant access to the Vertex AI Workbench instance,
see
Manage access to
a Vertex AI Workbench instance
.
JupyterLab access modes
Vertex AI Workbench instances support the
following access modes:
Single user only
: The
Single user only
access mode
grants access only to the user that you specify.
Service account
: The
Service account
access mode
grants access to a service account. You can grant access to one or more
users through this service account.
Note:
To grant access to the instance through the single user option
      or the service account, you must use an individual's
      user account email address. Group access is not supported.
Single user only
When you create a Vertex AI Workbench instance
with
Single user only
access, you specify a user account.
The specified user account is the only user with access to
the JupyterLab interface. If the specified user is not the creator of the
instance, you must grant the specified user the
Service Account User role
(
roles/iam.serviceAccountUser
) on the instance's service account. If the
instance needs to access other Google Cloud resources, this
service account
must also have access to those Google Cloud resources.
Note:
When you create a Vertex AI Workbench instance
with
Single user only
access, your instance completes the boot process
using the Compute Engine default service account.
Your specified user account can access the instance after the boot process
is finished.
Grant access to a single user
To grant access to a single user, complete the following steps.
Create
a Vertex AI Workbench instance
with the following specifications:
In the
Create instance
dialog, in
the
IAM and security
section, select the
Single user only
access mode.
In the
User email
field, enter the user account that you want
to grant access.
Complete the rest of the dialog, and then click
Create
.
Service account
When you create a Vertex AI Workbench instance
with
Service account
access, you specify a service account. If
the instance needs to access
other Google resources, this service account must have access to those
Google resources also.
When you specify a service account,
choose one of the following:
Select the Compute Engine default service account.
Specify a custom service account. The custom service account must be
in the same project as your Vertex AI Workbench instance.
To create the instance, you must have
the
iam.serviceAccounts.actAs
permission on the service account.
To grant access to users through a service account,
you grant the
iam.serviceAccounts.actAs
permission on
the specified service account for each user who needs
to access JupyterLab.
Grant access to multiple users through a service account
Create
a Vertex AI Workbench instance
with the following specifications:
In the
Create instance
dialog, in
the
IAM and security
section, select the
Service account
access mode.
Choose the Compute Engine default service account
or a
custom
service account
.
To use the Compute Engine default service account,
select
Use Compute Engine default service account
.
To use a custom service account, clear
Use Compute Engine default service account
, and then,
in the
Service account email
field, enter
your custom service account email address.
Complete the rest of the dialog, and then click
Create
.
For each user who needs to access JupyterLab,
grant the
iam.serviceAccounts.actAs
permission on your
service account
.
Access mode metadata
The access mode that you configure during
Vertex AI Workbench instance creation
is stored in the notebook metadata.
When you select the
Single user only
access mode,
Vertex AI Workbench stores a value for
proxy-mode
and
proxy-user-mail
.
The following are examples of single user access metadata entries:
proxy-mode=mail
proxy-user-mail=user@example.com
When you select the
Service account
access mode, Vertex AI Workbench
stores a
proxy-mode=service_account
metadata entry.
Caution:
Changing the access mode metadata is not supported and can make the
JupyterLab interface inaccessible.
What's next
Grant a principal access to
a Vertex AI Workbench instance.
To learn how to grant access to other Google resources, see
Manage access to
other resources
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/manage-access.txt
Manage access to a Vertex AI Workbench instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Manage access to an instance
This guide describes how you can grant access to
a specific Vertex AI Workbench instance.
To manage access to Vertex AI resources, see
the
Vertex AI page on access control
.
You grant access to a Vertex AI Workbench instance by setting an
Identity and Access Management (IAM) policy
on the instance.
The policy binds one or more principals, such as a user or a
service account, to one or more
roles
.
Each role contains a list of permissions that let the principal interact
with the instance.
You can grant access to an instance, instead of to a parent resource
such as a project, folder, or organization, to exercise the
principle of
least privilege
.
If you grant access to a
parent resource
(for example, to a project), you implicitly grant access to all its child
resources (for example, to all instances in that project). To limit access to
resources, set IAM policies on lower-level resources when
possible, instead of at the project level or above.
For general information about how to grant, change, and revoke access to
resources unrelated to Vertex AI Workbench, for example, to grant access to
a Google Cloud project, see the IAM documentation for
managing access to projects, folders, and
organizations
.
Access limitations
Access to an instance can include a broad range of abilities, depending
on the role you assign to the principal. For example,
you might grant a principal the ability to start, stop, upgrade, and
monitor the health status of an instance. For the complete list of
IAM permissions available, see
Predefined
Vertex AI Workbench IAM
roles
.
However, even granting a principal full access to
a Vertex AI Workbench instance doesn't grant
the ability to use the instance's JupyterLab interface.
To grant access to the JupyterLab interface, see
Manage access to an instance's
JupyterLab interface
.
Grant access to Vertex AI Workbench instances
To grant users permission to access
a specific Vertex AI Workbench instance,
set an
IAM policy
on the instance.
gcloud
To grant a role to a principal on
a Vertex AI Workbench instance, use the
get-iam-policy
command to retrieve the current policy,
edit the current policy's access, and then use the
set-iam-policy
command to update the policy on the instance.
Retrieve the current policy
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: the name of your instance
PROJECT_ID
: your Google Cloud project ID
LOCATION
: the zone where your instance is located
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
get-iam-policy
INSTANCE_NAME
\
--project
=
PROJECT_ID
\
--location
=
LOCATION
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
get-iam-policy
INSTANCE_NAME
`
--project
=
PROJECT_ID
`
--location
=
LOCATION
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
get-iam-policy
INSTANCE_NAME
^
--project
=
PROJECT_ID
^
--location
=
LOCATION
The response is the text of your instance's IAM policy.
See the following for an example.
{
  "bindings": [
    {
      "role": "roles/notebooks.viewer",
      "members": [
        "user:email@example.com"
      ]
    }
  ],
  "etag": "BwWWja0YfJA=",
  "version": 3
}
Edit the policy
Edit the policy with a text editor to add or remove principals and their
associated roles. For example, to grant the
notebooks.admin
role to
eve@example.com
, add the following new binding to the policy
in the
"bindings"
section:
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
After adding the new binding, the policy might look like the following:
{
"bindings"
:
[
{
"role": "roles/notebooks.viewer",
"members": [
"user:email@example.com"
]
}
,
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
]
,
"etag"
:
"BwWWja0YfJA="
,
"version"
:
3
}
Save the updated policy in a file named
request.json
.
Update the policy on the instance
In the body of the request, provide the updated IAM
policy from the previous step, nested inside a
"policy"
section.
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: the name of your instance
PROJECT_ID
: your Google Cloud project ID
LOCATION
: the zone where your instance is located
Save the following content in a file called
request.json
:
{
"policy"
:
{
"bindings"
:
[
{
"role"
:
"roles/notebooks.viewer"
,
"members"
:
[
"user:email@example.com"
]
},
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
],
"etag"
:
"BwWWja0YfJA="
,
"version"
:
3
}
}
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
set-iam-policy
INSTANCE_NAME
\
--project
=
PROJECT_ID
\
--location
=
LOCATION
\
request.json
--format
=
json
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
set-iam-policy
INSTANCE_NAME
`
--project
=
PROJECT_ID
`
--location
=
LOCATION
`
request.json
--format
=
json
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
set-iam-policy
INSTANCE_NAME
^
--project
=
PROJECT_ID
^
--location
=
LOCATION
^
request.json
--format
=
json
Grant access to the JupyterLab interface
Granting a principal access to
a Vertex AI Workbench instance doesn't grant
the ability to use the instance's JupyterLab interface.
To grant access to the JupyterLab interface, see
Manage access to a
Vertex AI Workbench instance's
JupyterLab interface
.
API
To grant a role to a principal on
a Vertex AI Workbench instance, use the
getIamPolicy
method to retrieve the current policy,
edit the current policy's access, and then use the
setIamPolicy
method to update the policy on the instance.
Retrieve the current policy
Before using any of the request data,
  make the following replacements:
PROJECT_ID
: your Google Cloud project ID
LOCATION
: the zone where your instance is located
INSTANCE_NAME
: the name of your instance
HTTP method and URL:
GET https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_NAME
:getIamPolicy
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Execute the following command:
curl -X GET \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_NAME
:getIamPolicy"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method GET `
-Headers $headers `
-Uri "https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_NAME
:getIamPolicy" | Select-Object -Expand Content
The response is the text of your instance's IAM policy.
See the following for an example.
{
  "bindings": [
    {
      "role": "roles/notebooks.viewer",
      "members": [
        "user:email@example.com"
      ]
    }
  ],
  "etag": "BwWWja0YfJA=",
  "version": 3
}
Edit the policy
Edit the policy with a text editor to add or remove principals and their
associated roles. For example, to grant the
notebooks.admin
role to
eve@example.com, add the following new binding to the policy
in the
"bindings"
section:
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
After adding the new binding, the policy might look like the following:
{
"bindings"
:
[
{
"role": "roles/notebooks.viewer",
"members": [
"user:email@example.com"
]
}
,
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
]
,
"etag"
:
"BwWWja0YfJA="
,
"version"
:
3
}
Update the policy on the instance
In the body of the request, provide the updated IAM
policy from the previous step, nested inside a
"policy"
section.
Before using any of the request data,
  make the following replacements:
PROJECT_ID
: your Google Cloud project ID
LOCATION
: the zone where your instance is located
INSTANCE_NAME
: the name of your instance
HTTP method and URL:
POST https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_NAME
:setIamPolicy
Request JSON body:
{
  "policy": {
    "bindings": [
      {
        "role": "roles/notebooks.viewer",
        "members": [
          "user:email@example.com"
        ]
      },
      {
        "role": "roles/notebooks.admin",
        "members": [
          "user:eve@example.com"
        ]
      }
    ],
    "etag": "BwWWja0YfJA=",
    "version": 3
  }
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_NAME
:setIamPolicy"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_NAME
:setIamPolicy" | Select-Object -Expand Content
You should receive a successful status code (2xx) and an empty response.
Grant access to the JupyterLab interface
Granting a principal access to
a Vertex AI Workbench instance doesn't grant
the ability to use the instance's JupyterLab interface.
To grant access to the JupyterLab interface, see
Manage access to an instance's
JupyterLab interface
.
What's next
Grant a principal access to
JupyterLab.
To learn about Identity and Access Management (IAM) and how
IAM roles can help grant and restrict access,
see the
IAM documentation
.
Learn about the
IAM roles available
to Vertex AI Workbench
.
Learn how to create and manage
custom roles
.
To learn how to grant access to other Google resources, see
Manage access to
other resources
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/manage-environment.txt
Manage the conda environment in your Vertex AI Workbench instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Manage your conda environment
This page describes how to manage a conda environment in your
Vertex AI Workbench instance.
Overview
If you've added a conda environment to your Vertex AI Workbench instance,
it appears as a
kernel
in your instance's JupyterLab interface.
You might have added a conda environment to your instance to use a kernel
that isn't available in a default Vertex AI Workbench instance.
This page describes how to modify and delete that kernel.
Open JupyterLab
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Next to your Vertex AI Workbench instance's name,
click
Open JupyterLab
.
Your Vertex AI Workbench instance opens JupyterLab.
Modify a conda kernel
Vertex AI Workbench instances come with pre-installed frameworks such as PyTorch
and TensorFlow. If you need a different version, you can modify the
libraries by using pip in the relevant conda environment.
For example, if you want to upgrade PyTorch:
# Check the name of the conda environment for PyTorch
conda
env
list
# Activate the environment for PyTorch
conda
activate
pytorch
# Display the PyTorch version
python
-
c
"import torch; print(torch.__version__)"
# Make sure to use pip from the conda environment for PyTorch
# This should be `/opt/conda/envs/pytorch/bin/pip`
which
pip
# Upgrade PyTorch
pip
install
--
upgrade
torch
Delete a conda kernel
Some conda packages add default kernels to your environment when the packages
are installed. For example, when you install R, conda might also add a
python3
kernel. This can cause a duplication of kernels in your
environment. To avoid duplicated kernels, delete the default kernel
before you create a new kernel with the same name.
rm -rf /opt/conda/envs/
CONDA_ENVIRONMENT_NAME
/share/jupyter/kernels/python3
Troubleshoot
To diagnose and resolve issues related to managing a conda environment in
your Vertex AI Workbench instance, see
Troubleshooting
Vertex AI Workbench
.
What's next
Learn more about
conda
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/manage-metadata.txt
Manage features through metadata  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Manage features through metadata
This page describes how to manage some Vertex AI Workbench instance features
by modifying the instance's metadata key-value pairs.
Metadata keys
For information about features and their respective metadata keys,
see the following table.
Feature
Description
Metadata key
Accepted values and defaults
Enables Cloud Storage FUSE on a container image
Mounts
/dev/fuse
onto the container and enables
gcsfuse
for use on the container.
container-allow-fuse
true
: Enables Cloud Storage FUSE.
false
(default): Doesn't enable Cloud Storage FUSE.
nbconvert
Lets you export and download notebooks as a different file type.
notebook-disable-nbconvert
true
: Turns off nbconvert.
false
(default): Enables nbconvert.
Delete to trash
Uses the operating system's trash behavior when
          deleting from JupyterLab.
notebook-enable-delete-to-trash
true
: Enables deleting to the trash.
false
(default): Uses the default JupyterLab behavior.
Dataproc
Enables access to Dataproc kernels.
For more information, see
Create
         a Dataproc-enabled instance
.
disable-mixer
true
: Turns off access to Dataproc kernels.
false
(default): Enables access to Dataproc kernels.
Idle shutdown
Enables idle shutdown.
For more information, see
Idle shutdown
.
idle-timeout-seconds
An integer representing the idle time in seconds. The default value
        is
10800
seconds (180 minutes).
Guest attributes
Enables guest attributes. Required for running idle shutdown.
For more information, see
Requirements
         for running idle shutdown
.
enable-guest-attributes
true
(default):  Enables guest attributes.
false
: Turns off guest attributes.
Scheduled OS patches
Schedules automatic OS updates of the instance. This enables Debian's
unattended
          upgrade service
and only applies to VM-based images.
install-unattended-upgrades
true
: Turns on automatic OS updates.
false
(default): Turns off automatic OS updates.
Custom Jupyter user
Specifies the name of the default Jupyter user. This setting
          determines the name of the folder for your notebooks. For example,
          instead of the default
/home/jupyter/
directory, you
          can change the directory to
/home/
CUSTOM_NAME
.
          This metadata key doesn't affect access to the instance.
jupyter-user
A string. The default value is
jupyter
.
File downloading
Lets you download files from JupyterLab.
notebook-disable-downloads
true
: Turns off file downloading.
false
(default): Enables file downloading.
Root access
Enables root access.
notebook-disable-root
true
: Turns off root access.
false
(default): Enables root access.
Terminal access
Enables terminal access.
notebook-disable-terminal
true
: Turns off terminal access.
false
(default): Enables terminal access.
Scheduled upgrades
Schedules automatic upgrades of the instance.
notebook-upgrade-schedule
The weekly or monthly schedule that you set, in
unix-cron
        format
, for example,
00 19 * * MON
means weekly on
        Monday, at 1900 hours Greenwich Mean Time (GMT).
        This feature is off by default.
Post-startup script
Runs a custom script after other startup scripts have completed. For details on the execution order, see
Startup script execution order
.
post-startup-script
The URI of a post-startup script in Cloud Storage, for example:
gs://bucket/hello.sh
. This feature is off by default.
Post-startup script behavior
Defines when and how the post-startup script runs.
post-startup-script-behavior
run_once
(default): Runs the post-startup script
            once after instance creation or upgrade.
run_every_start
: Runs the post-startup script
            after every start.
download_and_run_every_start
: Redownloads the
            post-startup script from its source then runs the script after
            every start.
Report event health
Checks health every 30 seconds for VM metrics.
report-event-health
true
(default): Enables event health reporting.
false
: Turns off event health reporting.
Enable JupyterLab 4 preview
Enables JupyterLab 4
          (
Preview
)
          on your instance. For more information, see
JupyterLab 4
          preview
.
enable-jupyterlab4-preview
true
: Enables JupyterLab 4.
false
(default): Enables JupyterLab 3.
Startup script execution order
If you use multiple startup scripts for your Vertex AI Workbench instance, they run in the following order:
startup-script
: Runs first during each boot after the initial boot.
startup-script-url
: Runs second during each boot after the initial boot.
workbench-startup-scripts
: Runs after the Compute Engine boot scripts (
startup-script
and
startup-script-url
) complete.
post-startup-script
: Runs after the
workbench-startup-scripts
complete.
Note that for the
post-startup-script
metadata key, you must provide the script as a Cloud Storage URI. You cannot provide the script content directly as the value.
Metadata managed by Compute Engine
Some of the metadata keys are predefined by Compute Engine. For more
information, see
Predefined metadata
keys
.
Protected metadata keys
Some metadata keys are reserved for system use only. If you assign
values to these metadata keys, the new values will be overwritten by the
system values.
Reserved metadata keys include and are not limited to:
data-disk-uri
enable-oslogin
framework
notebooks-api
notebooks-api-version
nvidia-driver-gcs-path
proxy-url
restriction
shutdown-script
title
version
Create an instance with specific metadata
You can create a Vertex AI Workbench instance with specific metadata
by using the Google Cloud console, the Google Cloud CLI,
Terraform, or the Notebooks API.
Console
When you create a Vertex AI Workbench instance, you can add
metadata in the
Environment
section of
Advanced options
.
gcloud
When you create a Vertex AI Workbench instance, you can add
metadata by using the following command:
gcloud
workbench
instances
create
INSTANCE_NAME
--metadata
=
KEY
=
VALUE
Terraform
To add metadata, create the resource with metadata key-value pairs.
To learn how to apply or remove a Terraform configuration, see
Basic Terraform commands
.
resource "google_workbench_instance" "default" {
  name     = "workbench-instance-example"
  location = "us-central1-a"

  gce_setup {
    machine_type = "n1-standard-1"
    vm_image {
      project = "cloud-notebooks-managed"
      family  = "workbench-instances"
    }
    metadata = {
      key = "value"
    }
  }
}
Notebooks API
Use the
instances.create
method with metadata values to manage the corresponding features.
Update an instance's metadata
You can update the metadata of a Vertex AI Workbench instance
by using the Google Cloud console, the Google Cloud CLI,
Terraform, or the Notebooks API.
Console
To update the metadata of a Vertex AI Workbench instance,
do the following:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
In the list of instances, click the name of the instance that you want
to update.
On the
Instance details
page, click
Software and security
.
In the
Metadata
section, update the metadata key-value pairs that
you want to change.
Click
Submit
.
gcloud
You can update the metadata on a Vertex AI Workbench instance
by using the following command:
gcloud
workbench
instances
update
INSTANCE_NAME
--metadata
=
KEY
=
VALUE
Terraform
You can change the metadata key-value pairs to manage
the corresponding features on Vertex AI Workbench instances.
To learn how to apply or remove a Terraform configuration, see
Basic Terraform commands
.
resource "google_workbench_instance" "default" {
  name     = "workbench-instance-example"
  location = "us-central1-a"

  gce_setup {
    machine_type = "n1-standard-1"
    vm_image {
      project = "cloud-notebooks-managed"
      family  = "workbench-instances"
    }
    metadata = {
      key = "updated_value"
    }
  }
}
Notebooks API
Use the
instances.patch
method with metadata values and
gce_setup.metadata
in the
updateMask
to manage the corresponding features.
Remove metadata from an instance
You can remove metadata from a Vertex AI Workbench instance
by using the Google Cloud console, the Google Cloud CLI,
Terraform, or the Notebooks API.
Console
To remove metadata from a Vertex AI Workbench instance,
do the following:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
In the list of instances, click the name of the instance that you want
to modify.
On the
Instance details
page, click
Software and security
.
In the
Metadata
section, to the right of a key-value pair that
you want to delete, click
delete
Delete
.
Click
Submit
.
gcloud
You can remove metadata from a Vertex AI Workbench instance
by using the following command:
gcloud
workbench
instances
update
INSTANCE_NAME
--metadata
=
KEY
Terraform
You can remove metadata key-value pairs to manage the
corresponding features of a Vertex AI Workbench instance.
To learn how to apply or remove a Terraform configuration, see
Basic Terraform commands
.
resource "google_workbench_instance" "default" {
  name     = "workbench-instance-example"
  location = "us-central1-a"

  gce_setup {
    machine_type = "n1-standard-1"
    vm_image {
      project = "cloud-notebooks-managed"
      family  = "workbench-instances"
    }
    metadata = {
    }
  }
}
Notebooks API
Use the
instances.patch
method with the metadata value set to an empty string and
gce_setup.metadata
in the
updateMask
to remove the
corresponding feature.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/migrate.txt
Migrate data to a new Vertex AI Workbench instance  |  Google Cloud Documentation
Migrate data to a new Vertex AI Workbench instance
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Migrate data to a new Vertex AI Workbench instance
This page describes how to migrate data and files from
an existing Vertex AI Workbench instance to a new one.
When to migrate
You might need to migrate your data to
a new Vertex AI Workbench instance if
you can't upgrade the environment of your existing instance.
See the
requirements for upgrading the environment of
a Vertex AI Workbench instance
.
Migration options
To migrate data and files from one Vertex AI Workbench instance to another,
consider using the following methods:
Use GitHub
: Copy your data and files to a GitHub repository
by using the Git extension for JupyterLab.
Use Cloud Storage and the terminal
: Copy your data and files
to Cloud Storage and then to another instance by using the terminal.
Use Cloud Storage within JupyterLab notebooks
:
Copy your data and files to Cloud Storage and then to another instance
by running commands within your respective instances' notebook cells.
This guide describes how to migrate data and files by using
Cloud Storage and the terminal.
Requirements
You must have terminal access to your Vertex AI Workbench instance.
Terminal access is manually set when you create an instance. The
terminal access setting cannot be changed after the instance is created.
Before you begin
Create a Cloud Storage bucket
in the same project where your Vertex AI Workbench instance
is located.
Migrate your data to a new Vertex AI Workbench instance
To migrate data and files to a new Vertex AI Workbench instance
by using Cloud Storage and the terminal, complete the following steps.
In your Vertex AI Workbench instance's
JupyterLab interface, select
File
>
New
>
Terminal
to open a terminal window.
Use the
gcloud CLI
to copy your user data
to a Cloud Storage bucket. The following example command
copies all of the files from your instance's
/home/jupyter/
directory
to a directory in a Cloud Storage bucket.
gcloud
storage
cp
/home/jupyter/*
gs://
BUCKET_NAME
PATH
--recursive
Replace the following:
BUCKET_NAME
: the name of your
Cloud Storage bucket
PATH
: the path to the directory
where you want to copy your files, for example:
/copy/jupyter/
Open your Vertex AI Workbench instance's JupyterLab interface.
In your Vertex AI Workbench instance's
JupyterLab interface, select
File
>
New
>
Terminal
to open a terminal window.
Use the gcloud CLI to restore your data on the new instance.
The following example command copies all of
the files from a Cloud Storage directory to the
your new instance's
/home/jupyter/
directory.
gcloud
storage
cp
gs://
BUCKET_NAME
PATH
*
/home/jupyter/
What's next
Learn how to
upgrade the environment of
Vertex AI Workbench instances
.
Learn more about
using
SSH access
to connect
to your Vertex AI Workbench instance.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/monitor-health.txt
Monitor health status  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Monitor health status
Vertex AI Workbench instances provide several methods
for monitoring the health
of your notebooks. This page describes how
to use each method.
Methods for monitoring health status
You can monitor the health of your Vertex AI Workbench instances
in a few different ways. This page describes how to
use the following methods:
Use guest attributes to report system health
Report custom metrics to Cloud Monitoring
Report system and application metrics
to Monitoring by installing Monitoring
on a Vertex AI Workbench instance
Use the diagnostic tool
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Install
the Google Cloud CLI.
If you're using an external identity provider (IdP), you must first
sign in to the gcloud CLI with your federated identity
.
To
initialize
the gcloud CLI, run the following command:
gcloud
init
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Install
the Google Cloud CLI.
If you're using an external identity provider (IdP), you must first
sign in to the gcloud CLI with your federated identity
.
To
initialize
the gcloud CLI, run the following command:
gcloud
init
Use guest attributes to report system health
You can use guest attributes to report the system health of the following
core services:
Docker service
Docker reverse proxy agent
Jupyter service
Jupyter API
Guest attributes
are a specific type of custom metadata that
applications can write to while running on
your Vertex AI Workbench
instance. To learn more about guest attributes,
see
About VM metadata
.
How instances use guest attributes to report system health
The
notebooks-collection-agent
service runs a Python process
in the background that verifies the status of
the Vertex AI Workbench instance's core services
and updates the guest attributes as either
1
if no problems are detected or
-1
if a failure is detected.
To use the
notebooks-collection-agent
service to
report on your Vertex AI Workbench instance's health,
you must enable the following guest attributes
while
creating
a Vertex AI Workbench instance:
enable-guest-attributes=TRUE
: This enables guest
attributes on your Vertex AI Workbench instance. All new
instances enable this attribute by default.
report-event-health=TRUE
: This records system
health check results to your guest attributes.
The
notebooks-collection-agent
service doesn't need
any special permissions to write to the instance's guest attributes.
Note:
The
notebooks-collection-agent
service uses
approximately 50 MB of memory to run in the background.
Create a Vertex AI Workbench instance with system health guest attributes enabled
To use system health guest attributes to report
on your Vertex AI Workbench instance's health,
you must select the
Enable system health report
checkbox when you create
a Vertex AI Workbench instance.
You can enable the system health report by using the Google Cloud console.
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
In the
New instance
dialog, click
Advanced options
.
In the
Create instance
dialog,
in the
Details
section,
provide the following information for your new instance:
Name
: Provide a name for your new instance.
Region
and
Zone
: Select a region and zone for
the new instance. For best network performance,
select the region that is geographically closest to you.
In the
System health
section, in
Reporting
,
select
Report system health
.
Complete the rest of the instance creation dialog,
and then click
Create
.
Monitor system health through guest attributes
For Vertex AI Workbench instances that
have
the related guest attributes enabled
,
you can retrieve the values of your system health guest attributes
by using either the Google Cloud console, the Google Cloud CLI with Compute Engine
commands, or the Google Cloud CLI with Vertex AI Workbench commands.
Console
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click the instance name that
you want to view the system health status of.
On the
Instance details
page, click the
Health
tab. Review
the status of your instance and its core services.
gcloud with Compute Engine
gcloud compute instances get-guest-attributes
INSTANCE_NAME
\
    --zone
ZONE
Replace the following:
INSTANCE_NAME
: the name of your instance
ZONE
: the zone where your instance is located
If your core services are healthy, the results look like the following.
A value of
1
means no failure was detected.
NAMESPACE   KEY                         VALUE
 notebooks   docker_proxy_agent_status   1
 notebooks   docker_status               1
 notebooks   jupyterlab_api_status       1
 notebooks   jupyterlab_status           1
 notebooks   system-health               1
 notebooks   updated                     2023-06-20 17:00:00.12345
If any of the four core services fail, system-health reports a
-1
value to indicate system failure. In most cases,
a system failure means that JupyterLab is not accessible.
An example of a failure result might look like the following.
NAMESPACE   KEY                         VALUE
 notebooks   docker_proxy_agent_status   -1
 notebooks   docker_status               -1
 notebooks   jupyterlab_api_status       1
 notebooks   jupyterlab_status           1
 notebooks   system-health               -1
 notebooks   updated                     2023-06-20 17:00:00.12345
Report custom metrics to Monitoring
Vertex AI Workbench instances let you
collect system status and JupyterLab metrics
and report them to Cloud Monitoring. These custom metrics
are different from the standard metrics that are reported when
you
install Monitoring on
your Vertex AI Workbench instance
.
The custom metrics reported to Monitoring include the following:
The system health of these Vertex AI Workbench core services:
Docker service
Docker reverse proxy agent
Jupyter service
Jupyter API
The following JupyterLab metrics:
Number of kernels
Number of terminals
Number of connections
Number of sessions
Maximum memory
High memory
Current memory
How instances report custom metrics to Monitoring
To report custom metrics to Monitoring, you must enable
the
report-notebook-metrics
metadata setting
while
creating
a Vertex AI Workbench instance.
You must also make sure that the Vertex AI Workbench
instance's service account
has Monitoring Metric Writer (
roles/monitoring.metricWriter
)
permissions. For more information, see
Manage access to projects, folders, and organizations
.
Create a Vertex AI Workbench instance that reports custom metrics to Monitoring
To report custom metrics to Monitoring, you must select
the
Report custom metrics to Cloud Monitoring
checkbox
when you create a Vertex AI Workbench instance.
You can enable reporting custom metrics to Cloud Monitoring by using
the Google Cloud console.
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
In the
New instance
dialog, click
Advanced options
.
In the
Create instance
dialog,
in the
Details
section,
provide the following information for your new instance:
Name
: Provide a name for your new instance.
Region
and
Zone
: Select a region and zone for
the new instance. For best network performance,
select the region that is geographically closest to you.
In the
System health
section, in
Reporting
,
select
Report custom metrics to Cloud Monitoring
.
Complete the rest of the instance creation dialog,
and then click
Create
.
Grant Monitoring Metric Writer permissions to the service account
After you've created
your new Vertex AI Workbench instance,
grant Monitoring Metric Writer permissions
(
roles/monitoring.metricWriter
) to
the service account for
the Vertex AI Workbench instance.
For more information, see
Manage access to projects, folders, and organizations
.
Monitor custom metrics through Monitoring
For Vertex AI Workbench instances that
have
reporting custom metrics enabled
,
you can monitor your custom metrics
by using the Google Cloud console.
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click the instance name that you want to view the custom metrics of.
On the
Instance details
page, click the
Monitoring
tab. Review
the custom metrics for your instance.
Install Monitoring on an instance
This option automatically installs Monitoring.
The installation requires 256 MB of disk space. An internet
connection is required for the metrics to be reported to
Monitoring.
How instances report system and application metrics
To report system and application metrics by installing
Cloud Monitoring on your Vertex AI Workbench instance,
you must select the
Install Cloud Monitoring agent
checkbox when you create
a Vertex AI Workbench instance.
These metrics are different from the custom metrics that are reported when
you
enable the
report-notebook-metrics
metadata
setting
.
Create a Vertex AI Workbench instance that reports system and application metrics to Monitoring
To install Monitoring on your
Vertex AI Workbench instance, you can use
the Google Cloud console.
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
In the
New instance
dialog, click
Advanced options
.
In the
Create instance
dialog,
in the
Details
section,
provide the following information for your new instance:
Name
: Provide a name for your new instance.
Region
and
Zone
: Select a region and zone for
the new instance. For best network performance,
select the region that is geographically closest to you.
In the
System health
section, in
Reporting
,
select
Install Cloud Monitoring
.
Complete the rest of the instance creation dialog,
and then click
Create
.
Monitor system and application metrics through Monitoring
For Vertex AI Workbench instances that
have
Monitoring installed
,
you can monitor your system and application metrics
by using the Google Cloud console:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click the instance name that
you want to view the system and application metrics of.
On the
Instance details
page, click the
Monitoring
tab. Review
the system and application metrics for your instance. To learn how to
interpret these metrics, see
Review resource
metrics
.
Use the diagnostic tool to monitor system health
Vertex AI Workbench instances include
a built-in diagnostic tool
that can help you monitor the system health
of your instances.
Tasks performed by the diagnostic tool
The diagnostic tool performs the following tasks:
Verifies the status of the following Vertex AI Workbench
core services:
Docker service
Docker reverse proxy agent
Jupyter service
Jupyter API
Checks whether the disk space for boot and data disks is
used beyond an 85% threshold.
Installs
lsof
(internet connection required).
Collects the following instance logs:
Network information (
ifconfig
,
netstat
)
Logs in the
/var/log/
folder
Docker status information
lsof
(open files) data
Docker service status
Proxy reverse agent status
Jupyter service status
Jupyter API status
Proxy agent configuration file
Python processes
Runs the following commands and collects the results:
pip freeze
conda list
gcloud compute instances describe
INSTANCE_NAME
gcloud config list
Run the diagnostic tool
If your instance uses a custom container, see
Run the
diagnostic tool
in the Vertex AI Workbench custom container documentation.
To run the diagnostic tool in an instance that doesn't use a custom container,
complete the following steps:
Use ssh to connect to your Vertex AI Workbench
instance
.
In the SSH terminal, run the following commands:
sudo
-i
cd
/opt/deeplearning/bin/
./diagnostic_tool.sh
The diagnostic tool collects the logs,
compresses them in a
.tar.gz
file,
and places the file in the
/tmp/
folder.
Extract the file and then evaluate the contents.
The contents include:
log
folder
: Logs from
the
var/log/
folder
report.log
: Output for
all commands collected
proxy-agent-config.json
:
Proxy configuration information
Docker log
: A
-json.log
file
that includes Docker container logs
You can use the following options with the diagnostic tool.
Option
Description
-r
A repair option that tries to restore
        failed Vertex AI Workbench core services status
-s
Runs without a confirmation
-b
Uploads the
.tar.gz
file
        to a Cloud Storage bucket.
-v
A debug option for troubleshooting the tool in case of failures
-c
Captures 30 seconds of packet traffic into
        your Vertex AI Workbench instance, filtering SSH
-d
A destination folder in which to save the logs
-h
Help
What's next
Learn more about VM metadata
.
Learn more about
Monitoring
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/reference/rest/v2/projects.locations.instances/getIamPolicy.txt
Method: projects.locations.instances.getIamPolicy  |  Vertex AI  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Vertex AI
Overview
Authenticate to Vertex AI
Vertex AI SDK for Python
Introduction to the Vertex AI SDK
Install the Vertex AI SDK
Vertex AI SDK reference
Vertex AI language model SDK reference
gcloud CLI reference
gcloud ai
gcloud beta ai
gcloud colab
gcloud beta colab
Client libraries
Install the client libraries
C# reference
Go reference
Java reference
Node.js reference
Long-running operations
REST reference
All methods
v1
REST Resources
media
Overview
upload
projects.locations
Overview
augmentPrompt
corroborateContent
deploy
evaluateInstances
getRagEngineConfig
retrieveContexts
updateRagEngineConfig
projects.locations.batchPredictionJobs
Overview
cancel
create
delete
get
list
projects.locations.cachedContents
Overview
create
delete
get
list
patch
projects.locations.customJobs
Overview
cancel
create
delete
get
list
projects.locations.datasets
Overview
create
delete
export
get
import
list
patch
searchDataItems
projects.locations.datasets.annotationSpecs
Overview
get
projects.locations.datasets.dataItems
Overview
list
projects.locations.datasets.dataItems.annotations
Overview
list
projects.locations.datasets.datasetVersions
Overview
create
delete
get
list
patch
restore
projects.locations.datasets.savedQueries
Overview
delete
list
projects.locations.deploymentResourcePools
Overview
create
delete
get
list
patch
queryDeployedModels
projects.locations.endpoints
Overview
create
delete
deployModel
directPredict
directRawPredict
explain
get
list
mutateDeployedModel
patch
predict
predictLongRunning
rawPredict
serverStreamingPredict
streamRawPredict
undeployModel
update
projects.locations.featureGroups
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureGroups.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.featureOnlineStores
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureOnlineStores.featureViews
Overview
create
delete
directWrite
fetchFeatureValues
get
getIamPolicy
list
patch
searchNearestEntities
setIamPolicy
sync
testIamPermissions
projects.locations.featureOnlineStores.featureViews.featureViewSyncs
Overview
get
list
projects.locations.featurestores
Overview
batchReadFeatureValues
create
delete
get
getIamPolicy
list
patch
searchFeatures
setIamPolicy
testIamPermissions
projects.locations.featurestores.entityTypes
Overview
create
delete
deleteFeatureValues
exportFeatureValues
get
getIamPolicy
importFeatureValues
list
patch
readFeatureValues
setIamPolicy
streamingReadFeatureValues
testIamPermissions
writeFeatureValues
projects.locations.featurestores.entityTypes.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.hyperparameterTuningJobs
Overview
cancel
create
delete
get
list
projects.locations.indexEndpoints
Overview
create
delete
deployIndex
get
list
mutateDeployedIndex
patch
undeployIndex
projects.locations.indexes
Overview
create
delete
get
list
patch
removeDatapoints
upsertDatapoints
projects.locations.metadataStores
Overview
create
delete
get
list
projects.locations.metadataStores.artifacts
Overview
create
delete
get
list
patch
purge
queryArtifactLineageSubgraph
projects.locations.metadataStores.contexts
Overview
addContextArtifactsAndExecutions
addContextChildren
create
delete
get
list
patch
purge
queryContextLineageSubgraph
removeContextChildren
projects.locations.metadataStores.executions
Overview
addExecutionEvents
create
delete
get
list
patch
purge
queryExecutionInputsAndOutputs
projects.locations.metadataStores.metadataSchemas
Overview
create
get
list
projects.locations.migratableResources
Overview
batchMigrate
search
projects.locations.modelDeploymentMonitoringJobs
Overview
create
delete
get
list
patch
pause
resume
searchModelDeploymentMonitoringStatsAnomalies
projects.locations.models
Overview
copy
delete
deleteVersion
export
get
getIamPolicy
list
listCheckpoints
listVersions
mergeVersionAliases
patch
setIamPolicy
testIamPermissions
updateExplanationDataset
upload
projects.locations.models.evaluations
Overview
get
import
list
projects.locations.models.evaluations.slices
Overview
batchImport
get
list
projects.locations.nasJobs
Overview
cancel
create
delete
get
list
projects.locations.nasJobs.nasTrialDetails
Overview
get
list
projects.locations.notebookExecutionJobs
Overview
create
delete
get
list
projects.locations.notebookRuntimeTemplates
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.notebookRuntimes
Overview
assign
delete
get
list
start
stop
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
wait
projects.locations.persistentResources
Overview
create
delete
get
list
patch
reboot
projects.locations.pipelineJobs
Overview
batchCancel
batchDelete
cancel
create
delete
get
list
projects.locations.publishers.models
Overview
predict
predictLongRunning
rawPredict
serverStreamingPredict
streamRawPredict
projects.locations.ragCorpora
Overview
create
delete
get
list
patch
projects.locations.ragCorpora.ragFiles
Overview
delete
get
import
list
projects.locations.reasoningEngines
Overview
create
delete
get
list
patch
query
streamQuery
projects.locations.schedules
Overview
create
delete
get
list
patch
pause
resume
projects.locations.specialistPools
Overview
create
delete
get
list
patch
projects.locations.studies
Overview
create
delete
get
list
lookup
projects.locations.studies.trials
Overview
addTrialMeasurement
checkTrialEarlyStoppingState
complete
create
delete
get
list
listOptimalTrials
stop
suggest
projects.locations.tensorboards
Overview
batchRead
create
delete
get
list
patch
readSize
readUsage
projects.locations.tensorboards.experiments
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs.timeSeries
Overview
create
delete
exportTensorboardTimeSeries
get
list
patch
read
readBlobData
projects.locations.trainingPipelines
Overview
cancel
create
delete
get
list
projects.locations.tuningJobs
Overview
cancel
create
get
list
rebaseTunedModel
publishers.models
Overview
get
Types
Annotation
ApiKeyConfig
AutomaticResources
BigQueryDestination
BigQuerySource
Content
CsvSource
CustomJobSpec
DataItem
DedicatedResources
DestinationFeatureSetting
DiskSpec
EncryptionSpec
EnvVar
Event
Explanation
ExplanationSpec
Fact
FeatureSelector
FeatureValue
FeatureValueDestination
FeatureViewDataKey
FetchFeatureValuesResponse
GcsDestination
GcsSource
JobState
LineageSubgraph
ListFeaturesResponse
MachineSpec
Measurement
ModelContainerSpec
ModelExplanation
NasTrial
NetworkSpec
NotebookEucConfig
NotebookExecutionJobView
NotebookIdleShutdownConfig
NotebookRuntimeType
NotebookSoftwareConfig
PSCAutomationConfig
PersistentDiskSpec
PipelineState
PredictResponse
PredictSchemata
PrivateServiceConnectConfig
PscInterfaceConfig
RagChunk
RagEngineConfig
RagFileTransformationConfig
ReadFeatureValuesResponse
ShieldedVmConfig
StreamingPredictResponse
StudySpec
Tensor
TensorboardBlob
TimeSeriesData
TimeSeriesDataPoint
v1beta1
REST Resources
media
Overview
upload
projects
Overview
fetchPublisherModelConfig
setPublisherModelConfig
projects.locations
Overview
augmentPrompt
corroborateContent
deploy
deployPublisherModel
evaluateDataset
evaluateInstances
getRagEngineConfig
recommendSpec
retrieveContexts
updateRagEngineConfig
projects.locations.batchPredictionJobs
Overview
cancel
create
delete
get
list
projects.locations.cachedContents
Overview
create
delete
get
list
patch
projects.locations.customJobs
Overview
cancel
create
delete
get
list
projects.locations.datasets
Overview
assemble
assess
create
delete
export
get
import
list
patch
searchDataItems
projects.locations.datasets.annotationSpecs
Overview
get
projects.locations.datasets.dataItems
Overview
list
projects.locations.datasets.dataItems.annotations
Overview
list
projects.locations.datasets.datasetVersions
Overview
create
delete
get
list
patch
restore
projects.locations.datasets.savedQueries
Overview
delete
list
projects.locations.deploymentResourcePools
Overview
create
delete
get
list
patch
queryDeployedModels
projects.locations.endpoints
Overview
countTokens
create
delete
deployModel
directPredict
directRawPredict
explain
get
getIamPolicy
list
mutateDeployedModel
patch
predict
rawPredict
serverStreamingPredict
setIamPolicy
streamRawPredict
testIamPermissions
undeployModel
update
projects.locations.endpoints.chat
Overview
completions
projects.locations.exampleStores
Overview
create
delete
fetchExamples
get
list
patch
removeExamples
searchExamples
upsertExamples
projects.locations.extensions
Overview
delete
execute
get
import
list
patch
query
projects.locations.featureGroups
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureGroups.featureMonitors
Overview
create
delete
get
list
patch
projects.locations.featureGroups.featureMonitors.featureMonitorJobs
Overview
create
get
list
projects.locations.featureGroups.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.featureOnlineStores
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureOnlineStores.featureViews
Overview
create
delete
directWrite
fetchFeatureValues
get
getIamPolicy
list
patch
searchNearestEntities
setIamPolicy
streamingFetchFeatureValues
sync
testIamPermissions
projects.locations.featureOnlineStores.featureViews.featureViewSyncs
Overview
get
list
projects.locations.featurestores
Overview
batchReadFeatureValues
create
delete
get
getIamPolicy
list
patch
searchFeatures
setIamPolicy
testIamPermissions
projects.locations.featurestores.entityTypes
Overview
create
delete
deleteFeatureValues
exportFeatureValues
get
getIamPolicy
importFeatureValues
list
patch
readFeatureValues
setIamPolicy
streamingReadFeatureValues
testIamPermissions
writeFeatureValues
projects.locations.featurestores.entityTypes.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.hyperparameterTuningJobs
Overview
cancel
create
delete
get
list
projects.locations.indexEndpoints
Overview
create
delete
deployIndex
get
list
mutateDeployedIndex
patch
undeployIndex
projects.locations.indexes
Overview
create
delete
get
import
list
patch
removeDatapoints
upsertDatapoints
projects.locations.metadataStores
Overview
create
delete
get
list
projects.locations.metadataStores.artifacts
Overview
create
delete
get
list
patch
purge
queryArtifactLineageSubgraph
projects.locations.metadataStores.contexts
Overview
addContextArtifactsAndExecutions
addContextChildren
create
delete
get
list
patch
purge
queryContextLineageSubgraph
removeContextChildren
projects.locations.metadataStores.executions
Overview
addExecutionEvents
create
delete
get
list
patch
purge
queryExecutionInputsAndOutputs
projects.locations.metadataStores.metadataSchemas
Overview
create
get
list
projects.locations.migratableResources
Overview
batchMigrate
search
projects.locations.modelDeploymentMonitoringJobs
Overview
create
delete
get
list
patch
pause
resume
searchModelDeploymentMonitoringStatsAnomalies
projects.locations.modelMonitors
Overview
create
delete
get
list
patch
searchModelMonitoringAlerts
searchModelMonitoringStats
projects.locations.modelMonitors.modelMonitoringJobs
Overview
create
delete
get
list
projects.locations.models
Overview
copy
delete
deleteVersion
export
get
getIamPolicy
list
listCheckpoints
listVersions
mergeVersionAliases
patch
setIamPolicy
testIamPermissions
updateExplanationDataset
upload
projects.locations.models.evaluations
Overview
get
import
list
projects.locations.models.evaluations.slices
Overview
batchImport
get
list
projects.locations.notebookExecutionJobs
Overview
create
delete
get
list
projects.locations.notebookRuntimeTemplates
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.notebookRuntimes
Overview
assign
delete
get
list
start
stop
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
wait
projects.locations.persistentResources
Overview
create
delete
get
list
patch
reboot
projects.locations.pipelineJobs
Overview
batchCancel
batchDelete
cancel
create
delete
get
list
projects.locations.publishers.models
Overview
countTokens
export
fetchPublisherModelConfig
getIamPolicy
predict
rawPredict
serverStreamingPredict
setPublisherModelConfig
streamRawPredict
projects.locations.ragCorpora
Overview
create
delete
get
list
patch
projects.locations.ragCorpora.ragFiles
Overview
delete
get
import
list
projects.locations.reasoningEngines
Overview
create
delete
get
list
patch
query
streamQuery
projects.locations.reasoningEngines.memories
Overview
create
delete
generate
get
list
patch
retrieve
projects.locations.reasoningEngines.sessions
Overview
appendEvent
create
delete
get
list
patch
projects.locations.reasoningEngines.sessions.events
Overview
list
projects.locations.schedules
Overview
create
delete
get
list
patch
pause
resume
projects.locations.specialistPools
Overview
create
delete
get
list
patch
projects.locations.studies
Overview
create
delete
get
list
lookup
projects.locations.studies.trials
Overview
addTrialMeasurement
checkTrialEarlyStoppingState
complete
create
delete
get
list
listOptimalTrials
stop
suggest
projects.locations.tensorboards
Overview
batchRead
create
delete
get
list
patch
readSize
readUsage
projects.locations.tensorboards.experiments
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs.timeSeries
Overview
create
delete
exportTensorboardTimeSeries
get
list
patch
read
readBlobData
projects.locations.trainingPipelines
Overview
cancel
create
delete
get
list
projects.locations.tuningJobs
Overview
cancel
create
get
list
rebaseTunedModel
projects.modelGardenEula
Overview
accept
check
publishers.models
Overview
get
list
Types
Annotation
ApiKeyConfig
AutomaticResources
BatchDedicatedResources
BigQueryDestination
BigQuerySource
Content
CountTokensResponse
CsvSource
CustomJobSpec
DataItem
DedicatedResources
DestinationFeatureSetting
DiskSpec
EncryptionSpec
EnvVar
Event
Example
ExamplesArrayFilter
Explanation
ExplanationSpec
Fact
FeatureSelectionConfig
FeatureSelector
FeatureStatsAndAnomaly
FeatureStatsAndAnomalySpec
FeatureStatsAnomaly
FeatureValue
FeatureValueDestination
FeatureViewDataFormat
FeatureViewDataKey
FeaturestoreMonitoringConfig
FetchFeatureValuesResponse
FlexStart
FunctionDeclaration
GcsDestination
GcsSource
GeminiRequestReadConfig
JobState
LineageSubgraph
ListFeaturesResponse
MachineSpec
ModelContainerSpec
ModelExplanation
ModelMonitoringAlertConfig
ModelMonitoringInput
ModelMonitoringNotificationSpec
ModelMonitoringObjectiveConfig
ModelMonitoringOutputSpec
NetworkSpec
NfsMount
NotebookEucConfig
NotebookExecutionJobView
NotebookIdleShutdownConfig
NotebookRuntimeType
NotebookSoftwareConfig
PSCAutomationConfig
PersistentDiskSpec
PipelineState
PredictResponse
PredictSchemata
PrivateServiceConnectConfig
PscInterfaceConfig
PublisherModelConfig
PublisherModelEulaAcceptance
PublisherModelView
RagChunk
RagEngineConfig
RagFileChunkingConfig
RagFileMetadataConfig
RagFileParsingConfig
RagFileTransformationConfig
ReadFeatureValuesResponse
SamplingStrategy
Schema
SearchKeyGenerationMethod
SessionEvent
ShieldedVmConfig
SliceSpec
StoredContentsExampleFilter
StreamingPredictResponse
StudySpec
TabularObjective
Tensor
TensorboardBlob
ThresholdConfig
TimeSeriesData
TimeSeriesDataPoint
Shared types
Types
Binding
CancelOperationRequest
DeleteOperationRequest
GetIamPolicyRequest
GetOperationRequest
HttpBody
Interval
LatLng
ListOperationsRequest
ListOperationsResponse
Policy
SetIamPolicyRequest
TestIamPermissionsRequest
TestIamPermissionsResponse
WaitOperationRequest
RPC reference
Overview
cloud.ai.large_models.vision
google.api
google.cloud.aiplatform.v1
Overview
schema
Overview
modelevaluation.metrics
predict.instance
predict.params
predict.prediction
trainingjob.definition
google.cloud.aiplatform.v1beta1
Overview
schema
Overview
modelevaluation.metrics
predict.instance
predict.params
predict.prediction
trainingjob.definition
google.iam.v1
google.longrunning
google.rpc
google.type
Google Cloud Pipelines Components reference
GCPC SDK reference
ML metadata artifact types
Vertex AI AutoML components
Batch prediction components
BigQuery ML components
CustomJob components
Dataflow components
Dataproc Serverless components
Dataset components
Forecasting components
Hyperparameter tuning components
Model and endpoint components
Model evaluation components
Email notification component
Vertex AI Neural Architecture Search reference
Prebuilt search spaces
PyGlove reference
Client library
Vertex AI Workbench: Notebooks API
Overview
Authenticate to Vertex AI Workbench
gcloud CLI reference
gcloud workbench
gcloud notebooks
gcloud beta notebooks
Client libraries
Overview
C++ reference
C# reference
Go reference
Java reference
Node.js reference
PHP reference
Python reference
Ruby reference
REST reference
Overview
v2
REST Resources
projects.locations
Overview
get
list
projects.locations.instances
Overview
checkUpgradability
create
delete
diagnose
get
getConfig
getIamPolicy
list
patch
reset
resizeDisk
restore
rollback
setIamPolicy
start
stop
testIamPermissions
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
v1
REST Resources
projects.locations
Overview
get
list
projects.locations.environments
Overview
create
delete
get
list
projects.locations.executions
Overview
create
delete
get
list
projects.locations.instances
Overview
create
delete
diagnose
get
getIamPolicy
getInstanceHealth
isUpgradeable
list
migrate
register
report
reset
rollback
setAccelerator
setIamPolicy
setLabels
setMachineType
start
stop
testIamPermissions
updateConfig
updateMetadataItems
updateShieldedInstanceConfig
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
projects.locations.runtimes
Overview
create
delete
get
getIamPolicy
list
migrate
patch
reportEvent
reset
setIamPolicy
start
stop
switch
testIamPermissions
projects.locations.schedules
Overview
create
delete
get
list
Types
ContainerImage
ExecutionTemplate
UpgradeType
VmImage
Shared types
Types
Binding
CancelOperationRequest
DeleteOperationRequest
GetIamPolicyRequest
GetLocationRequest
GetOperationRequest
ListLocationsRequest
ListLocationsResponse
ListOperationsRequest
ListOperationsResponse
Policy
SetIamPolicyRequest
TestIamPermissionsRequest
TestIamPermissionsResponse
RPC reference
Overview
google.cloud.common
google.cloud.location
google.cloud.notebooks.v1
google.cloud.notebooks.v2
google.iam.v1
google.longrunning
google.rpc
google.type
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI
Reference
Send feedback
Method: projects.locations.instances.getIamPolicy
Stay organized with collections
Save and categorize content based on your preferences.
Gets the access control policy for a resource. Returns an empty policy if the resource exists and does not have a policy set.
HTTP request
GET https://notebooks.googleapis.com/v2/{resource}:getIamPolicy
Path parameters
Parameters
resource
string
REQUIRED: The resource for which the policy is being requested. See
Resource names
for the appropriate value for this field.
Query parameters
Parameters
options
object (
GetPolicyOptions
)
OPTIONAL: A
GetPolicyOptions
object for specifying options to
instances.getIamPolicy
.
Request body
The request body must be empty.
Response body
If successful, the response body contains an instance of
Policy
.
Authorization scopes
Requires the following OAuth scope:
https://www.googleapis.com/auth/cloud-platform
For more information, see the
Authentication Overview
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-06-27 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/reference/rest/v2/projects.locations.instances/setIamPolicy.txt
Method: projects.locations.instances.setIamPolicy  |  Vertex AI  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Vertex AI
Overview
Authenticate to Vertex AI
Vertex AI SDK for Python
Introduction to the Vertex AI SDK
Install the Vertex AI SDK
Vertex AI SDK reference
Vertex AI language model SDK reference
gcloud CLI reference
gcloud ai
gcloud beta ai
gcloud colab
gcloud beta colab
Client libraries
Install the client libraries
C# reference
Go reference
Java reference
Node.js reference
Long-running operations
REST reference
All methods
v1
REST Resources
media
Overview
upload
projects.locations
Overview
augmentPrompt
corroborateContent
deploy
evaluateInstances
getRagEngineConfig
retrieveContexts
updateRagEngineConfig
projects.locations.batchPredictionJobs
Overview
cancel
create
delete
get
list
projects.locations.cachedContents
Overview
create
delete
get
list
patch
projects.locations.customJobs
Overview
cancel
create
delete
get
list
projects.locations.datasets
Overview
create
delete
export
get
import
list
patch
searchDataItems
projects.locations.datasets.annotationSpecs
Overview
get
projects.locations.datasets.dataItems
Overview
list
projects.locations.datasets.dataItems.annotations
Overview
list
projects.locations.datasets.datasetVersions
Overview
create
delete
get
list
patch
restore
projects.locations.datasets.savedQueries
Overview
delete
list
projects.locations.deploymentResourcePools
Overview
create
delete
get
list
patch
queryDeployedModels
projects.locations.endpoints
Overview
create
delete
deployModel
directPredict
directRawPredict
explain
get
list
mutateDeployedModel
patch
predict
predictLongRunning
rawPredict
serverStreamingPredict
streamRawPredict
undeployModel
update
projects.locations.featureGroups
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureGroups.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.featureOnlineStores
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureOnlineStores.featureViews
Overview
create
delete
directWrite
fetchFeatureValues
get
getIamPolicy
list
patch
searchNearestEntities
setIamPolicy
sync
testIamPermissions
projects.locations.featureOnlineStores.featureViews.featureViewSyncs
Overview
get
list
projects.locations.featurestores
Overview
batchReadFeatureValues
create
delete
get
getIamPolicy
list
patch
searchFeatures
setIamPolicy
testIamPermissions
projects.locations.featurestores.entityTypes
Overview
create
delete
deleteFeatureValues
exportFeatureValues
get
getIamPolicy
importFeatureValues
list
patch
readFeatureValues
setIamPolicy
streamingReadFeatureValues
testIamPermissions
writeFeatureValues
projects.locations.featurestores.entityTypes.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.hyperparameterTuningJobs
Overview
cancel
create
delete
get
list
projects.locations.indexEndpoints
Overview
create
delete
deployIndex
get
list
mutateDeployedIndex
patch
undeployIndex
projects.locations.indexes
Overview
create
delete
get
list
patch
removeDatapoints
upsertDatapoints
projects.locations.metadataStores
Overview
create
delete
get
list
projects.locations.metadataStores.artifacts
Overview
create
delete
get
list
patch
purge
queryArtifactLineageSubgraph
projects.locations.metadataStores.contexts
Overview
addContextArtifactsAndExecutions
addContextChildren
create
delete
get
list
patch
purge
queryContextLineageSubgraph
removeContextChildren
projects.locations.metadataStores.executions
Overview
addExecutionEvents
create
delete
get
list
patch
purge
queryExecutionInputsAndOutputs
projects.locations.metadataStores.metadataSchemas
Overview
create
get
list
projects.locations.migratableResources
Overview
batchMigrate
search
projects.locations.modelDeploymentMonitoringJobs
Overview
create
delete
get
list
patch
pause
resume
searchModelDeploymentMonitoringStatsAnomalies
projects.locations.models
Overview
copy
delete
deleteVersion
export
get
getIamPolicy
list
listCheckpoints
listVersions
mergeVersionAliases
patch
setIamPolicy
testIamPermissions
updateExplanationDataset
upload
projects.locations.models.evaluations
Overview
get
import
list
projects.locations.models.evaluations.slices
Overview
batchImport
get
list
projects.locations.nasJobs
Overview
cancel
create
delete
get
list
projects.locations.nasJobs.nasTrialDetails
Overview
get
list
projects.locations.notebookExecutionJobs
Overview
create
delete
get
list
projects.locations.notebookRuntimeTemplates
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.notebookRuntimes
Overview
assign
delete
get
list
start
stop
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
wait
projects.locations.persistentResources
Overview
create
delete
get
list
patch
reboot
projects.locations.pipelineJobs
Overview
batchCancel
batchDelete
cancel
create
delete
get
list
projects.locations.publishers.models
Overview
predict
predictLongRunning
rawPredict
serverStreamingPredict
streamRawPredict
projects.locations.ragCorpora
Overview
create
delete
get
list
patch
projects.locations.ragCorpora.ragFiles
Overview
delete
get
import
list
projects.locations.reasoningEngines
Overview
create
delete
get
list
patch
query
streamQuery
projects.locations.schedules
Overview
create
delete
get
list
patch
pause
resume
projects.locations.specialistPools
Overview
create
delete
get
list
patch
projects.locations.studies
Overview
create
delete
get
list
lookup
projects.locations.studies.trials
Overview
addTrialMeasurement
checkTrialEarlyStoppingState
complete
create
delete
get
list
listOptimalTrials
stop
suggest
projects.locations.tensorboards
Overview
batchRead
create
delete
get
list
patch
readSize
readUsage
projects.locations.tensorboards.experiments
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs.timeSeries
Overview
create
delete
exportTensorboardTimeSeries
get
list
patch
read
readBlobData
projects.locations.trainingPipelines
Overview
cancel
create
delete
get
list
projects.locations.tuningJobs
Overview
cancel
create
get
list
rebaseTunedModel
publishers.models
Overview
get
Types
Annotation
ApiKeyConfig
AutomaticResources
BigQueryDestination
BigQuerySource
Content
CsvSource
CustomJobSpec
DataItem
DedicatedResources
DestinationFeatureSetting
DiskSpec
EncryptionSpec
EnvVar
Event
Explanation
ExplanationSpec
Fact
FeatureSelector
FeatureValue
FeatureValueDestination
FeatureViewDataKey
FetchFeatureValuesResponse
GcsDestination
GcsSource
JobState
LineageSubgraph
ListFeaturesResponse
MachineSpec
Measurement
ModelContainerSpec
ModelExplanation
NasTrial
NetworkSpec
NotebookEucConfig
NotebookExecutionJobView
NotebookIdleShutdownConfig
NotebookRuntimeType
NotebookSoftwareConfig
PSCAutomationConfig
PersistentDiskSpec
PipelineState
PredictResponse
PredictSchemata
PrivateServiceConnectConfig
PscInterfaceConfig
RagChunk
RagEngineConfig
RagFileTransformationConfig
ReadFeatureValuesResponse
ShieldedVmConfig
StreamingPredictResponse
StudySpec
Tensor
TensorboardBlob
TimeSeriesData
TimeSeriesDataPoint
v1beta1
REST Resources
media
Overview
upload
projects
Overview
fetchPublisherModelConfig
setPublisherModelConfig
projects.locations
Overview
augmentPrompt
corroborateContent
deploy
deployPublisherModel
evaluateDataset
evaluateInstances
getRagEngineConfig
recommendSpec
retrieveContexts
updateRagEngineConfig
projects.locations.batchPredictionJobs
Overview
cancel
create
delete
get
list
projects.locations.cachedContents
Overview
create
delete
get
list
patch
projects.locations.customJobs
Overview
cancel
create
delete
get
list
projects.locations.datasets
Overview
assemble
assess
create
delete
export
get
import
list
patch
searchDataItems
projects.locations.datasets.annotationSpecs
Overview
get
projects.locations.datasets.dataItems
Overview
list
projects.locations.datasets.dataItems.annotations
Overview
list
projects.locations.datasets.datasetVersions
Overview
create
delete
get
list
patch
restore
projects.locations.datasets.savedQueries
Overview
delete
list
projects.locations.deploymentResourcePools
Overview
create
delete
get
list
patch
queryDeployedModels
projects.locations.endpoints
Overview
countTokens
create
delete
deployModel
directPredict
directRawPredict
explain
get
getIamPolicy
list
mutateDeployedModel
patch
predict
rawPredict
serverStreamingPredict
setIamPolicy
streamRawPredict
testIamPermissions
undeployModel
update
projects.locations.endpoints.chat
Overview
completions
projects.locations.exampleStores
Overview
create
delete
fetchExamples
get
list
patch
removeExamples
searchExamples
upsertExamples
projects.locations.extensions
Overview
delete
execute
get
import
list
patch
query
projects.locations.featureGroups
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureGroups.featureMonitors
Overview
create
delete
get
list
patch
projects.locations.featureGroups.featureMonitors.featureMonitorJobs
Overview
create
get
list
projects.locations.featureGroups.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.featureOnlineStores
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureOnlineStores.featureViews
Overview
create
delete
directWrite
fetchFeatureValues
get
getIamPolicy
list
patch
searchNearestEntities
setIamPolicy
streamingFetchFeatureValues
sync
testIamPermissions
projects.locations.featureOnlineStores.featureViews.featureViewSyncs
Overview
get
list
projects.locations.featurestores
Overview
batchReadFeatureValues
create
delete
get
getIamPolicy
list
patch
searchFeatures
setIamPolicy
testIamPermissions
projects.locations.featurestores.entityTypes
Overview
create
delete
deleteFeatureValues
exportFeatureValues
get
getIamPolicy
importFeatureValues
list
patch
readFeatureValues
setIamPolicy
streamingReadFeatureValues
testIamPermissions
writeFeatureValues
projects.locations.featurestores.entityTypes.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.hyperparameterTuningJobs
Overview
cancel
create
delete
get
list
projects.locations.indexEndpoints
Overview
create
delete
deployIndex
get
list
mutateDeployedIndex
patch
undeployIndex
projects.locations.indexes
Overview
create
delete
get
import
list
patch
removeDatapoints
upsertDatapoints
projects.locations.metadataStores
Overview
create
delete
get
list
projects.locations.metadataStores.artifacts
Overview
create
delete
get
list
patch
purge
queryArtifactLineageSubgraph
projects.locations.metadataStores.contexts
Overview
addContextArtifactsAndExecutions
addContextChildren
create
delete
get
list
patch
purge
queryContextLineageSubgraph
removeContextChildren
projects.locations.metadataStores.executions
Overview
addExecutionEvents
create
delete
get
list
patch
purge
queryExecutionInputsAndOutputs
projects.locations.metadataStores.metadataSchemas
Overview
create
get
list
projects.locations.migratableResources
Overview
batchMigrate
search
projects.locations.modelDeploymentMonitoringJobs
Overview
create
delete
get
list
patch
pause
resume
searchModelDeploymentMonitoringStatsAnomalies
projects.locations.modelMonitors
Overview
create
delete
get
list
patch
searchModelMonitoringAlerts
searchModelMonitoringStats
projects.locations.modelMonitors.modelMonitoringJobs
Overview
create
delete
get
list
projects.locations.models
Overview
copy
delete
deleteVersion
export
get
getIamPolicy
list
listCheckpoints
listVersions
mergeVersionAliases
patch
setIamPolicy
testIamPermissions
updateExplanationDataset
upload
projects.locations.models.evaluations
Overview
get
import
list
projects.locations.models.evaluations.slices
Overview
batchImport
get
list
projects.locations.notebookExecutionJobs
Overview
create
delete
get
list
projects.locations.notebookRuntimeTemplates
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.notebookRuntimes
Overview
assign
delete
get
list
start
stop
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
wait
projects.locations.persistentResources
Overview
create
delete
get
list
patch
reboot
projects.locations.pipelineJobs
Overview
batchCancel
batchDelete
cancel
create
delete
get
list
projects.locations.publishers.models
Overview
countTokens
export
fetchPublisherModelConfig
getIamPolicy
predict
rawPredict
serverStreamingPredict
setPublisherModelConfig
streamRawPredict
projects.locations.ragCorpora
Overview
create
delete
get
list
patch
projects.locations.ragCorpora.ragFiles
Overview
delete
get
import
list
projects.locations.reasoningEngines
Overview
create
delete
get
list
patch
query
streamQuery
projects.locations.reasoningEngines.memories
Overview
create
delete
generate
get
list
patch
retrieve
projects.locations.reasoningEngines.sessions
Overview
appendEvent
create
delete
get
list
patch
projects.locations.reasoningEngines.sessions.events
Overview
list
projects.locations.schedules
Overview
create
delete
get
list
patch
pause
resume
projects.locations.specialistPools
Overview
create
delete
get
list
patch
projects.locations.studies
Overview
create
delete
get
list
lookup
projects.locations.studies.trials
Overview
addTrialMeasurement
checkTrialEarlyStoppingState
complete
create
delete
get
list
listOptimalTrials
stop
suggest
projects.locations.tensorboards
Overview
batchRead
create
delete
get
list
patch
readSize
readUsage
projects.locations.tensorboards.experiments
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs.timeSeries
Overview
create
delete
exportTensorboardTimeSeries
get
list
patch
read
readBlobData
projects.locations.trainingPipelines
Overview
cancel
create
delete
get
list
projects.locations.tuningJobs
Overview
cancel
create
get
list
rebaseTunedModel
projects.modelGardenEula
Overview
accept
check
publishers.models
Overview
get
list
Types
Annotation
ApiKeyConfig
AutomaticResources
BatchDedicatedResources
BigQueryDestination
BigQuerySource
Content
CountTokensResponse
CsvSource
CustomJobSpec
DataItem
DedicatedResources
DestinationFeatureSetting
DiskSpec
EncryptionSpec
EnvVar
Event
Example
ExamplesArrayFilter
Explanation
ExplanationSpec
Fact
FeatureSelectionConfig
FeatureSelector
FeatureStatsAndAnomaly
FeatureStatsAndAnomalySpec
FeatureStatsAnomaly
FeatureValue
FeatureValueDestination
FeatureViewDataFormat
FeatureViewDataKey
FeaturestoreMonitoringConfig
FetchFeatureValuesResponse
FlexStart
FunctionDeclaration
GcsDestination
GcsSource
GeminiRequestReadConfig
JobState
LineageSubgraph
ListFeaturesResponse
MachineSpec
ModelContainerSpec
ModelExplanation
ModelMonitoringAlertConfig
ModelMonitoringInput
ModelMonitoringNotificationSpec
ModelMonitoringObjectiveConfig
ModelMonitoringOutputSpec
NetworkSpec
NfsMount
NotebookEucConfig
NotebookExecutionJobView
NotebookIdleShutdownConfig
NotebookRuntimeType
NotebookSoftwareConfig
PSCAutomationConfig
PersistentDiskSpec
PipelineState
PredictResponse
PredictSchemata
PrivateServiceConnectConfig
PscInterfaceConfig
PublisherModelConfig
PublisherModelEulaAcceptance
PublisherModelView
RagChunk
RagEngineConfig
RagFileChunkingConfig
RagFileMetadataConfig
RagFileParsingConfig
RagFileTransformationConfig
ReadFeatureValuesResponse
SamplingStrategy
Schema
SearchKeyGenerationMethod
SessionEvent
ShieldedVmConfig
SliceSpec
StoredContentsExampleFilter
StreamingPredictResponse
StudySpec
TabularObjective
Tensor
TensorboardBlob
ThresholdConfig
TimeSeriesData
TimeSeriesDataPoint
Shared types
Types
Binding
CancelOperationRequest
DeleteOperationRequest
GetIamPolicyRequest
GetOperationRequest
HttpBody
Interval
LatLng
ListOperationsRequest
ListOperationsResponse
Policy
SetIamPolicyRequest
TestIamPermissionsRequest
TestIamPermissionsResponse
WaitOperationRequest
RPC reference
Overview
cloud.ai.large_models.vision
google.api
google.cloud.aiplatform.v1
Overview
schema
Overview
modelevaluation.metrics
predict.instance
predict.params
predict.prediction
trainingjob.definition
google.cloud.aiplatform.v1beta1
Overview
schema
Overview
modelevaluation.metrics
predict.instance
predict.params
predict.prediction
trainingjob.definition
google.iam.v1
google.longrunning
google.rpc
google.type
Google Cloud Pipelines Components reference
GCPC SDK reference
ML metadata artifact types
Vertex AI AutoML components
Batch prediction components
BigQuery ML components
CustomJob components
Dataflow components
Dataproc Serverless components
Dataset components
Forecasting components
Hyperparameter tuning components
Model and endpoint components
Model evaluation components
Email notification component
Vertex AI Neural Architecture Search reference
Prebuilt search spaces
PyGlove reference
Client library
Vertex AI Workbench: Notebooks API
Overview
Authenticate to Vertex AI Workbench
gcloud CLI reference
gcloud workbench
gcloud notebooks
gcloud beta notebooks
Client libraries
Overview
C++ reference
C# reference
Go reference
Java reference
Node.js reference
PHP reference
Python reference
Ruby reference
REST reference
Overview
v2
REST Resources
projects.locations
Overview
get
list
projects.locations.instances
Overview
checkUpgradability
create
delete
diagnose
get
getConfig
getIamPolicy
list
patch
reset
resizeDisk
restore
rollback
setIamPolicy
start
stop
testIamPermissions
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
v1
REST Resources
projects.locations
Overview
get
list
projects.locations.environments
Overview
create
delete
get
list
projects.locations.executions
Overview
create
delete
get
list
projects.locations.instances
Overview
create
delete
diagnose
get
getIamPolicy
getInstanceHealth
isUpgradeable
list
migrate
register
report
reset
rollback
setAccelerator
setIamPolicy
setLabels
setMachineType
start
stop
testIamPermissions
updateConfig
updateMetadataItems
updateShieldedInstanceConfig
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
projects.locations.runtimes
Overview
create
delete
get
getIamPolicy
list
migrate
patch
reportEvent
reset
setIamPolicy
start
stop
switch
testIamPermissions
projects.locations.schedules
Overview
create
delete
get
list
Types
ContainerImage
ExecutionTemplate
UpgradeType
VmImage
Shared types
Types
Binding
CancelOperationRequest
DeleteOperationRequest
GetIamPolicyRequest
GetLocationRequest
GetOperationRequest
ListLocationsRequest
ListLocationsResponse
ListOperationsRequest
ListOperationsResponse
Policy
SetIamPolicyRequest
TestIamPermissionsRequest
TestIamPermissionsResponse
RPC reference
Overview
google.cloud.common
google.cloud.location
google.cloud.notebooks.v1
google.cloud.notebooks.v2
google.iam.v1
google.longrunning
google.rpc
google.type
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI
Reference
Send feedback
Method: projects.locations.instances.setIamPolicy
Stay organized with collections
Save and categorize content based on your preferences.
Sets the access control policy on the specified resource. Replaces any existing policy.
Can return
NOT_FOUND
,
INVALID_ARGUMENT
, and
PERMISSION_DENIED
errors.
HTTP request
POST https://notebooks.googleapis.com/v2/{resource}:setIamPolicy
Path parameters
Parameters
resource
string
REQUIRED: The resource for which the policy is being specified. See
Resource names
for the appropriate value for this field.
Request body
The request body contains data with the following structure:
JSON representation
{
"policy"
:
{
object (
Policy
)
}
}
Fields
policy
object (
Policy
)
REQUIRED: The complete policy to be applied to the
resource
. The size of the policy is limited to a few 10s of KB. An empty policy is a valid policy but certain Google Cloud services (such as Projects) might reject them.
Response body
If successful, the response body contains an instance of
Policy
.
Authorization scopes
Requires the following OAuth scope:
https://www.googleapis.com/auth/cloud-platform
For more information, see the
Authentication Overview
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-06-27 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/reservations.txt
Use reservations with Vertex AI Workbench instances  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Use reservations with Vertex AI Workbench instances
This document explains how to use
Compute Engine reservations
to gain a high level of assurance that your Vertex AI Workbench instances have
enough virtual machine (VM) resources to run.
Reservations are a Compute Engine feature. They help make sure that
you have the resources available to create VMs with the same hardware
(memory and vCPUs) and optional resources (GPUs and Local SSD disks)
whenever you need them.
When you create a reservation, Compute Engine verifies that the
requested capacity is available in the specified zone. If so, then
Compute Engine reserves the resources, creates the reservation, and the
following happens:
The reserved resources are immediately available for you to consume, and
they remain available until you delete the reservation.
You're charged for the reserved resources at the same on-demand rate as
running VMs, including any applicable discounts, until the reservation is
deleted. While consuming a reservation, a VM doesn't incur duplicate
charges for resources since the reservation is already billed for the
cost of the reserved resources. To learn more,
see
Reservations of Compute Engine
zonal resources
.
Limitations and requirements
All limitations of Compute Engine reservations apply when
Vertex AI Workbench instances consume reservations. See
How reservations
work
.
In addition, when using reservations with
Vertex AI Workbench instances, the following limitations
and requirements apply:
Your reservation must be one of the following:
In the same project as your Vertex AI Workbench instance.
Shared with the same project as your Vertex AI Workbench instance.
A reservation's VM properties must match exactly with your
Vertex AI Workbench instance to consume the reservation. For example,
if a reservation specifies an
e2-standard-8
machine type,
then the Vertex AI Workbench instance can only consume the
reservation if it also uses an
e2-standard-8
machine type. See
Requirements
.
Before you begin
Review the
requirements
and
restrictions
for reservations.
Review the
quota
    requirements
and
restrictions
for shared reservations.
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Enable the Compute Engine and Notebooks APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Enable the Compute Engine and Notebooks APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Required roles
To get the permissions that
      you need to use reservations with Vertex AI Workbench instances,
    
      ask your administrator to grant you the
    following IAM roles on the project:
Compute Admin
(
roles/compute.admin
)
Notebooks Admin
(
roles/notebooks.admin
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create a reservation
Create a Compute Engine reservation. It can be a single-project
reservation or a shared reservation. The reservation can be on-demand
or a future reservation. For more information, see
Choose a reservation type
.
Caution:
If you want to use your Vertex AI Workbench instance after
         your reservation has ended, use an on-demand reservation. Avoid
         consuming a future reservation because when a future reservation
         reaches its end time, Compute Engine automatically deletes
         the underlying VM of your instance, including its data disk. Your
         Vertex AI Workbench instance stops functioning and won't
         incur any further charges. To remove the instance from your list
         of Vertex AI Workbench instances, you must delete it.
Use a reservation with a new instance
To create a Vertex AI Workbench instance that consumes a reservation,
you can use the Google Cloud console or the REST API.
Console
To create a Vertex AI Workbench instance that consumes a reservation,
do the following:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
In the
New instance
dialog, click
Advanced options
.
In the
Create instance
dialog, in the
Machine type
section,
select the machine type and GPU configuration that matches your reservation.
Under
CPU Platform and GPU
, in the
Reservations
menu, select
the reservation to consume.
Click
Create
.
Vertex AI Workbench creates an instance and automatically starts it.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link.
REST
Before using any of the request data,
  make the following replacements:
PROJECT_ID
: your project ID
LOCATION
: the zone where your instance is located
INSTANCE_NAME
: the name of your instance
MACHINE_TYPE
: the machine type of your instance
RESERVATION_TYPE
: the type of reservation; must be
RESERVATION_ANY
or
RESERVATION_SPECIFIC
RESERVATION_NAME
: the name of your reservation when using the
RESERVATION_SPECIFIC
type
For reservations in the same project, you can use the reservation ID.
For reservations in a different project, you must use the full reservation path. For
        example:
projects/
PROJECT_ID
/reservations/
RESERVATION_NAME
.
HTTP method and URL:
POST https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances?instanceId=
INSTANCE_NAME
Request JSON body:
{
  "gce_setup": {
    "machine_type": "
MACHINE_TYPE
",
    "reservation_affinity": {
      "consume_reservation_type": "
RESERVATION_TYPE
",
      "key": "compute.googleapis.com/reservation-name",
      "values": ["
RESERVATION_NAME
"]
    }
  }
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances?instanceId=
INSTANCE_NAME
"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances?instanceId=
INSTANCE_NAME
" | Select-Object -Expand Content
If successful, the response body contains an instance of
Operation
.
For more information, see the
projects.locations.instances.create
REST API
documentation
.
Stop using your reservation
To stop using your reservation, you can
delete your reservation
.
If you want to continue using your reservation with other resources, but
don't want your existing Vertex AI Workbench instance to use it,
you must delete the instance.
Billing
When consuming a Compute Engine reservation, you're billed for
the following:
The Compute Engine resources, including any applicable
committed use discounts (CUDs), at Compute Engine pricing. These
charges have the label
goog-vertex-ai-product: workbench-instances
on
the SKU. See
Compute Engine pricing
.
Vertex AI Workbench management fees in addition to your
infrastructure usage. See
Vertex AI Workbench pricing
.
Troubleshoot
To find methods for diagnosing and resolving errors, related to
using reservations with Vertex AI Workbench instances, see
Troubleshooting Vertex AI Workbench instances
.
What's next
To learn more about Compute Engine reservations, see
Reservations of Compute Engine zonal
resources
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/restore-cloud-storage.txt
Back up and restore files by using Cloud Storage  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Back up and restore files by using Cloud Storage
This page describes how to use Cloud Storage to back up and restore files
on your Vertex AI Workbench instance.
Overview
This guide describes two ways to use Cloud Storage to help you
back up and restore files on your Vertex AI Workbench instance:
Mount a Cloud Storage bucket
to
your Vertex AI Workbench instance.
Export your files to Cloud Storage
and then restore them.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Required roles
To get the permissions that
      you need to mount a Cloud Storage bucket to a Vertex AI Workbench instance,
    
      ask your administrator to grant you the
    following IAM roles:
Notebooks Runner
(
roles/notebooks.runner
)
    
              on the project
Storage Object User
(
roles/storage.objectUser
)
    
              on the Vertex AI Workbench instance's service account
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Mount a Cloud Storage bucket
Vertex AI Workbench instances include a Cloud Storage integration
that lets you mount a Cloud Storage bucket. This means you can
browse the contents of the bucket and work with compatible files from within
the JupyterLab interface.
You can access any of the Cloud Storage buckets and files that
your instance has access to within the same project as
your Vertex AI Workbench instance.
Note:
Your Vertex AI Workbench instance's access to
Cloud Storage is determined by the single user or service account
that you used to grant access to your instance. For example,
if you granted a specific service account access to your instance,
you must also grant that service account access to the
Cloud Storage buckets that you want to use in JupyterLab.
Required permission for enabling shared storage mounting
To enable shared storage mounting in your Vertex AI Workbench instance,
ask your administrator to grant your Vertex AI Workbench instance's
service account the
storage.buckets.list
permission on the project.
The
storage.buckets.list
permission is required for the
Mount shared storage
button to appear in the JupyterLab interface of your
Vertex AI Workbench instance.
Create a bucket and a Vertex AI Workbench instance
You must have access to at least one Cloud Storage bucket in the
same project as your Vertex AI Workbench instance.
If you need to create a Cloud Storage bucket,
    see
Create a bucket
.
If you haven't already,
create a Vertex AI Workbench instance
in the same project as your Cloud Storage bucket.
Open JupyterLab
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Next to your Vertex AI Workbench instance's name,
      click
Open JupyterLab
.
Your Vertex AI Workbench instance opens JupyterLab.
Mount the Cloud Storage bucket
To mount and then access a Cloud Storage bucket, do the following:
In JupyterLab, make sure the
folder
File Browser
tab
      is selected.
In the left sidebar, click the
Mount
      shared storage
button. If you don't see the button, drag the right side
      of the sidebar to expand the sidebar until you see the button.
In the
Bucket name
field, enter the Cloud Storage
      bucket name that you want to mount.
Click
Mount
.
Your Cloud Storage bucket appears as a folder in the
File browser
tab of the left sidebar. Double-click the folder
      to open it and browse the contents.
Export to and restore files from Cloud Storage
This section describes how to export files to Cloud Storage and
restore files located in a Cloud Storage bucket.
Export to Cloud Storage
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Next to your Vertex AI Workbench instance's name,
      click
Open JupyterLab
.
Your Vertex AI Workbench instance opens JupyterLab.
In JupyterLab, make sure the
folder
File Browser
tab
      is selected.
In the left sidebar, click the
Export
      to GCS
button. If you don't see the button, drag the right side
      of the sidebar to expand the sidebar until you see the button.
In the
Provide export location
dialog, enter a
      Cloud Storage bucket name that you want to export files to.
      If you need to create a Cloud Storage bucket,
      see
Create a bucket
.
Vertex AI Workbench opens a new notebook that includes code
      for exporting your instance's files to Cloud Storage.
Run the code in this cell.
Restore files from Cloud Storage
To restore a file to your Vertex AI Workbench instance, you can use
gcloud storage cp
to copy the file.
Run the following code in a cell of one of your instance's notebooks:
!gcloud
storage
cp
URI
/home/jupyter/
FILE_NAME
Replace the following:
URI
: the gsutil URI of the file that you want to
copy, for example:
gs://
BUCKET_NAME
/
ZONE
/
INSTANCE_ID
/
FILE_NAME
FILE_NAME
: the name of the file to copy
For more information, see
Download the object from your
bucket
.
What's next
Use a snapshot to back up and restore data
Save a notebook to GitHub
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/restore-snapshot.txt
Back up and restore data by using a snapshot  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Back up and restore data by using a snapshot
This page describes how to use a snapshot to back up and restore the data on
your Vertex AI Workbench instance.
Back up the data
To back up data on a Vertex AI Workbench instance, you can take a
snapshot of the underlying Compute Engine virtual machine (VM) data disk.
You can create a snapshot of your instance's data disk by using
the Google Cloud console, the Google Cloud CLI, or the REST API:
Console
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click the instance name.
On the
Instance details
page, click
View in Compute Engine
to open
VM details
.
In the
Additional disks
section, click the name of the data disk.
The name of the data disk is in this format:
INSTANCE_NAME
-data-workspace
.
Click
Create snapshot
.
In the
Create a snapshot
dialog, click
Create
.
Compute Engine creates a snapshot of the data disk.
gcloud
To create a snapshot of your instance's data disk, use the
gcloud compute snapshots create
command.
Before using any of the command data below,
  make the following replacements:
SNAPSHOT_NAME
: a name for your snapshot
SOURCE_ZONE
: the zone where your instance is located
INSTANCE_NAME
: the name of your instance
STORAGE_LOCATION
: the
Cloud Storage multi-region
or the
Cloud Storage
    region
where you want to store your snapshot. You can specify only one storage location.
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
compute
snapshots
create
SNAPSHOT_NAME
\
--source-disk-zone
=
SOURCE_ZONE
\
--source-disk
=
INSTANCE_NAME
-data-workspace
\
--storage-location
=
STORAGE_LOCATION
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
compute
snapshots
create
SNAPSHOT_NAME
`
--source-disk-zone
=
SOURCE_ZONE
`
--source-disk
=
INSTANCE_NAME
-data-workspace
`
--storage-location
=
STORAGE_LOCATION
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
compute
snapshots
create
SNAPSHOT_NAME
^
--source-disk-zone
=
SOURCE_ZONE
^
--source-disk
=
INSTANCE_NAME
-data-workspace
^
--storage-location
=
STORAGE_LOCATION
REST
To create a snapshot of your instance's data disk, make a
POST
request to Compute Engine's
snapshots.insert
method.
Before using any of the request data,
  make the following replacements:
DESTINATION_PROJECT_ID
: the ID of the project where you want to
    create the snapshot
SNAPSHOT_NAME
: a name for your snapshot
SOURCE_PROJECT_ID
: the ID of the project where your instance is located
SOURCE_ZONE
: the zone where your instance is located
INSTANCE_NAME
: the name of your instance
STORAGE_LOCATION
: the
Cloud Storage multi-region
or the
Cloud Storage
    region
where you want to store your snapshot. You can specify only one storage location.
HTTP method and URL:
POST https://compute.googleapis.com/compute/v1/projects/
DESTINATION_PROJECT_ID
/global/snapshots
Request JSON body:
{
  "name": "
SNAPSHOT_NAME
",
  "sourceDisk": "projects/
SOURCE_PROJECT_ID
/zones/
SOURCE_ZONE
/disks/
INSTANCE_NAME
-data-workspace",
  "storageLocations": [
      "
STORAGE_LOCATION
"
  ],
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://compute.googleapis.com/compute/v1/projects/
DESTINATION_PROJECT_ID
/global/snapshots"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://compute.googleapis.com/compute/v1/projects/
DESTINATION_PROJECT_ID
/global/snapshots" | Select-Object -Expand Content
Restore data from a snapshot
You can restore data on an instance by using a snapshot. When you restore data
on an instance, Vertex AI Workbench deletes the existing data disk that is
attached to the instance, creates a new data disk based on the snapshot,
and attaches the new data disk to the instance.
You can restore data on an instance by using the gcloud CLI or
the REST API:
Caution:
Restoring a snapshot is supported only for snapshots of data disks
taken from Vertex AI Workbench instances. Restoring a snapshot that
was taken from a different source can cause the instance to function
incorrectly.
gcloud
To restore data on an instance, use the
gcloud workbench instances restore
command.
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: the name of your instance
LOCATION
: the zone where your instance is located
SNAPSHOT_PROJECT_NAME
: the project name where your snapshot is located
SNAPSHOT_NAME
: the name of the snapshot to restore
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
restore
INSTANCE_NAME
\
--location
=
LOCATION
\
--snapshot-project
=
SNAPSHOT_PROJECT_NAME
\
--snapshot
=
SNAPSHOT_NAME
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
restore
INSTANCE_NAME
`
--location
=
LOCATION
`
--snapshot-project
=
SNAPSHOT_PROJECT_NAME
`
--snapshot
=
SNAPSHOT_NAME
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
restore
INSTANCE_NAME
^
--location
=
LOCATION
^
--snapshot-project
=
SNAPSHOT_PROJECT_NAME
^
--snapshot
=
SNAPSHOT_NAME
REST
To restore data on an instance, make a
POST
request to the
projects.locations.instances.restore
method.
Before using any of the request data,
  make the following replacements:
PROJECT_ID
: your project ID
LOCATION
: the zone where your instance is located
INSTANCE_ID
: the ID of your instance
SNAPSHOT_ID
: the ID of the snapshot to restore; to get the
    ID of a snapshot, use Compute Engine's
snapshots.get
method
SNAPSHOT_PROJECT_ID
: the project ID of the snapshot
HTTP method and URL:
POST https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_ID
:restore
Request JSON body:
{
  "snapshot": {
    {
      "snapshotId":
SNAPSHOT_ID
,
      "projectId":
SNAPSHOT_PROJECT_ID
}
  }
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_ID
:restore"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_ID
:restore" | Select-Object -Expand Content
What's next
Use Cloud Storage to back up and restore
files
Save a notebook to GitHub
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/restore.txt
Back up and restore data by using a snapshot  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Back up and restore data by using a snapshot
This page describes how to use a snapshot to back up and restore the data on
your Vertex AI Workbench instance.
Back up the data
To back up data on a Vertex AI Workbench instance, you can take a
snapshot of the underlying Compute Engine virtual machine (VM) data disk.
You can create a snapshot of your instance's data disk by using
the Google Cloud console, the Google Cloud CLI, or the REST API:
Console
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click the instance name.
On the
Instance details
page, click
View in Compute Engine
to open
VM details
.
In the
Additional disks
section, click the name of the data disk.
The name of the data disk is in this format:
INSTANCE_NAME
-data-workspace
.
Click
Create snapshot
.
In the
Create a snapshot
dialog, click
Create
.
Compute Engine creates a snapshot of the data disk.
gcloud
To create a snapshot of your instance's data disk, use the
gcloud compute snapshots create
command.
Before using any of the command data below,
  make the following replacements:
SNAPSHOT_NAME
: a name for your snapshot
SOURCE_ZONE
: the zone where your instance is located
INSTANCE_NAME
: the name of your instance
STORAGE_LOCATION
: the
Cloud Storage multi-region
or the
Cloud Storage
    region
where you want to store your snapshot. You can specify only one storage location.
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
compute
snapshots
create
SNAPSHOT_NAME
\
--source-disk-zone
=
SOURCE_ZONE
\
--source-disk
=
INSTANCE_NAME
-data-workspace
\
--storage-location
=
STORAGE_LOCATION
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
compute
snapshots
create
SNAPSHOT_NAME
`
--source-disk-zone
=
SOURCE_ZONE
`
--source-disk
=
INSTANCE_NAME
-data-workspace
`
--storage-location
=
STORAGE_LOCATION
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
compute
snapshots
create
SNAPSHOT_NAME
^
--source-disk-zone
=
SOURCE_ZONE
^
--source-disk
=
INSTANCE_NAME
-data-workspace
^
--storage-location
=
STORAGE_LOCATION
REST
To create a snapshot of your instance's data disk, make a
POST
request to Compute Engine's
snapshots.insert
method.
Before using any of the request data,
  make the following replacements:
DESTINATION_PROJECT_ID
: the ID of the project where you want to
    create the snapshot
SNAPSHOT_NAME
: a name for your snapshot
SOURCE_PROJECT_ID
: the ID of the project where your instance is located
SOURCE_ZONE
: the zone where your instance is located
INSTANCE_NAME
: the name of your instance
STORAGE_LOCATION
: the
Cloud Storage multi-region
or the
Cloud Storage
    region
where you want to store your snapshot. You can specify only one storage location.
HTTP method and URL:
POST https://compute.googleapis.com/compute/v1/projects/
DESTINATION_PROJECT_ID
/global/snapshots
Request JSON body:
{
  "name": "
SNAPSHOT_NAME
",
  "sourceDisk": "projects/
SOURCE_PROJECT_ID
/zones/
SOURCE_ZONE
/disks/
INSTANCE_NAME
-data-workspace",
  "storageLocations": [
      "
STORAGE_LOCATION
"
  ],
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://compute.googleapis.com/compute/v1/projects/
DESTINATION_PROJECT_ID
/global/snapshots"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://compute.googleapis.com/compute/v1/projects/
DESTINATION_PROJECT_ID
/global/snapshots" | Select-Object -Expand Content
Restore data from a snapshot
You can restore data on an instance by using a snapshot. When you restore data
on an instance, Vertex AI Workbench deletes the existing data disk that is
attached to the instance, creates a new data disk based on the snapshot,
and attaches the new data disk to the instance.
You can restore data on an instance by using the gcloud CLI or
the REST API:
Caution:
Restoring a snapshot is supported only for snapshots of data disks
taken from Vertex AI Workbench instances. Restoring a snapshot that
was taken from a different source can cause the instance to function
incorrectly.
gcloud
To restore data on an instance, use the
gcloud workbench instances restore
command.
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: the name of your instance
LOCATION
: the zone where your instance is located
SNAPSHOT_PROJECT_NAME
: the project name where your snapshot is located
SNAPSHOT_NAME
: the name of the snapshot to restore
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
restore
INSTANCE_NAME
\
--location
=
LOCATION
\
--snapshot-project
=
SNAPSHOT_PROJECT_NAME
\
--snapshot
=
SNAPSHOT_NAME
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
restore
INSTANCE_NAME
`
--location
=
LOCATION
`
--snapshot-project
=
SNAPSHOT_PROJECT_NAME
`
--snapshot
=
SNAPSHOT_NAME
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
workbench
instances
restore
INSTANCE_NAME
^
--location
=
LOCATION
^
--snapshot-project
=
SNAPSHOT_PROJECT_NAME
^
--snapshot
=
SNAPSHOT_NAME
REST
To restore data on an instance, make a
POST
request to the
projects.locations.instances.restore
method.
Before using any of the request data,
  make the following replacements:
PROJECT_ID
: your project ID
LOCATION
: the zone where your instance is located
INSTANCE_ID
: the ID of your instance
SNAPSHOT_ID
: the ID of the snapshot to restore; to get the
    ID of a snapshot, use Compute Engine's
snapshots.get
method
SNAPSHOT_PROJECT_ID
: the project ID of the snapshot
HTTP method and URL:
POST https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_ID
:restore
Request JSON body:
{
  "snapshot": {
    {
      "snapshotId":
SNAPSHOT_ID
,
      "projectId":
SNAPSHOT_PROJECT_ID
}
  }
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_ID
:restore"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://notebooks.googleapis.com/v2/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_ID
:restore" | Select-Object -Expand Content
What's next
Use Cloud Storage to back up and restore
files
Save a notebook to GitHub
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/save-to-github.txt
Save a notebook to GitHub  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Save a notebook to GitHub
This page describes how you can save your Vertex AI Workbench instance's
notebook files to GitHub by using the
jupyterlab-git
extension. You might
do this to create a backup of the notebook or to make the notebook
available to others.
In Vertex AI Workbench instances, you can use the
jupyterlab-git
extension to help you with version control. To learn more, see
jupyterlab-git
on GitHub.
Create a GitHub repository
If you don't already have
a
GitHub
repository,
you must create one.
When you create your GitHub repository make sure that your GitHub repository
can be cloned by selecting the
Initialize this repository with a README
checkbox.
Clone your GitHub repository in your Vertex AI Workbench instance
To clone your GitHub repository in your Vertex AI Workbench instance,
complete the following steps:
In your GitHub repository, click the
Code
button,
and then click the
Local
tab.
Copy the
HTTPS
URL.
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
Open JupyterLab
to open
your Vertex AI Workbench instance.
In the JupyterLab
folder
File Browser
, select
the folder where you want to clone the GitHub repository. For example,
the home folder.
In JupyterLab, select
Git
>
Clone a Repository
.
In the
Clone a repo
dialog, paste the HTTPS URL for
your GitHub repository.
If prompted, enter your credentials.
If you use a GitHub username and password, enter your
GitHub username and password.
If you use two-factor authentication with GitHub,
create and use a
personal access
token
.
Click
Clone
.
Your Vertex AI Workbench instance shows your repository
as a new folder. If you don't see your cloned GitHub repository
as a folder, click the
Refresh File List
button.
Configure your instance with your GitHub user information
In JupyterLab, open the folder where your repository is located.
Select
Git
>
Open Git Repository in Terminal
to open a Git terminal window.
In the Git terminal window, enter the following commands to configure
your Git username and email:
git config --global user.name "
USERNAME
"
git config --global user.email "
EMAIL_ADDRESS
"
Replace the following:
USERNAME
: your GitHub username
EMAIL_ADDRESS
: your GitHub account email address
If your GitHub account requires SSH authentication, complete
the following steps to connect your account:
From your Git terminal in your Vertex AI Workbench
instance, follow GitHub's
instructions for generating a new
SSH key
.
Follow the
instructions for adding that SSH key to your GitHub
account
.
Close the Git terminal window.
Add your committed files to your GitHub repository
In JupyterLab, open the folder where your repository is located.
Add a new notebook
.
Select the
Git
tab. Your new notebook is listed in the
Untracked
grouping.
To add the new notebook as a file for your GitHub repository, right-click
the new notebook and select
Track
. On the
Git
tab, your notebook
is now added to the
Staged
grouping.
To commit your new notebook to your GitHub repository, on the
Git
tab,
in the
Summary
field, add a commit comment, and then click
Commit
.
Select
Git
>
Push to Remote
.
If you use a GitHub username and password, when prompted, enter your
GitHub username and password.
If you use two-factor authentication with GitHub, enter your
GitHub username and personal access token.
After the
git push
command completes, your committed files are in
your GitHub repository.
What's next
Use Cloud Storage to back up and restore
files
Use a snapshot to back up and restore
data
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/schedule-notebook-run-quickstart.txt
Quickstart: Schedule a notebook run  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Schedule a notebook run
This page shows you how to use
the Vertex AI Workbench executor
to run a Python notebook file on an hourly schedule.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Required roles
To ensure that your instance's service account has the necessary
      permissions to interact with the Vertex AI Workbench executor,
    
      ask your administrator to grant your instance's service account the
    following IAM roles on the project:
Important:
You must grant these roles
      to your instance's service account,
not
to your user account. Failure to grant the roles to the correct principal might result in permission errors.
Notebooks Runner
(
roles/notebooks.runner
)
Storage Admin
(
roles/storage.admin
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
Your administrator might also be able to give your instance's service account
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create an instance and example notebook file
Create an
instance
.
Open JupyterLab
.
Open a new notebook
file
.
In the first cell of the notebook file, enter the following:
# Import datetime
import
datetime
# Get the time and print it
datetime
.
datetime
.
now
()
print
(
datetime
.
datetime
.
now
())
To make sure your notebook file is saved, select
File
>
Save Notebook
.
Schedule a run
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Next to your instance's name,
click
Open JupyterLab
.
Your Vertex AI Workbench instance opens JupyterLab.
In the
folder
File Browser
,
double-click the example notebook file to open it.
Click the
Execute
button.
In the
Submit notebooks to Executor
dialog, in the
Type
field,
select
Schedule-based recurring executions
.
By default, the executor runs your notebook file
every hour at the
00
minute of the hour.
In
Advanced options
,
enter a name for your bucket in the
Cloud Storage bucket
field,
and then click
Create and select
.
The executor stores your notebook output
in the Cloud Storage bucket.
Click
Submit
.
Your notebook file runs automatically
on the schedule that you set.
Note:
If your instance is shut down, the
executor still runs your notebook file on schedule.
When you finish the tasks that are described in this document, you can avoid
   continued billing by deleting the resources that you created. For more information, see
Clean up
.
View, share, and import an executed notebook file
By using your instance's JupyterLab interface,
you can view your notebook output, share the results with others,
and import the executed notebook file into JupyterLab.
Note:
To use the Google Cloud console to view and share execution results,
on the
Executions
page,
click
View result
.
View the execution results
In JupyterLab's navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Under the execution that you want to view, click
View result
.
The executor opens your result in a new browser tab.
Share the execution results
In your instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Next to the execution that you want to share,
click the
more_vert
options menu,
and select
Share execution result
.
Follow the directions in the dialog
to grant users access to the execution result.
Import the executed notebook into JupyterLab
In your instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Next to the execution that you want to import,
click the
more_vert
options menu,
and select
Import executed notebook
.
If the
Select Kernel
dialog appears,
select the kernel that you want to open the notebook.
The executor opens the executed notebook file
in JupyterLab, and stores this notebook file in
the JupyterLab File Browser in a folder named
imported_notebook_jobs
.
View or delete a schedule
You can view and delete schedules by using either the Google Cloud console or
your instance's JupyterLab user interface.
View a schedule
View a schedule to see the frequency settings of the schedule
or to view the five most recent results of the notebook file execution.
Console
In the Google Cloud console, go to the
Schedules
page.
Go to Schedules
For the schedule that you want to view,
click its schedule name.
On the
Schedule details
page, you can view the schedule's last
five executions.
Next to an execution name, click
View result
to open
the executed notebook file.
The executor opens your result in a new browser tab.
JupyterLab
In your instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Schedules
tab.
Under the execution that you want to view, click
View latest
execution result
.
The executor opens your result in a new browser tab.
Delete a schedule
Deleting a schedule doesn't delete the executions that were
generated from that schedule.
Console
In the Google Cloud console, go to the
Schedules
page.
Go to Schedules
Select the schedule that you want to delete.
Click
delete
Delete
.
JupyterLab
In your instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Schedules
tab.
Click the schedule name. The
Schedule details
page
for that schedule opens in the Google Cloud console.
Click
delete
Delete
.
Clean up
To avoid incurring charges to your Google Cloud account for
          the resources used on this page, follow these steps.
Delete the instance
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Select the instance that you want to delete.
Click
delete
Delete
.
Delete the project
If you used resources outside of
your Vertex AI Workbench instance,
such as the Cloud Storage bucket required
for creating a schedule,
you might want to delete your project to avoid incurring additional charges.
Caution
: Deleting a project has the following effects:
Everything in the project is deleted.
If you used an existing project for
      the tasks in this document, when you delete it, you also delete any other work you've
      done in the project.
Custom project IDs are lost.
When you created this project, you might have created a custom project ID that you want to use in
      the future. To preserve the URLs that use the project ID, such as an
appspot.com
URL, delete selected resources inside the project instead of deleting the whole project.
If you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects
    can help you avoid exceeding project quota limits.
In the Google Cloud console, go to the
Manage resources
page.
Go to Manage resources
In the project list, select the project that you
    want to delete, and then click
Delete
.
In the dialog, type the project ID, and then click
Shut down
to delete the project.
What's next
To see an example of how to schedule a notebooks run as part of a more comprehensive workflow,
      run the "Train a multi-class classification model for ads-targeting" notebook in one of the following
      environments:
Open in Colab
|
Open in Colab Enterprise
|
Open
in Vertex AI Workbench
|
View on GitHub
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/service-perimeter.txt
Use a Vertex AI Workbench instance within a service perimeter  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Use an instance within a service perimeter
This page describes how to use VPC Service Controls to set up
a Vertex AI Workbench instance within a service perimeter.
Before you begin
Read the
Overview of
VPC Service Controls
.
Create a Vertex AI Workbench
instance
.
This instance is not within a service
perimeter yet.
Create a service perimeter using
VPC Service Controls
.
This service perimeter protects the Google-managed resources of services
that you specify. While creating your service perimeter, do the following:
When it's time to add projects to your service perimeter, add the
project that contains your Vertex AI Workbench instance.
When it's time to add services to your service perimeter, add the
Notebooks API
.
If you have created your service perimeter without adding the
projects and services you need, see
Managing service
perimeters
to learn how to update your service perimeter.
Configure your DNS entries using Cloud DNS
Vertex AI Workbench  instances use several domains that a
  Virtual Private Cloud network doesn't handle by default.
  To ensure that your VPC network correctly handles requests sent
  to those domains, use Cloud DNS to add DNS records. For more
  information about VPC routes, see
Routes
.
To create a
managed zone
for
  a domain, add a DNS entry that will route the request, and execute
  the transaction, complete the following steps.
  Repeat these steps for each of
several
  domains
that you need to handle requests for, starting
  with
*.notebooks.googleapis.com
.
In
Cloud Shell
or any environment where the
Google Cloud CLI
is installed, enter the following
Google Cloud CLI
commands.
To create a private managed zone
      for one of the domains that your
      VPC network needs to handle:
gcloud
dns
managed-zones
create
ZONE_NAME
\
--visibility
=
private
\
--networks
=
https://www.googleapis.com/compute/v1/projects/
PROJECT_ID
/global/networks/
NETWORK_NAME
\
--dns-name
=
DNS_NAME
\
--description
=
"Description of your managed zone"
Replace the following:
ZONE_NAME
: a name for the zone to create.
        You must use a separate zone for each domain. This zone name is used in
        each of the following steps.
PROJECT_ID
: the ID of the project that hosts your
        VPC network
NETWORK_NAME
: the name of the VPC
        network that you created earlier
DNS_NAME
: the part of the domain name that comes
        after the
*.
, with a period on the end.
        For example,
*.notebooks.googleapis.com
has a
DNS_NAME
of
notebooks.googleapis.com.
Start a transaction.
gcloud
dns
record-sets
transaction
start
--zone
=
ZONE_NAME
Add the following DNS A record. This reroutes traffic to
      Google's restricted IP addresses.
gcloud
dns
record-sets
transaction
add
\
--name
=
DNS_NAME
.
\
--type
=
A
199
.36.153.4
199
.36.153.5
199
.36.153.6
199
.36.153.7
\
--zone
=
ZONE_NAME
\
--ttl
=
300
Add the following DNS CNAME record to point to the A record
      that you just added. This redirects all traffic matching the
      domain to the IP addresses listed in the previous step.
gcloud
dns
record-sets
transaction
add
\
--name
=
\*
.
DNS_NAME
.
\
--type
=
CNAME
DNS_NAME
.
\
--zone
=
ZONE_NAME
\
--ttl
=
300
Execute the transaction.
gcloud
dns
record-sets
transaction
execute
--zone
=
ZONE_NAME
Repeat these steps for each of the following domains. For each
      repetition, change
ZONE_NAME
and
DNS_NAME
to the appropriate values for that
      domain. Keep
PROJECT_ID
and
NETWORK_NAME
the same each time. You already
      completed these steps for
*.notebooks.googleapis.com
.
*.notebooks.googleapis.com
*.notebooks.cloud.google.com
*.notebooks.googleusercontent.com
*.googleapis.com
to run code that interacts with other Google APIs
        and services
Configure the service perimeter
After
configuring the DNS records
, either
create a service
perimeter
or
update an existing
perimeter
to add your project to the service perimeter.
In the VPC network, add a route for the
199.36.153.4/30
range with a
next hop of
Default internet gateway
.
Note:
The
199.36.153.4/30
range is for
restricted.googleapis.com
to
      access APIs that are only VPC Service Controls compatible.
      If you aren't using VPC Service Controls, you can use the
199.36.153.8/30
range for
private.googleapis.com
. For more
      information about Private Google Access, see
Configure
      Private Google Access
.
Use Artifact Registry within your service perimeter
If you want to use Artifact Registry in your service perimeter,
see
Configure restricted access for GKE
private clusters
.
Use Shared VPC
If you are using
Shared VPC
,
you must add the host and the service projects to the service
perimeter. In the host project, you must also grant the
Compute Network User
(
roles/compute.networkUser
)
role to the
Notebooks Service
Agent
from the service project. For more information, see
Manage
service perimeters
.
Access your Vertex AI Workbench instance
To open a Jupyter notebook on your new instance:
In the Google Cloud console,
go to the
Instances
page.
Go to Instances
Next to your instance's name, click
Open JupyterLab
.
In JupyterLab,
select
File
>
New
>
Notebook
.
In the
Select kernel
dialog, choose a kernel,
and then click
Select
.
Your new notebook file opens.
Limitations
The following limitations apply when using VPC Service Controls with
Vertex AI Workbench:
Identity type for ingress and egress policies
When you specify an ingress or egress policy for a service perimeter,
you can't use
ANY_SERVICE_ACCOUNT
or
ANY_USER_ACCOUNT
as an identity type for
all Vertex AI Workbench operations.
Instead, use
ANY_IDENTITY
as the identity type.
Accessing the Vertex AI Workbench proxy from a workstation without internet
To access Vertex AI Workbench instances
from a workstation with limited internet access,
verify with your IT administrator that you can access the following domains:
*.accounts.google.com
*.accounts.youtube.com
*.googleusercontent.com
*.kernels.googleusercontent.com
*.gstatic.com
*.notebooks.cloud.google.com
*.notebooks.googleapis.com
You must have access to these domains for authentication to
Google Cloud. See the previous section,
Configure your DNS entries using Cloud DNS
,
for further configuration information.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/shut-down.txt
Shut down a Vertex AI Workbench instance  |  Google Cloud Documentation
Shut down a Vertex AI Workbench instance
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Shut down a Vertex AI Workbench instance
To stop any processing for a Vertex AI Workbench instance,
you can shut down the instance.
You incur costs for running an instance, in addition to other potential costs
(see
Pricing
).
To stop incurring costs for running an
instance, shut down the instance. You continue to incur other potential costs.
To shut down a Vertex AI Workbench instance,
complete the following steps:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Select the instance that you want to open.
Click
Open JupyterLab
.
It is
important to stop all running processes in case there are operations that
need to complete before you shut down
your Vertex AI Workbench instance, for example,
I/O processes that are writing to disk.
To stop all running processes:
To show all of the processes that are running, select the
Running
Terminals and Kernels
tab.
Next to each running process, click
Shutdown all
.
Close the browser tab or window for
your Vertex AI Workbench instance.
In the Google Cloud console, return to the
Instances
page.
Go to Instances
Select the instance that you want to shut down, and then
click
square
Stop
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/ssh-access.txt
Use SSH to access JupyterLab  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Use SSH to access JupyterLab
This guide describes how to access your Vertex AI Workbench instance's
JupyterLab user interface by using SSH port forwarding.
Set up SSH port forwarding and access the JupyterLab user interface
To set up
SSH port forwarding
,
complete the following steps, and then access your JupyterLab session through a
local browser:
Run the following command by using the
Google Cloud CLI
in
your preferred terminal or in
Cloud Shell
:
gcloud
compute
ssh
\
--project
PROJECT_ID
\
--zone
ZONE
\
INSTANCE_NAME
\
--
-L
8080
:localhost:8080
Replace the following:
PROJECT_ID
: your
Google Cloud project ID
ZONE
: the
zone
where your instance is located
INSTANCE_NAME
: the name of your
instance
If using Cloud Shell to run the command, add a
-4
to
the SSH flags to use IPv4 to connect. For example:
--
-4
-L
LOCAL_PORT
:localhost:
REMOTE_PORT
Access your JupyterLab session through a local browser:
If you ran the command on your local machine, visit
https://localhost:8080
to access JupyterLab.
If you ran the command using
Cloud Shell
,
access JupyterLab through the
Web
Preview on port 8080.
Why you might need to access your instance by using SSH
To get HTTPS access to JupyterLab, your Vertex AI Workbench
instance must have access to a Google Cloud proxy service.
When the instance starts, it attempts to register itself with
the proxy service. If it fails to get proxy access,
your instance prompts you to access JupyterLab through SSH.
The following are common reasons why you might not have HTTPS access to
JupyterLab:
Your JupyterLab instance's proxy-mode metadata setting
is incorrect.
Your network is configured to block internet access for the
virtual machines (VMs) running JupyterLab notebooks.
Your instance doesn't have an external IP address.
Your
VPC Service Controls
settings
block access to
Artifact Registry
.
The following sections show how to resolve these issues.
For changes to take effect, you might need to restart the notebook's VM when
attempting to resolve these issues.
Your JupyterLab instance's proxy-mode metadata setting is incorrect
By default, when you create a Vertex AI Workbench instance,
Vertex AI Workbench adds the proxy-mode metadata setting.
If you change or remove the proxy-mode metadata setting, then
the instance can't connect to the proxy service.
To add or modify the metadata to ensure there's a proxy-mode entry set
to the correct value (for example:
project_editors
), use the
projects.locations.instances.patch
method in the Notebooks API or the
gcloud workbench instances update
command in the Google Cloud SDK.
The network is blocking internet access
Your JupyterLab instance accesses the proxy service through a public URL.
If your Virtual Private Cloud network settings block access to the public internet
or your firewall rules block egress traffic, you must use SSH to access
your Vertex AI Workbench instance.
If possible, you might want to work
with your network and firewall administrators to allow access to your
instance through the public internet.
Your instance doesn't have an external IP address
You might have created your Vertex AI Workbench instance
without an external IP address. If you need to change this,
complete the following steps.
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click the name of the instance that you need to modify.
Click
View VM details
.
Click
Edit
.
In the
Network interfaces
section, expand the network that
you want to have an external IP address.
Click the
External IP address
drop-down menu,
and select the option that you want.
To resolve this issue, you must not choose
None
.
In the
Network interfaces
section, click
Done
.
Click
Save
.
VPC Service Controls settings are blocking access to Artifact Registry
To connect to the proxy service,
your Vertex AI Workbench instance runs an
agent that it downloads from Artifact Registry. Without this agent
your instance cannot connect to the proxy service.
If your VPC Service Controls settings are blocking access to
Artifact Registry, you must add the Artifact Registry
service to the service perimeter of your VPC Service Controls.
Learn more about how service perimeters
work and what services VPC Service Controls can be used
to secure
.
Further troubleshooting
If you are still having trouble connecting, try reviewing the console
logs for your virtual machine. These logs might help you discover why
the Vertex AI Workbench instance is unable
to register with the proxy service.
To access these logs, complete the following steps:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Select the instance that you want to troubleshoot.
In
Logs
, click
Serial port 1 (console)
.
What's next
For tips on resolving other issues,
see the
troubleshooting section on
Vertex AI Workbench instances
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/terraform.txt
Provision Vertex AI Workbench resources with Terraform  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Provision Vertex AI Workbench resources with Terraform
Stay organized with collections
Save and categorize content based on your preferences.
HashiCorp Terraform is an infrastructure-as-code (IaC) tool that lets you
    provision and manage cloud infrastructure. Terraform provides plugins called
providers
that let you interact with cloud providers and other APIs. You can
    use the
Terraform provider for Google Cloud
to provision and manage
    Google Cloud resources, including Vertex AI Workbench.
This page introduces you to using Terraform with Vertex AI Workbench, including an
     introduction to how Terraform works and some resources to help you get started using
     Terraform with Google Cloud. You'll also find links to Terraform reference docs for
     Vertex AI Workbench, code examples, and guides for using Terraform to provision
     Vertex AI Workbench resources.
For instructions on how to get started with Terraform for Google Cloud, see
Install and configure Terraform
or the
Terraform for Google Cloud quickstart
.
How Terraform works
Terraform has a declarative and configuration-oriented syntax, which you can
      use to describe the infrastructure that you want to provision in your
      Google Cloud project. After you author this configuration in one or more
      Terraform configuration files, you can use the Terraform CLI to apply this
      configuration to your Vertex AI Workbench resources.
The following steps explain how Terraform works:
You describe the infrastructure you want to provision in a
Terraform
      configuration file
. You don't need to write code describing how to
      provision the infrastructure. Terraform provisions the infrastructure for you.
You run the
terraform plan
command, which evaluates your configuration and
      generates an execution plan. You can review the plan and make changes as
      needed.
You run the
terraform apply
command, which performs the following
        actions:
It provisions your infrastructure based on your execution plan by invoking
          the corresponding Vertex AI Workbench APIs in the background.
It creates a
Terraform state file
, which is a JSON file that maps the resources
          in your configuration file to the resources in the real-world infrastructure. Terraform uses
          this file to keep a record of the most recent state of your infrastructure, and to determine
          when to create, update, and destroy resources.
When you run
terraform apply
, Terraform uses the mapping in
            the state file to compare the existing infrastructure to the code, and make
            updates as necessary:
If a resource object is defined in the configuration file, but doesn't exist in the
              state file, Terraform creates it.
If a resource object exists in the state file, but has a different
              configuration from your configuration file, Terraform updates the resource
              to match your configuration file.
If a resource object in the state file matches your configuration file,
              Terraform leaves the resource unchanged.
Terraform resources for Vertex AI Workbench
Resources
are the fundamental elements in the Terraform language. Each
resource block describes one or more infrastructure objects, such as virtual
networks or compute instances.
The following table lists the Terraform resources available for
Vertex AI Workbench:
Service
Terraform resources
Data sources
Vertex AI Workbench
google_workbench_instance
google_workbench_instance_iam
google_workbench_instance_iam_policy
Terraform-based guides for Vertex AI Workbench
The following table lists Terraform-based how-to guides and tutorials for
   Vertex AI Workbench:
Guide
Details
Create a
        Vertex AI Workbench instance
Create a Vertex AI Workbench instance with
        a basic configuration.
Manage
        idle shutdown on Terraform
Configure the idle shutdown settings on
        a Vertex AI Workbench instance.
Upgrade a
        custom container
Change the
container_image
field in the
        Terraform configuration to update the container payload.
Create
        an instance with specific metadata
Create a Vertex AI Workbench instance with
        specific metadata.
Terraform modules and blueprints for Vertex AI Workbench
Modules and blueprints help you automate provisioning and managing of
  Google Cloud resources at scale. A
module
is a
  reusable set of Terraform configuration files that creates a logical abstraction
  of Terraform resources. A
blueprint
is a package of deployable and reusable
  modules, and a policy that implements and documents a specific solution.
The following table lists modules and blueprints related to
  Vertex AI Workbench:
Module or blueprint
Details
ai-notebook
This module demonstrates how to protect confidential data in
        a Vertex AI Workbench instance.
What's next
Terraform code samples for Vertex AI Workbench
Terraform on Google Cloud documentation
Google Cloud provider documentation in HashiCorp
Infrastructure as code for Google Cloud
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/upgrade.txt
Upgrade the environment of a Vertex AI Workbench instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Upgrade the environment of an instance
Vertex AI Workbench instances are
Deep Learning VM Images
instances with JupyterLab notebook environments enabled and ready for use.
This page describes how to upgrade the environment of
a Vertex AI Workbench instance.
Reasons to upgrade
You might want to upgrade the environment of
your Vertex AI Workbench instance
for any of the following reasons:
To use new capabilities that are only available in a newer version
of your environment.
To benefit from framework updates, package updates, and bug fixes
that have been implemented in a newer version of your environment.
Upgrade methods
There are two ways to upgrade a Vertex AI Workbench instance:
Automatic upgrade
: Enable auto upgrade when you create
a Vertex AI Workbench instance. During a recurring
time period that you specify, Vertex AI Workbench checks whether
your instance can be upgraded, and if so,
Vertex AI Workbench upgrades your instance.
Manual upgrade
: If
an existing Vertex AI Workbench instance
meets the
requirements
for upgrading, you can upgrade the instance manually.
Requirements and limitations
Backward compatibility with your Vertex AI Workbench isn't
guaranteed.
Make a copy of
your data
before upgrading a Vertex AI Workbench instance.
To determine whether you can upgrade
a specific Vertex AI Workbench instance,
see the following requirements and limitations:
The Notebooks API must be
enabled in the instance's
Google Cloud project
.
For more information, see
List enabled services
and
Enable an API
.
If your Vertex AI Workbench instance is container-based,
Vertex AI Workbench upgrades the OS. The image version depends on
the specific image pulled by your Dockerfile.
To help make sure the upgrade uses the most recent version
of the image, consider using the
latest
tag in
your Dockerfile.
If upgrading your instance is not an option for you,
consider
migrating your data to
a new Vertex AI Workbench instance
.
How the upgrade works
Vertex AI Workbench instances that can be upgraded
are dual-disk, with one boot disk and one data disk.
The upgrade process upgrades the boot disk
to a new image while preserving your data on the data disk.
Which components are upgraded or preserved?
The following table shows which components of
your Vertex AI Workbench
instance are upgraded and which are preserved.
Component
Upgrade result
Machine learning frameworks
Upgraded
Machine learning data
Preserved
Preinstalled dependencies
Upgraded
User-installed libraries
By default, must be reinstalled
        (see
User-installed libraries
)
Local files in the
/home/jupyter
directory
Preserved
Local files in any other
/home/
directory
Not preserved
Preinstalled operating system packages
Upgraded
User-installed operating system packages
Not preserved
GPU drivers
Upgraded
Notebooks
Preserved
User configurations
Preserved
User-installed libraries
By default, Vertex AI Workbench instances store
pip and Conda libraries in the boot disk, which is replaced during an upgrade.
When you install pip libraries, you can include the
--user
flag to
install them in the
/home/jupyter/
directory,
where they are preserved during an upgrade.
By default, if you install pip or Conda libraries in a kernel created from a
custom container, the libraries only persist while the kernel is running.
Each time the kernel is restarted, those libraries will need to be
reinstalled. To install persistent libraries in a custom container,
include the library installations in your Dockerfile. When installing
pip libraries in a kernel created from a custom container, you can include
the
--user
flag so that the libraries will persist until instance restart.
Environment versions
Your Vertex AI Workbench instance has an environment
version number that you
can verify:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
In the list of instances, find the version number of your
instance's environment in the
Version
column.
Vertex AI Workbench updates environments regularly (
see the
Deep Learning VM release
notes
), but with each
released version, not all of the environments are updated.
Vertex AI Workbench only upgrades an instance if there is a newer
environment version for the VM image that your instance is based on.
For information about how to use a specific version to create
a Vertex AI Workbench instance, see
Create a specific version of
a Vertex AI Workbench instance
.
Before you begin
Before you upgrade, complete the following steps.
Check the
release notes
to learn about
updates to newer versions.
Make a copy of
your data
as a backup.
Automatic upgrade
Vertex AI Workbench can automatically upgrade instances that are
running. If your instance is stopped, it doesn't automatically upgrade your
instance, even if you enabled auto upgrade when you created it.
When you enable automatic environment upgrades, you specify
a recurring time period in which Vertex AI Workbench checks
whether the instance can be upgraded, and if it can be, upgrades the instance.
The time period you specify is stored as a
notebook-upgrade-schedule
metadata entry, in
unix-cron
format
, Greenwich Mean Time (GMT).
To check whether an instance can be upgraded,
Vertex AI Workbench uses the API method
checkUpgradability
.
This method checks for a newer version of the image on the instance's
boot disk.
If the instance can be upgraded, Vertex AI Workbench uses an
internal upgrade method to upgrade the instance.
Create a Vertex AI Workbench instance with auto upgrade enabled
To create a Vertex AI Workbench instance with auto upgrade
enabled, select the
Enable environment auto-upgrade
checkbox and set a
schedule when you create the instance.
You can specify auto-upgrade by using the Google Cloud console.
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click
add_box
Create new
.
In the
New instance
dialog, click
Advanced options
.
In the
Create instance
dialog,
in the
Details
section,
provide the following information for your new instance:
Name
: Provide a name for your new instance.
Region
and
Zone
: Select a region and zone for
the new instance. For best network performance,
select the region that is geographically closest to you.
See the available
Vertex AI Workbench locations
.
In the
System health
section, select
Environment auto-upgrade
.
Choose whether to upgrade your notebook
Weekly
or
Monthly
.
In the
Weekday
field, select the option that you want.
In the
Hour
field, choose an hour of the day.
Complete the rest of the instance creation dialog,
and then click
Create
.
Edit the auto upgrade schedule
To edit the auto upgrade schedule after you have created your
Vertex AI Workbench instance, complete the following steps:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click the instance name that needs the schedule change.
On the
Instance details
page,
in the
Environment auto-upgrade
section, edit the schedule.
Click
Submit
to save your changes.
Manual upgrade
You can manually upgrade Vertex AI Workbench instances
that meet the
requirements
.
Check for a newer version of your instance's environment
To check whether a newer version of your instance's environment is available,
access your instance from the Google Cloud console.
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click the instance name that
you want to check for availability of a newer environment version.
On the
Instance details
page, next to
VM details
, click
View in Compute Engine
.
If a newer version of the environment is available, a "This instance needs
to be upgraded" message appears.
Upgrade your instance's environment to a newer version
You can manually upgrade a Vertex AI Workbench instance in
the Google Cloud console.
Note:
Upgrades can affect user operations that are running.
In the Google Cloud console, go to the
Instances
page.
Go to Instances
If your instance isn't running,
start the
instance
.
Vertex AI Workbench can only upgrade instances when
they're running.
Click the instance name that you want to upgrade.
On the
Instance details
page, click
Upgrade
.
Make sure you have
made a copy of the data on your
instance
before
continuing.
After your data is backed up, click
Upgrade
.
Vertex AI Workbench upgrades and starts your instance.
Roll back an upgrade
To roll back an upgrade, complete the following steps:
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Click the instance name that you would like to roll back.
On the
Instance details
page, under
Upgrade history
,
click
Rollback
.
Vertex AI Workbench rolls your instance back to the previous version.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/instances/visualize-data-bigquery.txt
Explore and visualize data in BigQuery from within JupyterLab  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Explore and visualize data in BigQuery from within JupyterLab
This page shows you some examples of how to explore and visualize data
that is stored in BigQuery from within the JupyterLab interface
of your Vertex AI Workbench instance.
Before you begin
If you haven't already,
create
a Vertex AI Workbench instance
.
Required roles
To ensure that your instance's service account has the necessary
      permissions to query data in BigQuery,
    
      ask your administrator to grant your instance's service account the
    
  
  Service Usage Consumer (
roles/serviceusage.serviceUsageConsumer
)
   IAM role on the project.
Important:
You must grant this role
      to your instance's service account,
not
to your user account. Failure to grant the role to the correct principal might result in permission errors.
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
Your administrator might also be able to give your instance's service account
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Open JupyterLab
In the Google Cloud console, go to the
Instances
page.
Go to Instances
Next to your Vertex AI Workbench instance's name,
click
Open JupyterLab
.
Your Vertex AI Workbench instance opens JupyterLab.
Read data from BigQuery
In the next two sections, you read data from BigQuery
that you will use to visualize later. These steps are identical to those
in
Query data in BigQuery from
within JupyterLab
, so if you've completed
them already, you can skip to
Get a summary of data in a BigQuery table
.
Query data by using the %%bigquery magic command
In this section, you write SQL directly in notebook cells and read data from
BigQuery into the Python notebook.
Magic commands that use a single or double percentage character (
%
or
%%
)
let you use minimal syntax to interact with BigQuery within the
notebook. The BigQuery client library for Python is automatically
installed in a Vertex AI Workbench instance. Behind the scenes, the
%%bigquery
magic
command uses the BigQuery client library for Python to run the
given query, convert the results to a pandas DataFrame, optionally save the
results to a variable, and then display the results.
Note
: As of version 1.26.0 of the
google-cloud-bigquery
Python package,
the
BigQuery Storage API
is used by default to download results from the
%%bigquery
magics.
To open a notebook file, select
File
>
New
>
Notebook
.
In the
Select Kernel
dialog, select
Python 3
, and then click
Select
.
Your new IPYNB file opens.
To get the number of regions by country in the
international_top_terms
dataset, enter the following statement:
%%
bigquery
SELECT
country_code
,
country_name
,
COUNT
(
DISTINCT
region_code
)
AS
num_regions
FROM
`
bigquery
-
public
-
data
.
google_trends
.
international_top_terms
`
WHERE
refresh_date
=
DATE_SUB
(
CURRENT_DATE
,
INTERVAL
1
DAY
)
GROUP
BY
country_code
,
country_name
ORDER
BY
num_regions
DESC
;
Click
play_circle_filled
Run cell
.
The output is similar to the following:
Query complete after 0.07s: 100%|██████████| 4/4 [00:00<00:00, 1440.60query/s]
Downloading: 100%|██████████| 41/41 [00:02<00:00, 20.21rows/s]
country_code      country_name    num_regions
0   TR  Turkey         81
1   TH  Thailand       77
2   VN  Vietnam        63
3   JP  Japan          47
4   RO  Romania        42
5   NG  Nigeria        37
6   IN  India          36
7   ID  Indonesia      34
8   CO  Colombia       33
9   MX  Mexico         32
10  BR  Brazil         27
11  EG  Egypt          27
12  UA  Ukraine        27
13  CH  Switzerland    26
14  AR  Argentina      24
15  FR  France         22
16  SE  Sweden         21
17  HU  Hungary        20
18  IT  Italy          20
19  PT  Portugal       20
20  NO  Norway         19
21  FI  Finland        18
22  NZ  New Zealand    17
23  PH  Philippines    17
...
Note:
Your results might differ from what is above as the
google_trends
dataset being queried is refreshed with new data on an ongoing basis.
In the next cell (below the output from the previous cell), enter the
following command to run the same query, but this time save the results to
a new pandas DataFrame that's named
regions_by_country
. You provide that
name by using an argument with the
%%bigquery
magic command.
%%
bigquery
regions_by_country
SELECT
country_code
,
country_name
,
COUNT
(
DISTINCT
region_code
)
AS
num_regions
FROM
`
bigquery
-
public
-
data
.
google_trends
.
international_top_terms
`
WHERE
refresh_date
=
DATE_SUB
(
CURRENT_DATE
,
INTERVAL
1
DAY
)
GROUP
BY
country_code
,
country_name
ORDER
BY
num_regions
DESC
;
Note:
For more information about available arguments for the
%%bigquery
command, see the
client library magics documentation
.
Click
play_circle_filled
Run cell
.
In the next cell, enter the following command to look at the first few
rows of the query results that you just read in:
regions_by_country
.
head
()
Click
play_circle_filled
Run cell
.
The pandas DataFrame
regions_by_country
is ready to plot.
Query data by using the BigQuery client library directly
In this section, you use the BigQuery client library for Python
directly to read data into the Python notebook.
The client library gives you more control over your queries and lets you use
more complex configurations for queries and jobs. The library's integrations
with pandas enable you to combine the power of declarative SQL with imperative
code (Python) to help you analyze, visualize, and transform your data.
Note:
You can use a number of Python data analysis, data wrangling, and
visualization libraries, such as
numpy
,
pandas
,
matplotlib
, and many
others. Several of these libraries are built on top of a DataFrame object.
In the next cell, enter the following Python code to import the
BigQuery client library for Python and initialize a client:
from
google.cloud
import
bigquery
client
=
bigquery
.
Client
()
The BigQuery client is used to send and receive messages
from the BigQuery API.
Click
play_circle_filled
Run cell
.
In the next cell, enter the following code to retrieve the percentage of
daily top terms in the US
top_terms
that overlap across time by number of days apart. The idea here is to look
at each day's top terms and see what percentage of them overlap with the
top terms from the day before, 2 days prior, 3 days prior, and so on (for
all pairs of dates over about a month span).
sql
=
"""
WITH
TopTermsByDate AS (
SELECT DISTINCT refresh_date AS date, term
FROM `bigquery-public-data.google_trends.top_terms`
),
DistinctDates AS (
SELECT DISTINCT date
FROM TopTermsByDate
)
SELECT
DATE_DIFF(Dates2.date, Date1Terms.date, DAY)
AS days_apart,
COUNT(DISTINCT (Dates2.date || Date1Terms.date))
AS num_date_pairs,
COUNT(Date1Terms.term) AS num_date1_terms,
SUM(IF(Date2Terms.term IS NOT NULL, 1, 0))
AS overlap_terms,
SAFE_DIVIDE(
SUM(IF(Date2Terms.term IS NOT NULL, 1, 0)),
COUNT(Date1Terms.term)
) AS pct_overlap_terms
FROM
TopTermsByDate AS Date1Terms
CROSS JOIN
DistinctDates AS Dates2
LEFT JOIN
TopTermsByDate AS Date2Terms
ON
Dates2.date = Date2Terms.date
AND Date1Terms.term = Date2Terms.term
WHERE
Date1Terms.date <= Dates2.date
GROUP BY
days_apart
ORDER BY
days_apart;
"""
pct_overlap_terms_by_days_apart
=
client
.
query
(
sql
).
to_dataframe
()
pct_overlap_terms_by_days_apart
.
head
()
The SQL being used is encapsulated in a Python string and then passed to the
query()
method
to run a query. The
to_dataframe
method
waits for the query to finish and downloads the results to a pandas
DataFrame by using the BigQuery Storage API.
Click
play_circle_filled
Run
cell
.
The first few rows of query results appear below the code cell.
days_apart   num_date_pairs  num_date1_terms overlap_terms   pct_overlap_terms
 0          0             32               800            800            1.000000
 1          1             31               775            203            0.261935
 2          2             30               750             73            0.097333
 3          3             29               725             31            0.042759
 4          4             28               700             23            0.032857
Note:
Your results might differ from what is above as the
google_trends
dataset being queried is refreshed with new data on an ongoing basis.
For more information about using BigQuery client libraries, see
the quickstart
Using client libraries
.
Get a summary of data in a BigQuery table
In this section, you use a notebook shortcut to get summary statistics and
visualizations for all fields of a BigQuery table. This can
be a fast way to profile your data before exploring further.
The BigQuery client library provides a magic command,
%bigquery_stats
, that you can call with a specific table name to provide an
overview of the table and detailed statistics on each of the table's
columns.
In the next cell, enter the following code to run that analysis on the US
top_terms
table
:
%bigquery_stats
bigquery
-
public
-
data
.
google_trends
.
top_terms
Click
play_circle_filled
Run cell
.
After running for some time, an image appears with various statistics on
each of the 7 variables in the
top_terms
table. The following image shows
part of some example output:
Note:
Your results might differ from what is above as the
google_trends
dataset being queried is refreshed with new data on an ongoing basis.
Visualize BigQuery data
In this section, you use plotting capabilities to visualize the results from
the queries that you previously ran in your Jupyter notebook.
In the next cell, enter the following code to use the pandas
DataFrame.plot()
method to create a bar chart that visualizes the results
of the query that returns the number of regions by country:
regions_by_country
.
plot
(
kind
=
"bar"
,
x
=
"country_name"
,
y
=
"num_regions"
,
figsize
=
(
15
,
10
))
Click
play_circle_filled
Run cell
.
The chart is similar to the following:
In the next cell, enter the following code to use the pandas
DataFrame.plot()
method to create a scatter plot that visualizes the
results from the query for the percentage of overlap in the top search terms
by days apart:
pct_overlap_terms_by_days_apart.plot(
  kind="scatter",
  x="days_apart",
  y="pct_overlap_terms",
  s=len(pct_overlap_terms_by_days_apart["num_date_pairs"]) * 20,
  figsize=(15, 10)
  )
Click
play_circle_filled
Run cell
.
The chart is similar to the following. The size of each point reflects
the number of date pairs that are that many days apart in the data. For
example, there are more pairs that are 1 day apart than 30 days apart
because the top search terms are surfaced daily over about a month's time.
For more information about data visualization, see the
pandas documentation
.
What's next
To see an example of exploring and visualizing BigQuery data as part of a comprehensive workflow in Vertex AI Workbench,
      run the "Interactive exploratory analysis of BigQuery data in a notebook" notebook in one of the following
      environments:
Open in Colab
|
Open in Colab Enterprise
|
Open
in Vertex AI Workbench
|
View on GitHub
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/instances.txt
Vertex AI Workbench instances documentation  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Technology areas
More
Cross-product tools
More
Console
Google Cloud Documentation home
Get started with Google Cloud
Product list
Integrated AI assistance with Gemini for Google Cloud
What's new in documentation
Recent product release notes
Cloud Customer Care
Cross-product tools
Access and resource management
Access and resource management overview
Access management
Cloud Identity
Identity Platform
Managed Service for Microsoft Active Delivery
Access Approval
See additional products on overview page
Control and discoverability
Service Catalog
Service Usage
Optimization and service health
Cloud Hub
Recommender
See additional products on overview page
Resource management
Config Connector
See additional products on overview page
Costs and usage management
Costs and usage management overview
Monitor and optimize costs
Cloud Billing
Committed use discounts
Free cloud features and trial offer
FinOps hub
Manage resources and usage
Cloud Quotas
Sustainability
Carbon Footprint
Google Cloud SDK, languages, frameworks, and tools
Google Cloud SDK, languages, frameworks, and tools overview
General authentication guides
Google Cloud SDK client libraries and languages
Google Cloud SDK and client libraries
See additional products on overview page
Languages
C# and .NET
C++
Go
Java
JavaScript and Node.js
PHP
Python
Ruby
Frameworks and tools
Spring
Prometheus
See additional products on overview page
Infrastructure as code
Infrastructure as code overview
Provision deploy and manage infrastructure
Infrastructure Manager
Terraform on Google Cloud
Cloud Development Kit for Terraform (external website)
Google Cloud provider for Pulumi (external website)
Ansible (external website)
Crossplane (external website)
Selected related product
Cloud Deployment Manager
Migration
Migration overview
Migration tools and services
Kf
See additional products on overview page
Generative AI
Overview of generative AI
Vertex AI Studio
Generative AI Models
Google models
Model Garden
Model versions
Open models
Use open models
Google Gemma
Llama
Hugging Face text generation models
Hex-LLM
Partner models
Use partner models
Anthropic Claude
Generate an AI response
Text and code generation
Image generation
Embeddings
Prompt design and engineering
Improve AI responses
Grounding
Tuning
Function calling
Evaluation
Safety
AI/ML orchestration on GKE
AI and ML
Overview of AI and ML
Pre-trained models
Cloud Vision API
Video Intelligence AI
Cloud Natural Language
Timeseries Insights API
Customer service, conversation, and speech
Conversational Agents (Dialogflow CX)
Dialogflow ES
Text-to-Speech
Speech-to-Text
Conversational Insights
Contact Center as a Service
Agent Assist
Agentspace
Conversational AI
Document management
Document AI
Industry-specific products
See listing of products on overview page
Video, images, vision, and augmented reality
Live Stream API
Transcoder API
Vertex AI Vision
Video Stitcher API
Immersive Stream for XR
See additional products on overview page
Search and recommendations
Vertex AI Search
Vector Search
Enterprise Knowledge Graph
Translation
Cloud Translation
Translation Hub
ML model training and development
Automatic training
Vertex AI AutoML Tabular
Vertex AI AutoML Image
Vertex AI AutoML Text
Custom training
Vertex AI Training
Vertex AI Neural Architecture Search
Deep Learning Containers
Deep Learning VM Images
MLOps and production
Data and features
Vertex AI Datasets
Vertex AI Feature Store
Deployment
Vertex AI Prediction
Developer tools
Colab Enterprise
TensorFlow Enterprise
Vertex AI Workbench instances
Model iteration
Vertex AI Experiments
Monitoring and evaluation
Vertex Explainable AI
Vertex AI Model Monitoring
Vertex AI TensorBoard
Orchestration
Vertex AI Pipelines
Vertex AI Model Registry
Accelerators
Cloud TPU
See additional products on overview page
Application development
Overview of application development
API management
API Gateway
API Keys API
Apigee
Apigee Hybrid
Cloud Endpoints
Build and deploy apps with CI/CD
Cloud Build
Artifact Registry
Artifact Analysis
Software supply chain security
Cloud Deploy
Container Registry
Development platforms and tools
App Hub
Application Design Center
Cloud Code
Cloud Code for Cloud Shell
Cloud Code for IntelliJ
Cloud Code for VS Code
Cloud Shell
Cloud Workstations
Google Cloud SDK
Service Infrastructure
Tools for PowerShell
Tools for Visual Studio
See additional products on overview page
Event-driven app creation
Cloud Scheduler
Cloud Tasks
Eventarc
Workflows
Industry-specific APIs
Cloud Healthcare API
See additional products on overview page
Integration
Application Integration
Developer Connect
Integration Connectors
GitLab on Google Cloud
Source code management tools
Cloud Source Repositories
Secure Source Manager
Application hosting
Overview of application hosting infrastructure
Serverless
Cloud Run
Cloud Run functions
App Engine
Container orchestration
Google Kubernetes Engine (GKE)
See additional products on overview page
Fleet management
GKE fleet management
Selected related products
Buildpacks
Blockchain Node Engine
Blockchain RPC
Compute
Overview of compute
Virtual machines
Compute Engine
Capacity Planner
See additional products on overview page
Images
Container-Optimized OS
See additional products on overview page
Block storage
Local SSD
Persistent Disk
Hyperdisk
Migration
Migration Center
Migrate to Virtual Machines
Migrate to Containers
Mainframe Assessment Tool
Mainframe Connector
Dual Run
Workloads
AI Hypercomputer
Cluster Director
Batch
Cloud GPUs
Selected related products
Google Cloud VMware Engine
SAP on Google Cloud
Workload Manager
Cluster Toolkit
Shielded VM
VM Manager
Data analytics
Overview of data analytics
BigQuery
Data analysis
Looker
Looker Studio
See additional products on overview page
Data governance and sharing
Dataplex Universal Catalog
Data Catalog
See additional products on overview page
Data integration and orchestration
BigQuery Data Transfer Service
Cloud Composer
Cloud Data Fusion
Dataform
Datastream
See additional products on overview page
Data migration
BigQuery Migration Service
Data processing
Dataproc
Serverless for Apache Spark
Dataproc Metastore
See additional products on overview page
Streaming
Dataflow
Google Cloud Managed Service for Apache Kafka
Pub/Sub
Pub/Sub Lite
See additional products on overview page
AI and ML
BigQuery ML
Gemini in BigQuery
Gemini in Looker
Selected related products
Blockchain Analytics
Cortex Framework
Earth Engine (other Google site)
Manufacturing Data Engine
Databases
Overview of databases
Non-relational databases
Bigtable
Datastore
Firestore with MongoDB compatibility
Firestore in Native mode
Memorystore for Memcached
Memorystore for Redis
Memorystore for Redis Cluster
Memorystore for Valkey
Spanner Graph
See additional products on overview page
Relational databases
AlloyDB for PostgreSQL
AlloyDB Omni
Spanner
Cloud SQL
Cloud SQL for MySQL
Cloud SQL for PostgreSQL
Cloud SQL for SQL Server
Migration
Database Migration Service
See additional products on overview page
Fleet Management
Database Center
Distributed, hybrid, and multicloud
Overview of distributed, hybrid, and multicloud
Distributed cloud
Google Distributed Cloud air-gapped
Google Distributed Cloud connected
Google Distributed Cloud software for bare metal
Google Distributed Cloud software for VMware
Multicloud
GKE Multi-Cloud
GKE Attached Clusters
GKE on AWS
GKE on Azure
Cloud Location Finder
Knative serving
Fleet management
See product on overview page
Selected related products
Config Controller
Config Sync
Policy Controller
Service Directory
Industry solutions
Overview of industry solutions
Financial services
Anti Money Laundering AI
Healthcare
Cloud Healthcare API
Healthcare Natural Language API
Cloud Life Sciences
See additional products on overview page
Media and entertainment
See products on overview page
Commerce
Vertex AI Search for commerce
Vision API Product Search
Transport fleet, career, and telecom
Cloud Talent Solution
Telecom Subscriber Insights
Telecom Network Automation
Networking
Overview of networking
Connectivity
Core networking
Virtual Private Cloud (VPC)
Cloud NAT
Network Service Tiers
Hybrid networking
Network Connectivity Center
Cloud VPN
Cloud Interconnect
Cloud Router
Scalability
Core services
Cloud DNS
Cloud Load Balancing
Service Extensions
Cloud Domains
CDN
Media CDN
Cloud CDN
Networking security
Cloud NGFW
Secure Access Connect
Network Security Integration
Secure Web Proxy
VPC Service Controls
Google Cloud Armor
Cloud IDS
See additional products on overview page
Networking observability
Network Intelligence Center
VPC Flow Logs
See additional products on overview page
Selected related products
Cloud Service Mesh
Data Transfer Essentials
Observability and monitoring
Overview of observabililty and monitoring
Observability and monitoring
Google Cloud Observability
Cloud Logging
Cloud Monitoring
Cloud Trace
Error Reporting
Observability optimization
Cloud Profiler
Security
Overview of security
General security guides
Security operations
Advisory Notifications
Cyber Insurance Hub
Google Security Operations
Security Command Center
Access management
Access Context Manager
Certificate Authority Service
Identity and Access Management (IAM)
Application security
Binary Authorization
Certificate Manager
Identity-Aware Proxy
reCAPTCHA
Secure Web Proxy
Web Risk
See additional products on overview page
Auditing, monitoring, and logging
Access Transparency
Audit Manager
Cloud Audit Logs
Cloud provider access management
Endpoint Verification
Personalized Service Health
Unified Maintenance
See additional products on overview page
Cloud governance
Assured Workloads
Cloud Asset Inventory
Organization Policy Service
Policy Intelligence
Resource Manager
Data security
Cloud External Key Manager
Cloud HSM
Cloud Key Management Service
Confidential Computing
Secret Manager
Sensitive Data Protection
See additional products on overview page
Network security
Chrome Enterprise Premium
Spectrum Access System
See additional products on overview page
Selected related products
Assured Open Source Software
Sovereign Controls by Partners
T-Systems Sovereign Cloud
Storage
Overview of storage
Backing up your data resources
Backup and DR Service
Backup for GKE
Storage solutions
Cloud Storage
Filestore
NetApp Volumes
Parallelstore
See additional products on overview page
Data transfer
Storage Transfer Service
Transfer Appliance
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Vertex AI Workbench instances
 documentation
Read product documentation
Vertex AI Workbench instances are Jupyter notebook-based development environments
    for the entire data science workflow. Vertex AI Workbench instances are prepackaged with
JupyterLab
and have a preinstalled suite of deep learning packages, including support for the
    TensorFlow and PyTorch frameworks.
Learn more
.
Get started for free
Start your proof of concept with $300 in free credit
Get access to Gemini 2.0 Flash Thinking
Free monthly usage of popular products, including AI APIs and BigQuery
No automatic charges, no commitment
View free product offers
Keep exploring with 20+ always-free products
Access 20+ free products for common use cases, including AI APIs, VMs, data warehouses,
          and more.
Documentation resources
Find quickstarts and guides, review key references, and get help with common issues.
format_list_numbered
Guides
Create a Vertex AI Workbench instance
Introduction to Vertex AI Workbench instances
Query data in BigQuery from within JupyterLab
info
Manage your environment
Add a conda environment
Manage your conda environment
Change machine type and configure GPUs of a Vertex AI Workbench instance
info
Resources
Pricing
Release notes
Get support
Related videos
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/introduction.txt
Introduction to Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Introduction to Vertex AI Workbench
Vertex AI Workbench instances are Jupyter notebook-based development environments
for the entire data science workflow. You can interact with
Vertex AI and other Google Cloud services from within
a Vertex AI Workbench instance's Jupyter notebook.
Vertex AI Workbench integrations and features can make it easier
to access your data, process data faster, schedule notebook runs, and more.
Vertex AI Workbench instances are prepackaged with
JupyterLab
and have a preinstalled suite of deep learning packages,
including support for the TensorFlow and PyTorch
frameworks. You can configure either CPU-only or GPU-enabled instances.
Vertex AI Workbench instances support the ability to sync with a
GitHub
repository.
Vertex AI Workbench instances are protected
by Google Cloud authentication and authorization.
Access to data
You can access your data without leaving the JupyterLab user interface.
In JupyterLab's navigation menu on a Vertex AI Workbench instance,
you can use the Cloud Storage integration
to browse data and other files that you have access to.
See
Access Cloud Storage buckets and files
from within JupyterLab
.
You can also use the BigQuery integration
to browse tables that you have access to, write queries, preview results,
and load data into your notebook. See
Query data in BigQuery
tables from within JupyterLab
.
Execute notebook runs
Use the executor to run a notebook file as a one-time execution or
on a schedule. Choose the specific environment and hardware that you want
your execution to run on. Your notebook's code will run on
Vertex AI custom training, which can make it easier
to do distributed training, optimize hyperparameters, or
schedule continuous training jobs.
You can use parameters in your execution to make specific changes to each run.
For example, you might specify a different dataset to use,
change the learning rate on your model, or change the version of the model.
You can also set a notebook to run on a recurring schedule. Even while your
instance is shut down, Vertex AI Workbench will run your notebook file and
save the results for you to look at and share with others. See
Schedule a notebook run
.
Share insights
Executed notebook runs are stored in a Cloud Storage bucket,
so you can share your insights with others by granting access
to the results. See the
previous section on executing
notebook runs
.
Secure your instance
The following sections describe supported capabilities that can help you
secure your Vertex AI Workbench instance.
VPC
You can deploy your Vertex AI Workbench instance
with the default Google-managed network,
which uses a default VPC network and subnet.
Instead of the default network, you can specify a
VPC network to use with your instance.
To use Vertex AI Workbench within a service perimeter, see
Use a Vertex AI Workbench instance within a service
perimeter
.
Customer-managed encryption keys (CMEK)
By default, Google Cloud automatically
encrypts data when it is at
rest
using encryption keys
managed by Google. If you have specific compliance or regulatory requirements
related to the keys that protect your data, you can use customer-managed
encryption keys (CMEK) with your Vertex AI Workbench instances.
For more information,
see
Customer-managed encryption keys
.
Confidential Computing
Preview
This feature is
        
        subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the
Service Specific
        Terms
.
        
        Pre-GA features are available "as is" and might have limited support.
      
      For more information, see the
launch stage descriptions
.
You can encrypt your data-in-use by using Confidential Computing. To use
Confidential Computing, you enable the Confidential VM service
when you create a Vertex AI Workbench instance. To get started,
see
Create an instance with
Confidential Computing
.
Automated shutdown for idle instances
To help manage costs, Vertex AI Workbench instances shut down after
being idle for a specific time period by default. You can change the
amount of time or turn this feature off. For more information,
see
Idle shutdown
.
Add conda environments
Vertex AI Workbench instances use
kernels
based on conda environments. You can add a conda environment to
your Vertex AI Workbench instance, and the environment appears as
a kernel in your instance's JupyterLab interface.
Adding conda environments lets you use kernels that aren't available in the
default Vertex AI Workbench instance.
For example, you can add conda environments for R and Apache Beam. Or you
can add conda environments for specific earlier versions of the available
frameworks, such as TensorFlow, PyTorch, or Python.
For more information, see
Add a conda environment
.
Custom containers
You can create a Vertex AI Workbench instance based on a custom container.
Start with a Google-provided base container image, and modify it for
your needs. Then create an instance based on your custom container.
For more information, see
Create an instance using a
custom container
.
Dataproc integration
You can process data quickly by running a notebook on
a Dataproc cluster. After your cluster is set up, you can run
a notebook file on it without leaving the JupyterLab user interface.
For more information, see
Create a Dataproc-enabled
instance
.
Reserve VM resources
Use
Compute Engine reservations
to gain a high level of assurance that your Vertex AI Workbench instances
have enough virtual machine (VM) resources to run.
Reservations are a Compute Engine feature. They help make sure that
you have the resources available to create VMs with the same hardware
(memory and vCPUs) and optional resources (GPUs and Local SSD disks)
whenever you need them.
For more information, see
Use reservations
.
Create instances with third party credentials
You can create and manage Vertex AI Workbench instances with
third party credentials provided by Workforce Identity Federation.
Workforce Identity Federation uses your external identity provider (IdP)
to grant a group of users access to Vertex AI Workbench instances
through a proxy.
Access to a Vertex AI Workbench instance is granted by assigning a
workforce pool principal
to the Vertex AI Workbench instance's service account.
For more information, see
Create an instance with
third party credentials
.
Tags for Vertex AI Workbench instances
The underlying VM of a Vertex AI Workbench instance is a
Compute Engine VM. You can add and manage resource tags to your
Vertex AI Workbench instance through its Compute Engine VM.
When you create a Vertex AI Workbench instance,
Vertex AI Workbench attaches the Compute Engine resource tag
vertex-workbench-instances:prod=READ_ONLY
. This resource tag is
only used for internal purposes.
To learn more about managing tags for Compute Engine instances,
see
Manage tags for resources
.
Limitations
Consider the following limitations of
Vertex AI Workbench instances when planning your project:
Third party JupyterLab extensions aren't supported.
When you use
Access Context Manager
and
Chrome Enterprise Premium
to protect Vertex AI Workbench instances with context-aware
access controls, access is evaluated each time the user authenticates
to the instance. For example, access is evaluated the first time
the user accesses JupyterLab and whenever they access it thereafter
if their web browser's cookie has expired.
Using a custom container that isn't derived from the
Google-provided base container
(
gcr.io/deeplearning-platform-release/workbench-container:latest
)
increases the risks of compatibility issues with our services and
isn't supported. Instead, modify the base container to create a
custom container that meets your needs, and then
create an instance using
the custom container
.
Vertex AI Workbench instances expect images from the
cloud-notebooks-managed
project. The list of image names is available at
the creation page in the Google Cloud console. Although the use of custom
virtual machine (VM) images or
Deep Learning VM
images with Vertex AI Workbench instances can be possible,
Vertex AI Workbench doesn't provide any support for unexpected
behaviors or malfunctions when using those images.
The use of a user-managed notebooks image or
managed notebooks image to create a
Vertex AI Workbench instance isn't supported.
You can't edit the underlying VM of a Vertex AI Workbench instance
by using the Google Cloud console or the Compute Engine API. To edit a
Vertex AI Workbench instance's underlying VM, use the
projects.locations.instances.patch
method in the Notebooks API or the
gcloud workbench instances update
command in the Google Cloud SDK.
In instances that use VPC Service Controls, use of the
executor
isn't
supported.
To use accelerators with Vertex AI Workbench instances,
the accelerator type that you want must be available in your instance's
zone. To learn about accelerator availability by zone, see
GPU regions and zones availability
.
What's next
Create a Vertex AI Workbench instance
.
Compare Vertex AI's
notebook solutions
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/access-control.txt
Vertex AI Workbench managed notebooks access control  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Managed notebooks access control
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page describes how to use
Identity and Access Management (IAM)
and an access mode to manage access to
Vertex AI Workbench managed notebooks resources. To manage access to
Vertex AI resources, see the
Vertex AI page on
access control
.
Vertex AI Workbench uses IAM to manage
access to managed notebooks instances
and an access mode to manage access to
each instance's JupyterLab interface.
Control access to an instance with IAM
You can manage access to a managed notebooks instance at the
project level or per instance.
To grant access to resources at the project level, assign one or more
roles
to a principal (user, group, or
service account
).
To grant access to a specific instance, set an IAM policy
on that resource. The policy defines which roles are assigned
to which principals. To learn more, see
Manage access to
a managed notebooks instance
.
Access to an instance can include a broad range of abilities. For example,
you might grant a principal the ability to start, stop, and upgrade
an instance. However, even granting a principal full access to
a managed notebooks instance doesn't grant
the ability to use the instance's JupyterLab interface. See the following
section.
Control access to an instance's JupyterLab interface with the access mode
You control access to a managed notebooks instance's
JupyterLab interface through the instance's access mode.
You set a JupyterLab access mode when you create
a managed notebooks instance.
The access mode can't be changed after the notebook is created.
The JupyterLab access mode determines who can use
the instance's JupyterLab interface.
The access mode also determines which credentials are used when
your instance interacts with other Google Cloud services.
To learn more, see
Manage access to a
managed notebooks instance's
JupyterLab interface
.
Types of IAM roles
There are different types of IAM roles that can be used in
Vertex AI Workbench:
Predefined roles
let you grant a set of related
permissions to your Vertex AI Workbench resources at the project level.
Basic roles
(Owner,
Editor, and Viewer) provide access control to your Vertex AI Workbench
resources at the project level, and are common to all Google Cloud
services.
Custom roles
enable you to choose a
specific set of permissions, create your own role with those permissions,
and grant the role to users in your organization.
To add, update, or remove these roles in your Vertex AI Workbench project,
see the documentation on
managing access to projects, folders, and
organizations
.
Predefined roles for Vertex AI Workbench
Vertex AI Workbench resources are managed through the Notebooks API.
Therefore, Notebooks roles define permissions and access
to the use of Vertex AI Workbench.
Role
Permissions
Notebooks Admin
(
roles/
notebooks.admin
)
Full access to Notebooks, all resources.
Lowest-level resources where you can grant this role:
Instance
aiplatform.
notebookExecutionJobs.*
aiplatform.
notebookExecutionJobs.
create
aiplatform.
notebookExecutionJobs.
delete
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.operations.list
aiplatform.pipelineJobs.create
aiplatform.schedules.*
aiplatform.schedules.create
aiplatform.schedules.delete
aiplatform.schedules.get
aiplatform.schedules.list
aiplatform.schedules.update
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.*
notebooks.environments.create
notebooks.environments.delete
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.
environments.
setIamPolicy
notebooks.executions.create
notebooks.executions.delete
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
executions.
setIamPolicy
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.delete
notebooks.instances.diagnose
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.instances.reset
notebooks.
instances.
setAccelerator
notebooks.
instances.
setIamPolicy
notebooks.instances.setLabels
notebooks.
instances.
setMachineType
notebooks.instances.start
notebooks.instances.stop
notebooks.instances.update
notebooks.
instances.
updateConfig
notebooks.
instances.
updateShieldInstanceConfig
notebooks.instances.upgrade
notebooks.instances.use
notebooks.locations.get
notebooks.locations.list
notebooks.operations.cancel
notebooks.operations.delete
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.delete
notebooks.runtimes.diagnose
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.runtimes.reset
notebooks.
runtimes.
setIamPolicy
notebooks.runtimes.start
notebooks.runtimes.stop
notebooks.runtimes.switch
notebooks.runtimes.update
notebooks.runtimes.upgrade
notebooks.schedules.create
notebooks.schedules.delete
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
notebooks.
schedules.
setIamPolicy
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Legacy Admin
(
roles/
notebooks.legacyAdmin
)
Full access to Notebooks all resources through compute API.
backupdr.
backupPlanAssociations.
createForComputeDisk
backupdr.
backupPlanAssociations.
createForComputeInstance
backupdr.
backupPlanAssociations.
deleteForComputeDisk
backupdr.
backupPlanAssociations.
deleteForComputeInstance
backupdr.
backupPlanAssociations.
fetchForComputeDisk
backupdr.
backupPlanAssociations.
getForComputeDisk
backupdr.
backupPlanAssociations.
list
backupdr.
backupPlanAssociations.
triggerBackupForComputeDisk
backupdr.
backupPlanAssociations.
triggerBackupForComputeInstance
backupdr.
backupPlanAssociations.
updateForComputeDisk
backupdr.
backupPlanAssociations.
updateForComputeInstance
backupdr.backupPlans.get
backupdr.backupPlans.list
backupdr.
backupPlans.
useForComputeDisk
backupdr.
backupPlans.
useForComputeInstance
backupdr.backupVaults.get
backupdr.backupVaults.list
backupdr.locations.list
backupdr.operations.get
backupdr.operations.list
backupdr.
serviceConfig.
initialize
cloudkms.keyHandles.*
cloudkms.keyHandles.create
cloudkms.keyHandles.get
cloudkms.keyHandles.list
cloudkms.operations.get
cloudkms.
projects.
showEffectiveAutokeyConfig
compute.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.create
compute.
addresses.
createInternal
compute.
addresses.
createTagBinding
compute.addresses.delete
compute.
addresses.
deleteInternal
compute.
addresses.
deleteTagBinding
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.addresses.setLabels
compute.addresses.use
compute.addresses.useInternal
compute.advice.calendarMode
compute.autoscalers.create
compute.autoscalers.delete
compute.autoscalers.get
compute.autoscalers.list
compute.autoscalers.update
compute.
backendBuckets.
addSignedUrlKey
compute.backendBuckets.create
compute.
backendBuckets.
createTagBinding
compute.backendBuckets.delete
compute.
backendBuckets.
deleteSignedUrlKey
compute.
backendBuckets.
deleteTagBinding
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.
backendBuckets.
setIamPolicy
compute.
backendBuckets.
setSecurityPolicy
compute.backendBuckets.update
compute.backendBuckets.use
compute.
backendServices.
addSignedUrlKey
compute.backendServices.create
compute.
backendServices.
createTagBinding
compute.backendServices.delete
compute.
backendServices.
deleteSignedUrlKey
compute.
backendServices.
deleteTagBinding
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.
backendServices.
setIamPolicy
compute.
backendServices.
setSecurityPolicy
compute.backendServices.update
compute.backendServices.use
compute.commitments.create
compute.commitments.get
compute.commitments.list
compute.commitments.update
compute.
commitments.
updateReservations
compute.
crossSiteNetworks.
create
compute.
crossSiteNetworks.
delete
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.
crossSiteNetworks.
update
compute.diskSettings.get
compute.diskSettings.update
compute.diskTypes.get
compute.diskTypes.list
compute.
disks.
addResourcePolicies
compute.disks.create
compute.disks.createSnapshot
compute.disks.createTagBinding
compute.disks.delete
compute.disks.deleteTagBinding
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
disks.
removeResourcePolicies
compute.disks.resize
compute.disks.setIamPolicy
compute.disks.setLabels
compute.
disks.
startAsyncReplication
compute.
disks.
stopAsyncReplication
compute.
disks.
stopGroupAsyncReplication
compute.disks.update
compute.disks.updateKmsKey
compute.disks.use
compute.disks.useReadOnly
compute.
externalVpnGateways.
create
compute.
externalVpnGateways.
createTagBinding
compute.
externalVpnGateways.
delete
compute.
externalVpnGateways.
deleteTagBinding
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.
externalVpnGateways.
setLabels
compute.
externalVpnGateways.
use
compute.
firewallPolicies.
cloneRules
compute.
firewallPolicies.
copyRules
compute.
firewallPolicies.
create
compute.
firewallPolicies.
createTagBinding
compute.
firewallPolicies.
delete
compute.
firewallPolicies.
deleteTagBinding
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewallPolicies.move
compute.
firewallPolicies.
setIamPolicy
compute.
firewallPolicies.
update
compute.firewallPolicies.use
compute.firewalls.create
compute.
firewalls.
createTagBinding
compute.firewalls.delete
compute.
firewalls.
deleteTagBinding
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.firewalls.update
compute.forwardingRules.create
compute.
forwardingRules.
createTagBinding
compute.forwardingRules.delete
compute.
forwardingRules.
deleteTagBinding
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.
forwardingRules.
pscCreate
compute.
forwardingRules.
pscDelete
compute.
forwardingRules.
pscSetLabels
compute.
forwardingRules.
pscUpdate
compute.
forwardingRules.
setLabels
compute.
forwardingRules.
setTarget
compute.forwardingRules.update
compute.forwardingRules.use
compute.
futureReservations.
cancel
compute.
futureReservations.
create
compute.
futureReservations.
delete
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.
futureReservations.
setIamPolicy
compute.
futureReservations.
update
compute.globalAddresses.create
compute.
globalAddresses.
createInternal
compute.
globalAddresses.
createTagBinding
compute.globalAddresses.delete
compute.
globalAddresses.
deleteInternal
compute.
globalAddresses.
deleteTagBinding
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalAddresses.
setLabels
compute.globalAddresses.use
compute.
globalForwardingRules.
create
compute.
globalForwardingRules.
createTagBinding
compute.
globalForwardingRules.
delete
compute.
globalForwardingRules.
deleteTagBinding
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalForwardingRules.
pscCreate
compute.
globalForwardingRules.
pscDelete
compute.
globalForwardingRules.
pscSetLabels
compute.
globalForwardingRules.
pscUpdate
compute.
globalForwardingRules.
setLabels
compute.
globalForwardingRules.
setTarget
compute.
globalForwardingRules.
update
compute.
globalNetworkEndpointGroups.
attachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
create
compute.
globalNetworkEndpointGroups.
createTagBinding
compute.
globalNetworkEndpointGroups.
delete
compute.
globalNetworkEndpointGroups.
deleteTagBinding
compute.
globalNetworkEndpointGroups.
detachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.
globalNetworkEndpointGroups.
use
compute.
globalOperations.
delete
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalOperations.
setIamPolicy
compute.
globalPublicDelegatedPrefixes.
create
compute.
globalPublicDelegatedPrefixes.
delete
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.
globalPublicDelegatedPrefixes.
updatePolicy
compute.healthChecks.create
compute.
healthChecks.
createTagBinding
compute.healthChecks.delete
compute.
healthChecks.
deleteTagBinding
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.healthChecks.update
compute.healthChecks.use
compute.
healthChecks.
useReadOnly
compute.
httpHealthChecks.
create
compute.
httpHealthChecks.
createTagBinding
compute.
httpHealthChecks.
delete
compute.
httpHealthChecks.
deleteTagBinding
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.
httpHealthChecks.
update
compute.httpHealthChecks.use
compute.
httpHealthChecks.
useReadOnly
compute.
httpsHealthChecks.
create
compute.
httpsHealthChecks.
createTagBinding
compute.
httpsHealthChecks.
delete
compute.
httpsHealthChecks.
deleteTagBinding
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.
httpsHealthChecks.
update
compute.httpsHealthChecks.use
compute.
httpsHealthChecks.
useReadOnly
compute.images.create
compute.
images.
createTagBinding
compute.images.delete
compute.
images.
deleteTagBinding
compute.images.deprecate
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.images.setIamPolicy
compute.images.setLabels
compute.images.update
compute.images.useReadOnly
compute.
instanceGroupManagers.
create
compute.
instanceGroupManagers.
createTagBinding
compute.
instanceGroupManagers.
delete
compute.
instanceGroupManagers.
deleteTagBinding
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.
instanceGroupManagers.
update
compute.
instanceGroupManagers.
use
compute.instanceGroups.create
compute.
instanceGroups.
createTagBinding
compute.instanceGroups.delete
compute.
instanceGroups.
deleteTagBinding
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceGroups.update
compute.instanceGroups.use
compute.instanceSettings.get
compute.
instanceSettings.
update
compute.
instanceTemplates.
create
compute.
instanceTemplates.
delete
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.
instanceTemplates.
setIamPolicy
compute.
instanceTemplates.
useReadOnly
compute.
instances.
addAccessConfig
compute.
instances.
addNetworkInterface
compute.
instances.
addResourcePolicies
compute.instances.attachDisk
compute.instances.create
compute.
instances.
createTagBinding
compute.instances.delete
compute.
instances.
deleteAccessConfig
compute.
instances.
deleteNetworkInterface
compute.
instances.
deleteTagBinding
compute.instances.detachDisk
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instances.osAdminLogin
compute.instances.osLogin
compute.
instances.
pscInterfaceCreate
compute.
instances.
removeResourcePolicies
compute.instances.reset
compute.instances.resume
compute.
instances.
sendDiagnosticInterrupt
compute.
instances.
setDeletionProtection
compute.
instances.
setDiskAutoDelete
compute.instances.setIamPolicy
compute.instances.setLabels
compute.
instances.
setMachineResources
compute.
instances.
setMachineType
compute.instances.setMetadata
compute.
instances.
setMinCpuPlatform
compute.instances.setName
compute.
instances.
setScheduling
compute.
instances.
setSecurityPolicy
compute.
instances.
setServiceAccount
compute.
instances.
setShieldedInstanceIntegrityPolicy
compute.
instances.
setShieldedVmIntegrityPolicy
compute.instances.setTags
compute.
instances.
simulateMaintenanceEvent
compute.instances.start
compute.
instances.
startWithEncryptionKey
compute.instances.stop
compute.instances.suspend
compute.instances.update
compute.
instances.
updateAccessConfig
compute.
instances.
updateDisplayDevice
compute.
instances.
updateNetworkInterface
compute.
instances.
updateSecurity
compute.
instances.
updateShieldedInstanceConfig
compute.
instances.
updateShieldedVmConfig
compute.instances.use
compute.instances.useReadOnly
compute.
instantSnapshots.
create
compute.
instantSnapshots.
delete
compute.
instantSnapshots.
export
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
instantSnapshots.
setIamPolicy
compute.
instantSnapshots.
setLabels
compute.
instantSnapshots.
useReadOnly
compute.
interconnectAttachmentGroups.
create
compute.
interconnectAttachmentGroups.
delete
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachmentGroups.
patch
compute.
interconnectAttachments.
create
compute.
interconnectAttachments.
createTagBinding
compute.
interconnectAttachments.
delete
compute.
interconnectAttachments.
deleteTagBinding
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.
interconnectAttachments.
setLabels
compute.
interconnectAttachments.
update
compute.
interconnectAttachments.
use
compute.
interconnectGroups.
create
compute.
interconnectGroups.
delete
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectGroups.
patch
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.create
compute.
interconnects.
createTagBinding
compute.interconnects.delete
compute.
interconnects.
deleteTagBinding
compute.interconnects.get
compute.
interconnects.
getMacsecConfig
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.
interconnects.
setLabels
compute.interconnects.update
compute.interconnects.use
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.
licenseCodes.
setIamPolicy
compute.licenses.create
compute.licenses.delete
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.licenses.setIamPolicy
compute.licenses.update
compute.machineImages.create
compute.machineImages.delete
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.
machineImages.
setIamPolicy
compute.
machineImages.
setLabels
compute.
machineImages.
useReadOnly
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.create
compute.multiMig.delete
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.
networkAttachments.
create
compute.
networkAttachments.
createTagBinding
compute.
networkAttachments.
delete
compute.
networkAttachments.
deleteTagBinding
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkAttachments.
setIamPolicy
compute.
networkAttachments.
update
compute.networkAttachments.use
compute.
networkEdgeSecurityServices.
create
compute.
networkEdgeSecurityServices.
createTagBinding
compute.
networkEdgeSecurityServices.
delete
compute.
networkEdgeSecurityServices.
deleteTagBinding
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEdgeSecurityServices.
update
compute.
networkEndpointGroups.
attachNetworkEndpoints
compute.
networkEndpointGroups.
create
compute.
networkEndpointGroups.
createTagBinding
compute.
networkEndpointGroups.
delete
compute.
networkEndpointGroups.
deleteTagBinding
compute.
networkEndpointGroups.
detachNetworkEndpoints
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.
networkEndpointGroups.
use
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.access
compute.networks.addPeering
compute.networks.create
compute.
networks.
createTagBinding
compute.networks.delete
compute.
networks.
deleteTagBinding
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.networks.mirror
compute.networks.removePeering
compute.
networks.
setFirewallPolicy
compute.
networks.
switchToCustomMode
compute.networks.update
compute.networks.updatePeering
compute.networks.updatePolicy
compute.networks.use
compute.networks.useExternalIp
compute.nodeGroups.addNodes
compute.nodeGroups.create
compute.nodeGroups.delete
compute.nodeGroups.deleteNodes
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.
nodeGroups.
performMaintenance
compute.
nodeGroups.
setIamPolicy
compute.
nodeGroups.
setNodeTemplate
compute.
nodeGroups.
simulateMaintenanceEvent
compute.nodeGroups.update
compute.nodeTemplates.create
compute.nodeTemplates.delete
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.
nodeTemplates.
setIamPolicy
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
disableXpnHost
compute.
organizations.
disableXpnResource
compute.
organizations.
enableXpnHost
compute.
organizations.
enableXpnResource
compute.
organizations.
listAssociations
compute.
organizations.
setFirewallPolicy
compute.
organizations.
setSecurityPolicy
compute.
oslogin.
updateExternalUser
compute.
packetMirrorings.
create
compute.
packetMirrorings.
createTagBinding
compute.
packetMirrorings.
delete
compute.
packetMirrorings.
deleteTagBinding
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.
packetMirrorings.
update
compute.previewFeatures.get
compute.previewFeatures.list
compute.previewFeatures.update
compute.projects.get
compute.
projects.
setCloudArmorTier
compute.
projects.
setCommonInstanceMetadata
compute.
projects.
setDefaultNetworkTier
compute.
projects.
setDefaultServiceAccount
compute.
projects.
setManagedProtectionTier
compute.
projects.
setUsageExportBucket
compute.
publicAdvertisedPrefixes.
create
compute.
publicAdvertisedPrefixes.
delete
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicAdvertisedPrefixes.
update
compute.
publicAdvertisedPrefixes.
updatePolicy
compute.
publicDelegatedPrefixes.
announce
compute.
publicDelegatedPrefixes.
create
compute.
publicDelegatedPrefixes.
createTagBinding
compute.
publicDelegatedPrefixes.
delete
compute.
publicDelegatedPrefixes.
deleteTagBinding
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
publicDelegatedPrefixes.
update
compute.
publicDelegatedPrefixes.
updatePolicy
compute.
publicDelegatedPrefixes.
use
compute.
publicDelegatedPrefixes.
withdraw
compute.
regionBackendBuckets.
create
compute.
regionBackendBuckets.
createTagBinding
compute.
regionBackendBuckets.
delete
compute.
regionBackendBuckets.
deleteTagBinding
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendBuckets.
setIamPolicy
compute.
regionBackendBuckets.
update
compute.
regionBackendBuckets.
use
compute.
regionBackendServices.
create
compute.
regionBackendServices.
createTagBinding
compute.
regionBackendServices.
delete
compute.
regionBackendServices.
deleteTagBinding
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionBackendServices.
setIamPolicy
compute.
regionBackendServices.
setSecurityPolicy
compute.
regionBackendServices.
update
compute.
regionBackendServices.
use
compute.
regionCompositeHealthChecks.
create
compute.
regionCompositeHealthChecks.
delete
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionCompositeHealthChecks.
update
compute.
regionFirewallPolicies.
cloneRules
compute.
regionFirewallPolicies.
create
compute.
regionFirewallPolicies.
createTagBinding
compute.
regionFirewallPolicies.
delete
compute.
regionFirewallPolicies.
deleteTagBinding
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionFirewallPolicies.
setIamPolicy
compute.
regionFirewallPolicies.
update
compute.
regionFirewallPolicies.
use
compute.
regionHealthAggregationPolicies.
create
compute.
regionHealthAggregationPolicies.
delete
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthAggregationPolicies.
update
compute.
regionHealthCheckServices.
create
compute.
regionHealthCheckServices.
delete
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.
regionHealthCheckServices.
update
compute.
regionHealthCheckServices.
use
compute.
regionHealthChecks.
create
compute.
regionHealthChecks.
createTagBinding
compute.
regionHealthChecks.
delete
compute.
regionHealthChecks.
deleteTagBinding
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthChecks.
update
compute.regionHealthChecks.use
compute.
regionHealthChecks.
useReadOnly
compute.
regionHealthSources.
create
compute.
regionHealthSources.
delete
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionHealthSources.
update
compute.
regionNetworkEndpointGroups.
attachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
create
compute.
regionNetworkEndpointGroups.
createTagBinding
compute.
regionNetworkEndpointGroups.
delete
compute.
regionNetworkEndpointGroups.
deleteTagBinding
compute.
regionNetworkEndpointGroups.
detachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNetworkEndpointGroups.
use
compute.
regionNotificationEndpoints.
create
compute.
regionNotificationEndpoints.
delete
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.
regionNotificationEndpoints.
update
compute.
regionNotificationEndpoints.
use
compute.
regionOperations.
delete
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionOperations.
setIamPolicy
compute.
regionSecurityPolicies.
create
compute.
regionSecurityPolicies.
createTagBinding
compute.
regionSecurityPolicies.
delete
compute.
regionSecurityPolicies.
deleteTagBinding
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSecurityPolicies.
update
compute.
regionSecurityPolicies.
use
compute.
regionSslCertificates.
create
compute.
regionSslCertificates.
createTagBinding
compute.
regionSslCertificates.
delete
compute.
regionSslCertificates.
deleteTagBinding
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.
regionSslPolicies.
create
compute.
regionSslPolicies.
createTagBinding
compute.
regionSslPolicies.
delete
compute.
regionSslPolicies.
deleteTagBinding
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionSslPolicies.
update
compute.regionSslPolicies.use
compute.
regionTargetHttpProxies.
create
compute.
regionTargetHttpProxies.
createTagBinding
compute.
regionTargetHttpProxies.
delete
compute.
regionTargetHttpProxies.
deleteTagBinding
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpProxies.
setUrlMap
compute.
regionTargetHttpProxies.
use
compute.
regionTargetHttpsProxies.
create
compute.
regionTargetHttpsProxies.
createTagBinding
compute.
regionTargetHttpsProxies.
delete
compute.
regionTargetHttpsProxies.
deleteTagBinding
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
setSslCertificates
compute.
regionTargetHttpsProxies.
setUrlMap
compute.
regionTargetHttpsProxies.
update
compute.
regionTargetHttpsProxies.
use
compute.
regionTargetTcpProxies.
create
compute.
regionTargetTcpProxies.
createTagBinding
compute.
regionTargetTcpProxies.
delete
compute.
regionTargetTcpProxies.
deleteTagBinding
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.
regionTargetTcpProxies.
use
compute.regionUrlMaps.create
compute.
regionUrlMaps.
createTagBinding
compute.regionUrlMaps.delete
compute.
regionUrlMaps.
deleteTagBinding
compute.regionUrlMaps.get
compute.
regionUrlMaps.
invalidateCache
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.update
compute.regionUrlMaps.use
compute.regionUrlMaps.validate
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationBlocks.
performMaintenance
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.
reservationSubBlocks.
performMaintenance
compute.
reservationSubBlocks.
reportFaulty
compute.reservations.create
compute.reservations.delete
compute.reservations.get
compute.reservations.list
compute.
reservations.
performMaintenance
compute.reservations.resize
compute.reservations.update
compute.
resourcePolicies.
create
compute.
resourcePolicies.
delete
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.
resourcePolicies.
setIamPolicy
compute.
resourcePolicies.
update
compute.resourcePolicies.use
compute.
resourcePolicies.
useReadOnly
compute.rolloutPlans.create
compute.rolloutPlans.delete
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.cancel
compute.rollouts.delete
compute.rollouts.get
compute.rollouts.list
compute.routers.create
compute.
routers.
createTagBinding
compute.routers.delete
compute.
routers.
deleteRoutePolicy
compute.
routers.
deleteTagBinding
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routers.update
compute.
routers.
updateRoutePolicy
compute.routers.use
compute.routes.create
compute.
routes.
createTagBinding
compute.routes.delete
compute.
routes.
deleteTagBinding
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.
securityPolicies.
addAssociation
compute.
securityPolicies.
copyRules
compute.
securityPolicies.
create
compute.
securityPolicies.
createTagBinding
compute.
securityPolicies.
delete
compute.
securityPolicies.
deleteTagBinding
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.securityPolicies.move
compute.
securityPolicies.
removeAssociation
compute.
securityPolicies.
setLabels
compute.
securityPolicies.
update
compute.securityPolicies.use
compute.
serviceAttachments.
create
compute.
serviceAttachments.
createTagBinding
compute.
serviceAttachments.
delete
compute.
serviceAttachments.
deleteTagBinding
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.
serviceAttachments.
setIamPolicy
compute.
serviceAttachments.
update
compute.serviceAttachments.use
compute.snapshotSettings.get
compute.
snapshotSettings.
update
compute.snapshots.create
compute.
snapshots.
createTagBinding
compute.snapshots.delete
compute.
snapshots.
deleteTagBinding
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.snapshots.setIamPolicy
compute.snapshots.setLabels
compute.snapshots.updateKmsKey
compute.snapshots.useReadOnly
compute.spotAssistants.get
compute.sslCertificates.create
compute.
sslCertificates.
createTagBinding
compute.sslCertificates.delete
compute.
sslCertificates.
deleteTagBinding
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.create
compute.
sslPolicies.
createTagBinding
compute.sslPolicies.delete
compute.
sslPolicies.
deleteTagBinding
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.sslPolicies.update
compute.sslPolicies.use
compute.storagePools.create
compute.storagePools.delete
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.
storagePools.
setIamPolicy
compute.storagePools.update
compute.storagePools.use
compute.subnetworks.create
compute.
subnetworks.
createTagBinding
compute.subnetworks.delete
compute.
subnetworks.
deleteTagBinding
compute.
subnetworks.
expandIpCidrRange
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.subnetworks.mirror
compute.
subnetworks.
setIamPolicy
compute.
subnetworks.
setPrivateIpGoogleAccess
compute.subnetworks.update
compute.subnetworks.use
compute.
subnetworks.
useExternalIp
compute.
subnetworks.
usePeerMigration
compute.
targetGrpcProxies.
create
compute.
targetGrpcProxies.
createTagBinding
compute.
targetGrpcProxies.
delete
compute.
targetGrpcProxies.
deleteTagBinding
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.
targetGrpcProxies.
update
compute.targetGrpcProxies.use
compute.
targetHttpProxies.
create
compute.
targetHttpProxies.
createTagBinding
compute.
targetHttpProxies.
delete
compute.
targetHttpProxies.
deleteTagBinding
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.
targetHttpProxies.
setUrlMap
compute.
targetHttpProxies.
update
compute.targetHttpProxies.use
compute.
targetHttpsProxies.
create
compute.
targetHttpsProxies.
createTagBinding
compute.
targetHttpsProxies.
delete
compute.
targetHttpsProxies.
deleteTagBinding
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.
targetHttpsProxies.
setCertificateMap
compute.
targetHttpsProxies.
setQuicOverride
compute.
targetHttpsProxies.
setSslCertificates
compute.
targetHttpsProxies.
setSslPolicy
compute.
targetHttpsProxies.
setUrlMap
compute.
targetHttpsProxies.
update
compute.targetHttpsProxies.use
compute.targetInstances.create
compute.
targetInstances.
createTagBinding
compute.targetInstances.delete
compute.
targetInstances.
deleteTagBinding
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.
targetInstances.
setSecurityPolicy
compute.targetInstances.use
compute.
targetPools.
addHealthCheck
compute.
targetPools.
addInstance
compute.targetPools.create
compute.
targetPools.
createTagBinding
compute.targetPools.delete
compute.
targetPools.
deleteTagBinding
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.
targetPools.
removeHealthCheck
compute.
targetPools.
removeInstance
compute.
targetPools.
setSecurityPolicy
compute.targetPools.update
compute.targetPools.use
compute.
targetSslProxies.
create
compute.
targetSslProxies.
createTagBinding
compute.
targetSslProxies.
delete
compute.
targetSslProxies.
deleteTagBinding
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.
targetSslProxies.
setBackendService
compute.
targetSslProxies.
setCertificateMap
compute.
targetSslProxies.
setProxyHeader
compute.
targetSslProxies.
setSslCertificates
compute.
targetSslProxies.
setSslPolicy
compute.
targetSslProxies.
update
compute.targetSslProxies.use
compute.
targetTcpProxies.
create
compute.
targetTcpProxies.
createTagBinding
compute.
targetTcpProxies.
delete
compute.
targetTcpProxies.
deleteTagBinding
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.
targetTcpProxies.
update
compute.targetTcpProxies.use
compute.
targetVpnGateways.
create
compute.
targetVpnGateways.
createTagBinding
compute.
targetVpnGateways.
delete
compute.
targetVpnGateways.
deleteTagBinding
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.
targetVpnGateways.
setLabels
compute.targetVpnGateways.use
compute.urlMaps.create
compute.
urlMaps.
createTagBinding
compute.urlMaps.delete
compute.
urlMaps.
deleteTagBinding
compute.urlMaps.get
compute.
urlMaps.
invalidateCache
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.update
compute.urlMaps.use
compute.urlMaps.validate
compute.vpnGateways.create
compute.
vpnGateways.
createTagBinding
compute.vpnGateways.delete
compute.
vpnGateways.
deleteTagBinding
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnGateways.setLabels
compute.vpnGateways.use
compute.vpnTunnels.create
compute.
vpnTunnels.
createTagBinding
compute.vpnTunnels.delete
compute.
vpnTunnels.
deleteTagBinding
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.vpnTunnels.setLabels
compute.wireGroups.create
compute.wireGroups.delete
compute.wireGroups.get
compute.wireGroups.list
compute.wireGroups.update
compute.zoneOperations.delete
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.
zoneOperations.
setIamPolicy
compute.zones.get
compute.zones.list
notebooks.*
notebooks.environments.create
notebooks.environments.delete
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.
environments.
setIamPolicy
notebooks.executions.create
notebooks.executions.delete
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
executions.
setIamPolicy
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.delete
notebooks.instances.diagnose
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.instances.reset
notebooks.
instances.
setAccelerator
notebooks.
instances.
setIamPolicy
notebooks.instances.setLabels
notebooks.
instances.
setMachineType
notebooks.instances.start
notebooks.instances.stop
notebooks.instances.update
notebooks.
instances.
updateConfig
notebooks.
instances.
updateShieldInstanceConfig
notebooks.instances.upgrade
notebooks.instances.use
notebooks.locations.get
notebooks.locations.list
notebooks.operations.cancel
notebooks.operations.delete
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.delete
notebooks.runtimes.diagnose
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.runtimes.reset
notebooks.
runtimes.
setIamPolicy
notebooks.runtimes.start
notebooks.runtimes.stop
notebooks.runtimes.switch
notebooks.runtimes.update
notebooks.runtimes.upgrade
notebooks.schedules.create
notebooks.schedules.delete
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
notebooks.
schedules.
setIamPolicy
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Legacy Viewer
(
roles/
notebooks.legacyViewer
)
Read-only access to Notebooks all resources through compute API.
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
instances.
checkUpgradability
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.locations.*
notebooks.locations.get
notebooks.locations.list
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Runner
(
roles/
notebooks.runner
)
Restricted access for running scheduled Notebooks.
aiplatform.
notebookExecutionJobs.*
aiplatform.
notebookExecutionJobs.
create
aiplatform.
notebookExecutionJobs.
delete
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.operations.list
aiplatform.pipelineJobs.create
aiplatform.schedules.*
aiplatform.schedules.create
aiplatform.schedules.delete
aiplatform.schedules.get
aiplatform.schedules.list
aiplatform.schedules.update
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.executions.create
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.locations.*
notebooks.locations.get
notebooks.locations.list
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.schedules.create
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
AI Platform Notebooks Service Agent
(
roles/
notebooks.serviceAgent
)
Provide access for notebooks service agent to manage notebook instances in user projects
Warning:
Do not grant service agent roles to any principals except
service agents
.
aiplatform.customJobs.cancel
aiplatform.customJobs.create
aiplatform.customJobs.get
aiplatform.customJobs.list
aiplatform.
notebookExecutionJobs.*
aiplatform.
notebookExecutionJobs.
create
aiplatform.
notebookExecutionJobs.
delete
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.operations.list
aiplatform.pipelineJobs.create
aiplatform.schedules.*
aiplatform.schedules.create
aiplatform.schedules.delete
aiplatform.schedules.get
aiplatform.schedules.list
aiplatform.schedules.update
backupdr.
backupPlanAssociations.
createForComputeDisk
backupdr.
backupPlanAssociations.
createForComputeInstance
backupdr.
backupPlanAssociations.
deleteForComputeDisk
backupdr.
backupPlanAssociations.
deleteForComputeInstance
backupdr.
backupPlanAssociations.
fetchForComputeDisk
backupdr.
backupPlanAssociations.
getForComputeDisk
backupdr.
backupPlanAssociations.
list
backupdr.
backupPlanAssociations.
triggerBackupForComputeDisk
backupdr.
backupPlanAssociations.
triggerBackupForComputeInstance
backupdr.
backupPlanAssociations.
updateForComputeDisk
backupdr.
backupPlanAssociations.
updateForComputeInstance
backupdr.backupPlans.get
backupdr.backupPlans.list
backupdr.
backupPlans.
useForComputeDisk
backupdr.
backupPlans.
useForComputeInstance
backupdr.backupVaults.get
backupdr.backupVaults.list
backupdr.locations.list
backupdr.operations.get
backupdr.operations.list
backupdr.
serviceConfig.
initialize
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.
addresses.
createInternal
compute.
addresses.
deleteInternal
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.addresses.use
compute.addresses.useInternal
compute.autoscalers.*
compute.autoscalers.create
compute.autoscalers.delete
compute.autoscalers.get
compute.autoscalers.list
compute.autoscalers.update
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.*
compute.
disks.
addResourcePolicies
compute.disks.create
compute.disks.createSnapshot
compute.disks.createTagBinding
compute.disks.delete
compute.disks.deleteTagBinding
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
disks.
removeResourcePolicies
compute.disks.resize
compute.disks.setIamPolicy
compute.disks.setLabels
compute.
disks.
startAsyncReplication
compute.
disks.
stopAsyncReplication
compute.
disks.
stopGroupAsyncReplication
compute.disks.update
compute.disks.updateKmsKey
compute.disks.use
compute.disks.useReadOnly
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.globalAddresses.use
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.*
compute.
globalNetworkEndpointGroups.
attachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
create
compute.
globalNetworkEndpointGroups.
createTagBinding
compute.
globalNetworkEndpointGroups.
delete
compute.
globalNetworkEndpointGroups.
deleteTagBinding
compute.
globalNetworkEndpointGroups.
detachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.
globalNetworkEndpointGroups.
use
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.*
compute.images.create
compute.
images.
createTagBinding
compute.images.delete
compute.
images.
deleteTagBinding
compute.images.deprecate
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.images.setIamPolicy
compute.images.setLabels
compute.images.update
compute.images.useReadOnly
compute.
instanceGroupManagers.*
compute.
instanceGroupManagers.
create
compute.
instanceGroupManagers.
createTagBinding
compute.
instanceGroupManagers.
delete
compute.
instanceGroupManagers.
deleteTagBinding
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.
instanceGroupManagers.
update
compute.
instanceGroupManagers.
use
compute.instanceGroups.*
compute.instanceGroups.create
compute.
instanceGroups.
createTagBinding
compute.instanceGroups.delete
compute.
instanceGroups.
deleteTagBinding
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceGroups.update
compute.instanceGroups.use
compute.instanceSettings.*
compute.instanceSettings.get
compute.
instanceSettings.
update
compute.instanceTemplates.*
compute.
instanceTemplates.
create
compute.
instanceTemplates.
delete
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.
instanceTemplates.
setIamPolicy
compute.
instanceTemplates.
useReadOnly
compute.instances.*
compute.
instances.
addAccessConfig
compute.
instances.
addNetworkInterface
compute.
instances.
addResourcePolicies
compute.instances.attachDisk
compute.instances.create
compute.
instances.
createTagBinding
compute.instances.delete
compute.
instances.
deleteAccessConfig
compute.
instances.
deleteNetworkInterface
compute.
instances.
deleteTagBinding
compute.instances.detachDisk
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instances.osAdminLogin
compute.instances.osLogin
compute.
instances.
pscInterfaceCreate
compute.
instances.
removeResourcePolicies
compute.instances.reset
compute.instances.resume
compute.
instances.
sendDiagnosticInterrupt
compute.
instances.
setDeletionProtection
compute.
instances.
setDiskAutoDelete
compute.instances.setIamPolicy
compute.instances.setLabels
compute.
instances.
setMachineResources
compute.
instances.
setMachineType
compute.instances.setMetadata
compute.
instances.
setMinCpuPlatform
compute.instances.setName
compute.
instances.
setScheduling
compute.
instances.
setSecurityPolicy
compute.
instances.
setServiceAccount
compute.
instances.
setShieldedInstanceIntegrityPolicy
compute.
instances.
setShieldedVmIntegrityPolicy
compute.instances.setTags
compute.
instances.
simulateMaintenanceEvent
compute.instances.start
compute.
instances.
startWithEncryptionKey
compute.instances.stop
compute.instances.suspend
compute.instances.update
compute.
instances.
updateAccessConfig
compute.
instances.
updateDisplayDevice
compute.
instances.
updateNetworkInterface
compute.
instances.
updateSecurity
compute.
instances.
updateShieldedInstanceConfig
compute.
instances.
updateShieldedVmConfig
compute.instances.use
compute.instances.useReadOnly
compute.instantSnapshots.*
compute.
instantSnapshots.
create
compute.
instantSnapshots.
delete
compute.
instantSnapshots.
export
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
instantSnapshots.
setIamPolicy
compute.
instantSnapshots.
setLabels
compute.
instantSnapshots.
useReadOnly
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.*
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.
licenseCodes.
setIamPolicy
compute.licenses.*
compute.licenses.create
compute.licenses.delete
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.licenses.setIamPolicy
compute.licenses.update
compute.machineImages.*
compute.machineImages.create
compute.machineImages.delete
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.
machineImages.
setIamPolicy
compute.
machineImages.
setLabels
compute.
machineImages.
useReadOnly
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.*
compute.multiMig.create
compute.multiMig.delete
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.*
compute.
networkEndpointGroups.
attachNetworkEndpoints
compute.
networkEndpointGroups.
create
compute.
networkEndpointGroups.
createTagBinding
compute.
networkEndpointGroups.
delete
compute.
networkEndpointGroups.
deleteTagBinding
compute.
networkEndpointGroups.
detachNetworkEndpoints
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.
networkEndpointGroups.
use
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.networks.use
compute.networks.useExternalIp
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
projects.
setCommonInstanceMetadata
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.*
compute.
regionNetworkEndpointGroups.
attachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
create
compute.
regionNetworkEndpointGroups.
createTagBinding
compute.
regionNetworkEndpointGroups.
delete
compute.
regionNetworkEndpointGroups.
deleteTagBinding
compute.
regionNetworkEndpointGroups.
detachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNetworkEndpointGroups.
use
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.*
compute.
resourcePolicies.
create
compute.
resourcePolicies.
delete
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.
resourcePolicies.
setIamPolicy
compute.
resourcePolicies.
update
compute.resourcePolicies.use
compute.
resourcePolicies.
useReadOnly
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.*
compute.snapshots.create
compute.
snapshots.
createTagBinding
compute.snapshots.delete
compute.
snapshots.
deleteTagBinding
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.snapshots.setIamPolicy
compute.snapshots.setLabels
compute.snapshots.updateKmsKey
compute.snapshots.useReadOnly
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.storagePools.use
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.subnetworks.use
compute.
subnetworks.
useExternalIp
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
dataproc.clusters.get
dataproc.clusters.use
dataproc.jobs.cancel
dataproc.jobs.create
dataproc.jobs.delete
dataproc.jobs.get
dataproc.jobs.list
dataproc.jobs.update
iam.serviceAccounts.actAs
iam.serviceAccounts.get
iam.
serviceAccounts.
getAccessToken
iam.serviceAccounts.list
ml.jobs.create
ml.jobs.get
ml.jobs.list
notebooks.*
notebooks.environments.create
notebooks.environments.delete
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.
environments.
setIamPolicy
notebooks.executions.create
notebooks.executions.delete
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
executions.
setIamPolicy
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.delete
notebooks.instances.diagnose
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.instances.reset
notebooks.
instances.
setAccelerator
notebooks.
instances.
setIamPolicy
notebooks.instances.setLabels
notebooks.
instances.
setMachineType
notebooks.instances.start
notebooks.instances.stop
notebooks.instances.update
notebooks.
instances.
updateConfig
notebooks.
instances.
updateShieldInstanceConfig
notebooks.instances.upgrade
notebooks.instances.use
notebooks.locations.get
notebooks.locations.list
notebooks.operations.cancel
notebooks.operations.delete
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.delete
notebooks.runtimes.diagnose
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.runtimes.reset
notebooks.
runtimes.
setIamPolicy
notebooks.runtimes.start
notebooks.runtimes.stop
notebooks.runtimes.switch
notebooks.runtimes.update
notebooks.runtimes.upgrade
notebooks.schedules.create
notebooks.schedules.delete
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
notebooks.
schedules.
setIamPolicy
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Viewer
(
roles/
notebooks.viewer
)
Read-only access to Notebooks, all resources.
Lowest-level resources where you can grant this role:
Instance
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.schedules.get
aiplatform.schedules.list
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
instances.
checkUpgradability
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.locations.*
notebooks.locations.get
notebooks.locations.list
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Basic roles
The older Google Cloud
basic roles
are common to all Google Cloud services. These roles are Owner, Editor,
and Viewer.
The basic roles provide permissions across Google Cloud, not just for
Vertex AI Workbench. For this reason, you should
use Vertex AI Workbench roles whenever possible.
Custom roles
If the predefined IAM roles for Vertex AI Workbench
don't meet your needs, you can define custom roles. Custom roles enable you
to choose a specific set of permissions, create your own role with
those permissions, and grant the role to users in your organization.
For more information, see
Understanding
IAM custom roles
.
Project-level access versus resource-level policies
A resource inherits all policies from its
ancestry
.
A
policy
set at the resource level doesn't
affect project-level policies.  You can use project-level access and
resource-level policies to customize permissions.
For example, you can grant users
roles/notebooks.viewer
permissions
at the project level so that they can view all
Vertex AI Workbench resources in the project,
and then you can grant each user
roles/notebooks.admin
permissions
on a specific managed notebooks instance so that they
have all of the
admin
abilities to administer that instance.
Not all Vertex AI Workbench predefined roles and resources support
resource-level policies. To see which roles can be used on which resources,
view the descriptions
for each role.
Changes to the ability to access a resource take time to propagate. For more
information, see
Access change
propagation
.
About service accounts
A
service account
is a special account used by an application or a virtual machine (VM)
instance, not a person.
You can create and assign permissions to service
accounts to grant specific permissions to a resource or application.
In a managed notebooks instance, if your notebook
runs code that interacts with Vertex AI or
other Google Cloud services, you can use a service account
with specific IAM roles to
authenticate your managed notebooks instance
to those services.
Service accounts are identified by an email address.
What's next
Grant a principal access to
a managed notebooks instance.
Grant a principal access to
JupyterLab.
Learn more about IAM
.
Learn how to
create and manage custom IAM
roles
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/audit-logging.txt
Managed notebooks audit logging  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Managed notebooks audit logging
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
If you're looking for information on audit logs created by
Vertex AI, see the
Vertex AI page on
audit logging
.
This document describes the audit logs created by Vertex AI Workbench as part of
Cloud Audit Logs
.
Overview
Google Cloud services write audit logs to help you answer the questions, "Who
did what, where, and when?" within your Google Cloud resources.
Your Google Cloud projects contain only the audit logs for resources that are
directly within the Google Cloud project. Other Google Cloud resources,
such as folders, organizations, and billing accounts, contain the audit logs for
the entity itself.
For a general overview of Cloud Audit Logs, see
Cloud Audit Logs overview
. For a deeper understanding
of the audit log format, see
Understand audit logs
.
Available audit logs
The following types of audit logs are available for Vertex AI Workbench:
Admin Activity audit logs
Includes "admin write" operations that write metadata or configuration
        information.
You can't disable Admin Activity audit logs.
Data Access audit logs
Includes "admin read" operations that read metadata or configuration
        information.
        
        Also includes "data read" and "data write" operations that
        read or write user-provided data.
To receive Data Access audit logs, you must
explicitly enable
them.
For fuller descriptions of the audit log types, see
Types of audit logs
.
Audited operations
The following table summarizes which API operations correspond to each audit log
type in Vertex AI Workbench:
Audit logs category
Vertex AI Workbench operations
Admin Activity audit logs
CreateInstance
RegisterInstance
SetInstanceAccelerator
SetInstanceMachineType
SetInstanceLabels
DeleteInstance
StartInstance
StopInstance
ResetInstance
UpgradeInstance
DiagnoseInstance
CreateEnvironment
DeleteEnvironment
ReportInstanceInfo
UpgradeInstanceInternal
UpdateShieldInstanceConfig
CreateRuntime
DeleteRuntime
StartRuntime
StopRuntime
ResetRuntime
UpgradeRuntime
RollbackRuntime
UpdateRuntime
DiagnoseRuntime
Data Access (ADMIN_READ) audit logs
ListInstances
GetInstance
ListEnvironments
GetEnvironment
GetHealth
ListRun
ListSchedules
GetRun
GetSchedules
RuntimeList
GetRuntime
Audit log format
Audit log entries include the following objects:
The log entry itself, which is an object of type
LogEntry
.
Useful fields include the following:
The
logName
contains the resource ID and audit log type.
The
resource
contains the target of the audited operation.
The
timeStamp
contains the time of the audited operation.
The
protoPayload
contains the audited information.
The audit logging data, which is an
AuditLog
object held in
the
protoPayload
field of the log entry.
Optional service-specific audit information, which is a service-specific
object. For earlier integrations, this object is held in the
serviceData
field of the
AuditLog
object; later integrations use the
metadata
field.
For other fields in these objects, and how to interpret them, review
Understand audit logs
.
Log name
Cloud Audit Logs log names include resource identifiers indicating the
Google Cloud project or other Google Cloud entity that owns the audit
logs, and whether the log contains Admin Activity, Data Access, Policy Denied,
or System Event audit logging data.
The following are the audit log names, including variables for the resource
identifiers:
projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Factivity
   projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fdata_access
   projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fsystem_event
   projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fpolicy

   folders/
FOLDER_ID
/logs/cloudaudit.googleapis.com%2Factivity
   folders/
FOLDER_ID
/logs/cloudaudit.googleapis.com%2Fdata_access
   folders/
FOLDER_ID
/logs/cloudaudit.googleapis.com%2Fsystem_event
   folders/
FOLDER_ID
/logs/cloudaudit.googleapis.com%2Fpolicy

   billingAccounts/
BILLING_ACCOUNT_ID
/logs/cloudaudit.googleapis.com%2Factivity
   billingAccounts/
BILLING_ACCOUNT_ID
/logs/cloudaudit.googleapis.com%2Fdata_access
   billingAccounts/
BILLING_ACCOUNT_ID
/logs/cloudaudit.googleapis.com%2Fsystem_event
   billingAccounts/
BILLING_ACCOUNT_ID
/logs/cloudaudit.googleapis.com%2Fpolicy

   organizations/
ORGANIZATION_ID
/logs/cloudaudit.googleapis.com%2Factivity
   organizations/
ORGANIZATION_ID
/logs/cloudaudit.googleapis.com%2Fdata_access
   organizations/
ORGANIZATION_ID
/logs/cloudaudit.googleapis.com%2Fsystem_event
   organizations/
ORGANIZATION_ID
/logs/cloudaudit.googleapis.com%2Fpolicy
Note:
The part of the log name following
/logs/
must be URL-encoded. The
forward-slash character,
/
, must be written as
%2F
.
Service name
Vertex AI Workbench audit logs use the service name
notebooks.googleapis.com
.
For a list of all the Cloud Logging API service names and their corresponding
monitored resource type, see
Map services to resources
.
Resource types
Vertex AI Workbench audit logs use the resource type
audited_resource
for all audit logs.
For a list of all the Cloud Logging monitored resource types and descriptive
information, see
Monitored resource types
.
Caller identities
The IP address of the caller is held in the
RequestMetadata.caller_ip
field of
the
AuditLog
object. Logging might redact certain
caller identities and IP addresses.
For information about what information is redacted in audit logs, see
Caller identities in audit logs
.
Enable audit logging
Admin Activity audit logs are always enabled; you can't disable them.
Data Access audit logs are disabled by default and aren't written unless
explicitly enabled (the exception is Data Access audit logs for
BigQuery, which can't be disabled).
For information about enabling some or all of your Data Access audit logs, see
Enable Data Access audit logs
.
Permissions and roles
IAM
permissions and roles determine your ability to
access audit logs data in Google Cloud resources.
When deciding which
Logging-specific permissions and roles
apply to your use case, consider the following:
The Logs Viewer role (
roles/logging.viewer
) gives you read-only access to
Admin Activity, Policy Denied, and System Event audit logs. If you have just
this role, you cannot view Data Access audit logs that are in the
_Default
bucket.
The Private Logs Viewer role
(roles/logging.privateLogViewer
) includes the
permissions contained in
roles/logging.viewer
, plus the ability to read
Data Access audit logs in the
_Default
bucket.
Note that if these private logs are stored in user-defined buckets, then any
user who has permissions to read logs in those buckets can read the private
logs. For more information about log buckets, see
Routing and storage overview
.
For more information about the IAM permissions and roles that
apply to audit logs data, see
Access control with IAM
.
View logs
You can query for all audit logs or you can query for logs by their
audit log name. The audit log name includes the
resource identifier
of the Google Cloud project, folder, billing account, or
organization for which you want to view audit logging information.
Your queries can specify indexed
LogEntry
fields.
For more information about querying your logs, see
Build queries in the Logs Explorer
The Logs Explorer lets you view filter individual log entries. If you want
to use SQL to analyze groups of log entries, then use the
Log Analytics
page. For more information, see:
Query and view logs in Log Analytics
.
Sample queries for security insights
.
Chart query results
.
Most audit logs can be viewed in Cloud Logging by using the
Google Cloud console, the Google Cloud CLI, or the Logging API.
However, for audit logs related to billing, you can only use the
Google Cloud CLI or the Logging API.
Console
In the Google Cloud console, you can use the Logs Explorer
to retrieve your audit log entries for your Google Cloud project, folder,
or organization:
Note:
You can't view audit logs for Cloud Billing accounts in the
Google Cloud console. You must use the API or the gcloud CLI.
In the Google Cloud console, go to the
Logs Explorer
page:
Go to
Logs Explorer
If you use the search bar to find this page, then select the result whose subheading is
Logging
.
Select an existing Google Cloud project, folder, or organization.
To display all audit logs, enter either of the following queries
into the query-editor field, and then click
Run query
:
logName:"cloudaudit.googleapis.com"
protoPayload."@type"="type.googleapis.com/google.cloud.audit.AuditLog"
To display the audit logs for a specific resource and audit log type,
in the
Query builder
pane, do the following:
In
Resource type
, select the Google Cloud resource whose
audit logs you want to see.
In
Log name
, select the audit log type that you want to see:
For Admin Activity audit logs, select
activity
.
For Data Access audit logs, select
data_access
.
For System Event audit logs, select
system_event
.
For Policy Denied audit logs, select
policy
.
Click
Run query
.
If you don't see these options, then there aren't any audit logs of
that type available in the Google Cloud project, folder, or
organization.
If you're experiencing issues when trying to view logs in the
Logs Explorer, see the
troubleshooting
information.
For more information about querying by using the Logs Explorer, see
Build queries in the Logs Explorer
.
gcloud
The Google Cloud CLI provides a command-line interface to the
Logging API. Supply a valid resource identifier in each of the log
names. For example, if your query includes a
PROJECT_ID
, then the
project identifier you supply must refer to the currently selected
Google Cloud project.
To read your Google Cloud project-level audit log entries, run
the following command:
gcloud logging read "logName : projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com" \
    --project=
PROJECT_ID
To read your folder-level audit log entries, run the following command:
gcloud logging read "logName : folders/
FOLDER_ID
/logs/cloudaudit.googleapis.com" \
    --folder=
FOLDER_ID
To read your organization-level audit log entries, run the following
command:
gcloud logging read "logName : organizations/
ORGANIZATION_ID
/logs/cloudaudit.googleapis.com" \
    --organization=
ORGANIZATION_ID
To read your Cloud Billing account-level audit log entries, run the following command:
gcloud logging read "logName : billingAccounts/
BILLING_ACCOUNT_ID
/logs/cloudaudit.googleapis.com" \
    --billing-account=
BILLING_ACCOUNT_ID
Add the
--freshness
flag
to your command to read logs that are more than 1 day old.
For more information about using the gcloud CLI, see
gcloud logging read
.
REST
When building your queries, supply a valid resource identifier in each of
the log names. For example, if your query includes a
PROJECT_ID
,
then the project identifier you supply must refer to the currently selected
Google Cloud project.
For example, to use the Logging API to view your project-level
audit log entries, do the following:
Go to the
Try this API
section in the documentation for the
entries.list
method.
Put the following into the
Request body
part of the
Try this
API
form. Clicking this
prepopulated form
automatically fills the request body, but you need to supply a valid
PROJECT_ID
in each of the log names.
{
  "resourceNames": [
    "projects/
PROJECT_ID
"
  ],
  "pageSize": 5,
  "filter": "logName : projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com"
}
Click
Execute
.
For example, to view all the project-level audit logs for
Vertex AI Workbench, use the following query, supplying a valid resource
identifier in each of the log names:
logName
=
(
"projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Factivity"
OR
"projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fdata_access"
OR
"projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fsystem_event"
OR
"projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fpolicy"
)
protoPayload
.
serviceName
=
"notebooks.googleapis.com"
Route audit logs
You can
route audit logs
to supported
destinations in the same way that you can route other kinds of logs. Here are
some reasons you might want to route your audit logs:
To keep audit logs for a longer period of time or to use more powerful
search capabilities, you can route copies of your audit logs to
Cloud Storage, BigQuery, or Pub/Sub. Using
Pub/Sub, you can route to other applications, other
repositories, and to third parties.
To manage your audit logs across an entire organization, you can create
aggregated sinks
that can
route logs from any or all Google Cloud projects in the organization.
If your enabled Data Access audit logs are pushing your
Google Cloud projects over your log allotments, you can create sinks that
exclude the Data Access audit logs from Logging.
For instructions about routing logs, see
Route logs to supported destinations
.
Pricing
For more information about pricing, see the Cloud Logging sections in the
Google Cloud Observability pricing
page.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/bigquery.txt
Query data in BigQuery from within JupyterLab  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Query data in BigQuery from within JupyterLab
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page shows you how to query data that is stored
in BigQuery from within the JupyterLab interface
of your Vertex AI Workbench managed notebooks instance.
Methods for querying BigQuery data in notebook (IPYNB) files
To query BigQuery data from within a JupyterLab notebook file,
you can use the
%%bigquery
magic command and
the BigQuery client library for Python.
Managed notebooks instances also
include a BigQuery integration that lets you
browse and query data from within the JupyterLab interface.
This page describes how to use each of these methods.
Open JupyterLab
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Next to your managed notebooks instance's name,
click
Open JupyterLab
.
Your managed notebooks instance opens JupyterLab.
Browse BigQuery resources
The BigQuery integration provides a pane for browsing
the BigQuery resources that you have access to.
In the JupyterLab navigation menu, click
BigQuery in Notebooks
.
The
BigQuery
pane lists available projects and datasets, where you
can perform tasks as follows:
To view a description of a dataset, double-click the dataset name.
To show a dataset's tables, views, and models, expand the dataset.
To open a summary description as a tab in JupyterLab, double-click a
table, view, or model.
Note:
On the summary description for a table, click the
Preview
tab to preview a table's data. The following image shows a preview of the
international_top_terms
table
found in the
google_trends
dataset in the
bigquery-public-data
project:
Query data by using the %%bigquery magic command
In this section, you write SQL directly in notebook cells and read data from
BigQuery into the Python notebook.
Magic commands that use a single or double percentage character (
%
or
%%
)
let you use minimal syntax to interact with BigQuery within the
notebook. The BigQuery client library for Python is automatically
installed in a managed notebooks instance. Behind the scenes, the
%%bigquery
magic
command uses the BigQuery client library for Python to run the
given query, convert the results to a pandas DataFrame, optionally save the
results to a variable, and then display the results.
Note
: As of version 1.26.0 of the
google-cloud-bigquery
Python package,
the
BigQuery Storage API
is used by default to download results from the
%%bigquery
magics.
To open a notebook file, select
File
>
New
>
Notebook
.
In the
Select Kernel
dialog, select
Python (Local)
, and then click
Select
.
Your new IPYNB file opens.
To get the number of regions by country in the
international_top_terms
dataset, enter the following statement:
%%
bigquery
SELECT
country_code
,
country_name
,
COUNT
(
DISTINCT
region_code
)
AS
num_regions
FROM
`
bigquery
-
public
-
data
.
google_trends
.
international_top_terms
`
WHERE
refresh_date
=
DATE_SUB
(
CURRENT_DATE
,
INTERVAL
1
DAY
)
GROUP
BY
country_code
,
country_name
ORDER
BY
num_regions
DESC
;
Click
play_circle_filled
Run cell
.
The output is similar to the following:
Query complete after 0.07s: 100%|██████████| 4/4 [00:00<00:00, 1440.60query/s]
Downloading: 100%|██████████| 41/41 [00:02<00:00, 20.21rows/s]
country_code      country_name    num_regions
0   TR  Turkey         81
1   TH  Thailand       77
2   VN  Vietnam        63
3   JP  Japan          47
4   RO  Romania        42
5   NG  Nigeria        37
6   IN  India          36
7   ID  Indonesia      34
8   CO  Colombia       33
9   MX  Mexico         32
10  BR  Brazil         27
11  EG  Egypt          27
12  UA  Ukraine        27
13  CH  Switzerland    26
14  AR  Argentina      24
15  FR  France         22
16  SE  Sweden         21
17  HU  Hungary        20
18  IT  Italy          20
19  PT  Portugal       20
20  NO  Norway         19
21  FI  Finland        18
22  NZ  New Zealand    17
23  PH  Philippines    17
...
Note:
Your results might differ from what is above as the
google_trends
dataset being queried is refreshed with new data on an ongoing basis.
In the next cell (below the output from the previous cell), enter the
following command to run the same query, but this time save the results to
a new pandas DataFrame that's named
regions_by_country
. You provide that
name by using an argument with the
%%bigquery
magic command.
%%
bigquery
regions_by_country
SELECT
country_code
,
country_name
,
COUNT
(
DISTINCT
region_code
)
AS
num_regions
FROM
`
bigquery
-
public
-
data
.
google_trends
.
international_top_terms
`
WHERE
refresh_date
=
DATE_SUB
(
CURRENT_DATE
,
INTERVAL
1
DAY
)
GROUP
BY
country_code
,
country_name
ORDER
BY
num_regions
DESC
;
Note:
For more information about available arguments for the
%%bigquery
command, see the
client library magics documentation
.
Click
play_circle_filled
Run cell
.
In the next cell, enter the following command to look at the first few
rows of the query results that you just read in:
regions_by_country
.
head
()
Click
play_circle_filled
Run cell
.
The pandas DataFrame
regions_by_country
is ready to plot.
Query data by using the BigQuery client library directly
In this section, you use the BigQuery client library for Python
directly to read data into the Python notebook.
The client library gives you more control over your queries and lets you use
more complex configurations for queries and jobs. The library's integrations
with pandas enable you to combine the power of declarative SQL with imperative
code (Python) to help you analyze, visualize, and transform your data.
Note:
You can use a number of Python data analysis, data wrangling, and
visualization libraries, such as
numpy
,
pandas
,
matplotlib
, and many
others. Several of these libraries are built on top of a DataFrame object.
In the next cell, enter the following Python code to import the
BigQuery client library for Python and initialize a client:
from
google.cloud
import
bigquery
client
=
bigquery
.
Client
()
The BigQuery client is used to send and receive messages
from the BigQuery API.
Click
play_circle_filled
Run cell
.
In the next cell, enter the following code to retrieve the percentage of
daily top terms in the US
top_terms
that overlap across time by number of days apart. The idea here is to look
at each day's top terms and see what percentage of them overlap with the
top terms from the day before, 2 days prior, 3 days prior, and so on (for
all pairs of dates over about a month span).
sql
=
"""
WITH
TopTermsByDate AS (
SELECT DISTINCT refresh_date AS date, term
FROM `bigquery-public-data.google_trends.top_terms`
),
DistinctDates AS (
SELECT DISTINCT date
FROM TopTermsByDate
)
SELECT
DATE_DIFF(Dates2.date, Date1Terms.date, DAY)
AS days_apart,
COUNT(DISTINCT (Dates2.date || Date1Terms.date))
AS num_date_pairs,
COUNT(Date1Terms.term) AS num_date1_terms,
SUM(IF(Date2Terms.term IS NOT NULL, 1, 0))
AS overlap_terms,
SAFE_DIVIDE(
SUM(IF(Date2Terms.term IS NOT NULL, 1, 0)),
COUNT(Date1Terms.term)
) AS pct_overlap_terms
FROM
TopTermsByDate AS Date1Terms
CROSS JOIN
DistinctDates AS Dates2
LEFT JOIN
TopTermsByDate AS Date2Terms
ON
Dates2.date = Date2Terms.date
AND Date1Terms.term = Date2Terms.term
WHERE
Date1Terms.date <= Dates2.date
GROUP BY
days_apart
ORDER BY
days_apart;
"""
pct_overlap_terms_by_days_apart
=
client
.
query
(
sql
).
to_dataframe
()
pct_overlap_terms_by_days_apart
.
head
()
The SQL being used is encapsulated in a Python string and then passed to the
query()
method
to run a query. The
to_dataframe
method
waits for the query to finish and downloads the results to a pandas
DataFrame by using the BigQuery Storage API.
Click
play_circle_filled
Run
cell
.
The first few rows of query results appear below the code cell.
days_apart   num_date_pairs  num_date1_terms overlap_terms   pct_overlap_terms
 0          0             32               800            800            1.000000
 1          1             31               775            203            0.261935
 2          2             30               750             73            0.097333
 3          3             29               725             31            0.042759
 4          4             28               700             23            0.032857
Note:
Your results might differ from what is above as the
google_trends
dataset being queried is refreshed with new data on an ongoing basis.
For more information about using BigQuery client libraries, see
the quickstart
Using client libraries
.
Query data by using the BigQuery integration in managed notebooks
The BigQuery integration provides two additional methods
for querying data. These methods are different from using
the
%%bigquery
magic command.
The
In-cell query editor
is a cell type that you can use within
your notebook files.
The
Stand-alone query editor
opens as a separate tab in JupyterLab.
In-cell
To use the in-cell query editor to query data
in a BigQuery table, complete the following steps:
In JupyterLab, open a notebook (IPYNB) file or
create a
new one
.
To create an in-cell query editor, click the cell,
and then to the right of the cell, click the
BigQuery Integration
button.
Or in a markdown cell, enter
#@BigQuery
.
The BigQuery integration converts the cell into
an in-cell query editor.
On a new line below
#@BigQuery
, write your query using
the
supported statements and SQL dialects of
BigQuery
.
If errors are detected in your query, an error message appears
in the top right corner of the query editor. If the query is valid,
the estimated number of bytes to be processed appears.
Click
Submit Query
. Your query results appear.
By default, query results are paginated at 100 rows per page
and limited to 1,000 rows total, but you can change
these settings at the bottom of the results table. In the
query editor, keep the query limited to only the data you need
to verify your query. You'll run this query again in a notebook cell,
where you can adjust the limit to retrieve the
full results set if you want.
Note:
Query text and results persist after closing and reopening
the notebook file.
You can click
Query and load as DataFrame
to automatically
add a new cell that contains a code segment
that imports the BigQuery client library for Python,
runs your query in a notebook cell, and stores the results
in a pandas dataframe named
df
.
Stand-alone
To use the stand-alone query editor to query data
in a BigQuery table, complete the following steps:
In JupyterLab, in the
BigQuery in Notebooks
pane, right-click
a table, and select
Query table
,
or double-click a table to open a description
in a separate tab, and then click the
Query table
link.
Write your query using the
supported statements and SQL dialects of
BigQuery
.
If errors are detected in your query, an error message appears
in the top right corner of the query editor. If the query is valid,
the estimated number of bytes to be processed appears.
Click
Submit Query
. Your query results appear.
By default, query results are paginated at 100 rows per page
and limited to 1,000 rows total, but you can change
these settings at the bottom of the results table. In the
query editor, keep the query limited to only the data you need
to verify your query. You'll run this query again in a notebook cell,
where you can adjust the limit to retrieve the
full results set if you want.
You can click
Copy code for DataFrame
to copy a code segment
that imports the BigQuery client library for Python,
runs your query in a notebook cell, and stores the results
in a pandas dataframe named
df
. Paste this code into a notebook cell
where you want to run it.
View query history and reuse queries
To view your query history as a tab in JupyterLab, perform the following steps:
In the JupyterLab navigation menu, click
BigQuery in Notebooks
to open the
BigQuery
pane.
In the
BigQuery
pane, scroll down and click
Query history
.
A list of your queries opens in a new tab, where you can perform tasks such
as the following:
To view the details of a query such as its Job ID, when the query was
run, and how long it took, click the query.
To revise the query, run it again, or copy it into your notebook for
future use, click
Open query in editor
.
Note:
If you run queries after opening this tab, click
refresh
Refresh
to show
the most recent queries.
What's next
To see examples of how to visualize the data from
your BigQuery tables, see
Explore and visualize data
in BigQuery from
within JupyterLab
.
To learn more about writing queries for BigQuery, see
Running interactive and batch query jobs
.
Learn how to
control access to
BigQuery datasets
.
Learn how to
access Cloud Storage buckets and files
from within JupyterLab
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/change-machine-type.txt
Change machine type and configure GPUs of a Vertex AI Workbench managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Change machine type and configure GPUs of a managed notebooks instance
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page shows you how to change the machine type and
GPU configuration of a managed notebooks instance.
Overview
The machine type determines some specifications of
your managed notebooks instance,
such as the amount of memory, virtual cores, and persistent disk limits.
Changing the machine type can improve performance or help avoid errors
caused by high memory utilization.
GPUs provide hardware acceleration that can improve the performance
of your notebook code. You may want to add or increase the number of GPUs
for greater performance, or, to reduce the cost of running
your managed notebooks instance,
you may want to remove GPUs.
Before you change a machine type or GPU configuration
Consider the following before you make any changes to
your managed notebooks instance's machine type
or GPU configuration.
Billing implications
Each machine type and GPU configuration is billed at a different rate.
Make sure you understand
the
pricing
implications of making a change.
For example, an
e2-highmem-2
machine type costs more than an
e2-standard-2
machine type.
Changing a machine type might also affect sustained use discounts.
Sustained use discounts are calculated
separately for different
categories
in
the same region. If you change machine types so that the new machine type is
in a different category, the subsequent running time of
your instance's VM counts toward the
sustained use discount of the new category.
Moving to a smaller machine type
If you move from a machine type with more resources to a machine type with fewer
resources, such as moving from an
e2-standard-8
machine type to an
e2-standard-2
, you might run into hardware resource issues or performance
limitations because smaller machine types are less powerful than larger machine
types.
Change the machine type and configure GPUs
Note:
To change the machine type or GPUs for
    a managed notebooks, instance must be running.
To change the machine type or configure the GPUs on
a managed notebooks instance, complete the following steps:
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
In the
Notebook name
column, click the name of the
instance that you want to modify.
The
Notebook details
page opens.
Click the
Hardware
tab.
In the
Modify hardware configuration
section,
select the
Machine type
that you want to use.
If there are GPUs available for your instance's combination of
zone, environment, and machine type, you can configure the GPUs.
In the
GPUs
section, select the
GPU type
and
Number of GPUs
that you want to use.
Learn more about GPU regions and zone
availability
.
If you haven't already installed the required GPU drivers on your instance,
select
Install NVIDIA GPU driver automatically for me
to install the drivers automatically on next startup.
Click submit to confirm changes.
What's next
Learn more about the available
GPU platforms
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/cloud-storage.txt
Access Cloud Storage buckets and files in JupyterLab  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Access Cloud Storage buckets and files in JupyterLab
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page shows you how to mount a Cloud Storage bucket to the
JupyterLab interface of
your Vertex AI Workbench managed notebooks instance
so that you can browse files that are stored
in Cloud Storage. You can also open and edit files that are compatible
with JupyterLab, such as text files and notebook (IPYNB) files.
Overview
Vertex AI Workbench managed notebooks instances
include a Cloud Storage integration that lets you
mount a Cloud Storage bucket. This means you can browse the contents
of the bucket and work with compatible files from within
the JupyterLab interface.
You can access any of
the Cloud Storage buckets and files that your instance
has access to within the same project as
your managed notebooks instance.
Note:
Your managed notebooks instance's access to
Cloud Storage is determined by the single user or service account
that you used to grant access to your instance. For example,
if you granted a specific service account access to your instance,
you must also grant that service account access to the
Cloud Storage buckets that you want to use in JupyterLab.
Before you begin
This guide requires you to have access to
at least one Cloud Storage bucket in the same project
as your managed notebooks instance.
If you need to create a Cloud Storage bucket,
see
Create buckets
.
Open JupyterLab
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Next to your managed notebooks instance's name,
click
Open JupyterLab
.
Your managed notebooks instance opens JupyterLab.
Mount the Cloud Storage buckets and files
To mount and then access a Cloud Storage bucket, do the following:
In JupyterLab, make sure the
folder
File Browser
tab
is selected.
In the left sidebar, click the
Mount
shared storage
button. If you don't see the button, drag the right side
of the sidebar to expand the sidebar until you see the button.
In the
Bucket name
field, enter the Cloud Storage bucket name
that you want to mount.
Click
Mount
.
Your Cloud Storage bucket appears as a folder in the
File browser
tab of the left sidebar. Double-click the folder to open
it and browse the contents.
What's next
Learn more about
Cloud Storage
.
Learn how to
query data in BigQuery
from within JupyterLab
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/cmek.txt
Use customer-managed encryption keys  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Use customer-managed encryption keys
Stay organized with collections
Save and categorize content based on your preferences.
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
By default, Vertex AI Workbench encrypts customer content at
    rest. Vertex AI Workbench handles encryption for you without any
  additional actions on your part. This option is called
Google default encryption
.
If you want to control your encryption keys, then you can use customer-managed encryption keys
  (CMEKs) in
Cloud KMS
with CMEK-integrated services including
  Vertex AI Workbench. Using Cloud KMS keys gives you control over their protection
  level, location, rotation schedule, usage and access permissions, and cryptographic boundaries.
  Using Cloud KMS also lets
you view audit logs and control key lifecycles.
  
  Instead of Google owning and managing the symmetric
key encryption keys (KEKs)
that protect your data, you control and
  manage these keys in Cloud KMS.
After you set up your resources with CMEKs, the experience of accessing your
  Vertex AI Workbench resources is similar to using Google default encryption.
  For more information about your encryption
  options, see
Customer-managed encryption keys (CMEK)
.
This page describes some specific benefits and limitations of using CMEK with
managed notebooks and shows
how to configure a new managed notebooks instance
to use CMEK.
Benefits of CMEK
In general, CMEK is most useful if you need full control over the keys used to
encrypt your data. With CMEK, you can manage your keys within
Cloud Key Management Service. For example, you can rotate or disable a key or you can set
up a rotation schedule by using the Cloud KMS API.
When you run a managed notebooks instance,
your instance runs in a compute infrastructure managed by Google.
When you enable
CMEK for a managed notebooks instance,
Vertex AI Workbench uses the key that you designate,
rather than a key managed by Google, to encrypt your user data.
The CMEK key
doesn't
encrypt metadata, like the instance's name and region,
associated with your managed notebooks instance.
Metadata associated with
managed notebooks instances is always
encrypted using Google's default encryption mechanism.
Limitations of CMEK
To decrease latency and to prevent cases where resources depend on
services that are spread across multiple failure domains, Google recommends
that you protect regional
managed notebooks instances with keys in the same location.
You can encrypt regional managed notebooks instances
by using keys in the same location or in the global location. For example,
you can encrypt user data in region
us-west1
by using
a key in
us-west1
or
global
.
Configuring CMEK for
managed notebooks
doesn't
automatically configure CMEK
for other Google Cloud products that you use. To use CMEK to encrypt
data in other Google Cloud products, you must complete additional
configuration.
Configure CMEK for your managed notebooks instance
The following sections describe how to create a
key ring and key in Cloud Key Management Service,
grant the service account encrypter and decrypter permissions for your key,
and create a managed notebooks instance that uses CMEK.
Before you begin
We recommend using a setup that supports a
separation of
duties
. To configure CMEK
for managed notebooks, you can use
two separate Google Cloud projects:
A Cloud KMS project: a project for managing your encryption key
A managed notebooks project: a project for accessing
managed notebooks instances and interacting with any
other Google Cloud products that you need for your use case
Alternatively, you can use a single Google Cloud project. To do so,
use the same project for all of the following tasks.
Set up the Cloud KMS project
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Cloud KMS API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Cloud KMS API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Set up the managed notebooks project
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Set up the Google Cloud CLI
The gcloud CLI is required for some steps on this page and optional
for others.
Install
the Google Cloud CLI.
        
          After installation,
initialize
the Google Cloud CLI by running the following command:
gcloud
init
If you're using an external identity provider (IdP), you must first
sign in to the gcloud CLI with your federated identity
.
Note:
You can run the gcloud CLI in
      the Google Cloud console without installing the Google Cloud CLI. To run the
      gcloud CLI in the Google Cloud console,
use
        Cloud Shell
.
Create a key ring and key
When you create a key ring and key, keep the following requirements in mind:
When you choose your key ring's location, use either
global
or the location where your managed notebooks instance
will be.
Make sure to create your key ring and key in your Cloud KMS project.
To create a key ring and a key, see
Create symmetric encryption keys
.
Grant managed notebooks permissions
If you set up your instance with
single user
access
,
you must grant your managed notebooks instance's project
permission to encrypt and decrypt data using your key.
You grant this permission to your project's
service agent
.
This service agent's email address looks like:
service
-
NOTEBOOKS_PROJECT_NUMBER
@gcp
-
sa
-
notebooks
.
iam.gserviceaccount.com
Replace
NOTEBOOKS_PROJECT_NUMBER
with the
project number
for your managed notebooks instance's project.
Make note of the email address for your service agent.
You will use it in the following steps to grant your
managed notebooks instance's project permission
to encrypt and decrypt data using your key.
You can grant permission by using the Google Cloud console or
by using the Google Cloud CLI.
Note:
If you set up your instance with
service account
      access
,
      you don't need to grant any further permissions.
Console
In the Google Cloud console, go to the
Key management
page.
Go to Key management
Select your Cloud KMS project.
Click the name of the  key ring that you created in
Create a key ring and key
. The
Key ring details
page opens.
Select the checkbox for the key that you created in
Create a key ring and key
.
If an info panel labeled with the name of your key isn't already
open, click
Show info panel
.
In the info panel, click
person_add
Add member
.
The
Add members to "
KEY_NAME
"
dialog opens. In this
dialog, do the following:
In the
New members
field, enter the service agent
email address that you made a note of in the preceding section.
In the
Select a role
list, click
Cloud KMS
and then select the
Cloud KMS CryptoKey Encrypter/Decrypter
role.
Click
Save
.
gcloud
Run the following command to grant the service agent
permission to encrypt and decrypt data using your key:
gcloud kms keys add-iam-policy-binding
KEY_NAME
\
    --keyring=
KEY_RING_NAME
\
    --location=
REGION
\
    --project=
KMS_PROJECT_ID
\
    --member=serviceAccount:
EMAIL_ADDRESS
\
    --role=roles/cloudkms.cryptoKeyEncrypterDecrypter
Replace the following:
KEY_NAME
: the name of the key that you created
in
Create a key ring and key
KEY_RING_NAME
: the key ring that you created
in
Create a key ring and key
REGION
: the region where you created your key
ring
KMS_PROJECT_ID
: the ID of your
Cloud KMS project
EMAIL_ADDRESS
: the email address of the
service agent that you made note of in the previous section
Create a managed notebooks instance with CMEK
After you have granted your managed notebooks instance
permission to encrypt and decrypt data using your key,
you can create a managed notebooks instance
that encrypts data using this key. Use the following steps:
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Click
add_box
New notebook
.
In the
Notebook name
field, enter a name for your instance.
Click the
Region
list, and select a region for your instance.
Click
Advanced settings
.
In the
Disk encryption
section,
select
Customer-managed encryption key (CMEK)
.
Click
Select a customer-managed key
.
If the customer-managed key that you want to use is in the list,
select it.
If the customer-managed key that you want to use isn't in the list,
enter the resource ID for your customer-managed key. The resource
ID for your customer-managed key looks like this:
projects/
NOTEBOOKS_PROJECT_NUMBER
/locations/global/keyRings/
KEY_RING_NAME
/cryptoKeys/
KEY_NAME
Replace the following:
NOTEBOOKS_PROJECT_NUMBER
: the ID of your
managed notebooks project
KEY_RING_NAME
: the key ring that you created
in
Create a key ring and key
KEY_NAME
: the name of the key that you
created in
Create a key ring and key
Complete the rest of the
Create a managed notebook
dialog
according to your needs.
Click
Create
.
Vertex AI Workbench creates
a managed notebooks instance based on your
specified properties and automatically starts the instance. When the
instance is ready to use, Vertex AI Workbench activates an
Open JupyterLab
link.
What's next
Learn more about
CMEK on Google Cloud
Learn
how to use CMEK with other Google Cloud
products
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/create-instance.txt
Vertex AI deprecations  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Vertex AI
Release notes
Glossary
Pricing
Quotas and limits
Locations
Support policy
Shared responsibility
Vertex AI framework support policy
Supported frameworks list
Security bulletins
Service Level Agreement
Security controls
Vertex AI deprecations
AI Agent Ecosystem partners
Overview
AI Agent Ecosystem partners
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI
Resources
Send feedback
Vertex AI deprecations
Stay organized with collections
Save and categorize content based on your preferences.
The
Google Cloud Platform Terms of Service (section "Discontinuation of Services")
defines the deprecation policy that applies to Vertex AI.
  The
deprecation policy
only applies to the services,
  features, or products listed therein.
After a service, feature, or product is officially
  deprecated, it continues to be available for at least the period of time defined in the
  Terms of Service. After this period of time, the service is scheduled for shutdown.
Feature
Deprecated date
Shutdown date
Details
AutoML Video
July 31, 2024
July 31, 2025
AutoML Video is no longer available. See
Video
        tuning
for detailed instructions for fine-tuning Gemini on
        video data using supervised learning.
AutoML Text
September 15, 2024
June 15, 2025
Starting on September 15, 2024, you can only customize Text
        classification, entity extraction, and sentiment analysis objectives by
        moving to Vertex AI Gemini prompts and tuning. Training or
        updating models for Vertex AI AutoML for
        Text classification, entity extraction, and sentiment analysis
        objectives will no longer be available. You can continue using
        existing Vertex AI AutoML Text models until
        June 15, 2025. For more information about how Gemini offers enhanced
        user experience through improved prompting capabilities, see
Introduction
        to tuning
.
Legacy AI Platform Training
January 23, 2023
April 7, 2025
Migrate to
        Vertex AI custom training
, which includes all
        functionality of legacy AI Platform Training as well as new features.
Legacy AI Platform Prediction
January 23, 2023
April 7, 2025
All models, associated metadata, and deployments were deleted after
        the shutdown date.
Migrate your resources
        to Vertex AI prediction
to get the latest machine learning
        features, simplify end-to-end journeys, and productionize models
        with MLOps.
Vertex AI Workbench managed notebooks
January 16, 2024
April 14, 2025
On April 14, 2025, support for
          managed notebooks ended, and the ability to create
          managed notebooks instances was removed. Existing instances will
          continue to function until
          March 30, 2026, but patches,
          updates, and upgrades won't be available.
On March 30, 2026, all
          managed notebooks instances and their associated data will be
          deleted. There is no recovery of managed notebooks instances
          after deletion. There isn't a way to convert these instances
          to Compute Engine virtual machines (VMs). To continue using
          Vertex AI Workbench,
migrate
          your managed notebooks instances to
          Vertex AI Workbench instances
before
          March 30, 2026.
Vertex AI Workbench user-managed notebooks
January 16, 2024
April 14, 2025
On April 14, 2025, support for
          user-managed notebooks ended, and the ability to create
          user-managed notebooks instances was removed. Existing instances
          will continue to function until
          March 30, 2026, but patches,
          updates, and upgrades won't be available.
On March 30, 2026, your
          user-managed notebooks instances will be converted to standard
          Compute Engine virtual machines (VMs). You will have full control
          over these VMs, but they won't be supported or available
          through Vertex AI Workbench. You will incur costs for
          these Compute Engine VMs, so we recommend that you delete
          any VMs that you don't plan to use. To continue using
          Vertex AI Workbench,
migrate
          your user-managed notebooks instances to
          Vertex AI Workbench instances
before
          March 30, 2026.
Legacy AI Platform Pipelines
July 31, 2023
January 31, 2025
Migrate to
        Vertex AI Pipelines
, which includes all functionality of
        legacy AI Platform Pipelines as well as new features.
Legacy AI Platform Data Labeling Service
January 23, 2023
January 31, 2025
For new labeling tasks, you can
add labels
        using the Google Cloud console
or access data labeling solutions
        from our partners in the
Google Cloud
        Console Marketplace
.
Vertex AI Data Labeling Service
June 30, 2023
October 3, 2024
For new labeling tasks, you can
add labels
        using the Google Cloud console
or access data labeling solutions
        from our partners in the
Google Cloud
        Console Marketplace
.
Legacy AutoML Natural Language
July 31, 2023
August 7, 2024
New models can no longer be trained nor deployed on the
        legacy platform. Already deployed models stopped working on
        May 30, 2024. All the functionality of legacy Vertex AI and
        new features are available on the Vertex AI platform. See
Migrate to
        Vertex AI
to learn how to migrate your resources.
Legacy AutoML Video Intelligence
January 23, 2023
July 31, 2024
Migrate to
        Vertex AI
, which includes all functionality of
        legacy AutoML Video Intelligence as well as new features.
Legacy AutoML Vision
January 23, 2023
July 31, 2024
Migrate to
        Vertex AI
, which includes all functionality of
        legacy AutoML Vision as well as new features.
Legacy AutoML Tables
January 23, 2023
July 24, 2024
Migrate to
        Vertex AI
, which includes all functionality of
        legacy AutoML Tables as well as new features.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/create-managed-notebooks-instance-console-quickstart.txt
Quickstart: Create a managed notebooks instance by using the Google Cloud console  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create a managed notebooks instance
by using the Google Cloud console
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
Learn how to create a Vertex AI Workbench managed notebooks instance
and open JupyterLab by using the Google Cloud console.
This page also describes how to stop, start, reset, or delete
a managed notebooks instance.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Create an instance
In the Google Cloud console,
go to the
Managed notebooks
page.
Go to Managed notebooks
Click
add_box
Create new
.
In the
Create instance
window, in the
Name
field,
enter
my-instance
.
Click
Create
.
When you finish the tasks that are described in this document, you can avoid
   continued billing by deleting the resources that you created. For more information, see
Clean up
.
Open JupyterLab
After you create your instance, Vertex AI Workbench automatically starts
the instance. When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link.
Next to your managed notebooks instance's name,
click
Open JupyterLab
.
In the
Authenticate your managed notebook
dialog, click the button
to get an authentication code.
Choose an account and click
Allow
. Copy the authentication code.
In the
Authenticate your managed notebook
dialog,
paste the authentication code, and then click
Authenticate
.
Your managed notebooks instance opens JupyterLab.
Open a new notebook file
Select
File
>
New
>
Notebook
.
In the
Select kernel
dialog, select
Python
,
and then click
Select
.
Your new notebook file opens.
Change the kernel
You can change the kernel of your JupyterLab notebook file from the menu
or in the file.
Menu
In JupyterLab, on the
Kernel
menu, click
Change kernel
.
In the
Select kernel
dialog, select another kernel to use.
Click
Select
.
In the file
In your JupyterLab notebook file, click the kernel name.
In the
Select kernel
dialog, select another kernel to use.
Click
Select
.
Stop your instance
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the instance that you want to stop.
Click
square
Stop
.
Start your instance
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the instance that you want to start.
Click
arrow_right
Start
.
Reset your instance
Resetting an instance forcibly wipes the memory contents of your instance and
resets the instance to its initial state. To learn more about how resetting an
instance works, see
Resetting an instance
.
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the instance that you want to reset.
Click
Reset
, and then click
Reset
to confirm.
Clean up
To avoid incurring charges to your Google Cloud account for
          the resources used on this page, follow these steps.
If you created a new project to learn about
Vertex AI Workbench managed notebooks
and you no longer need the project, then
delete the project
.
If you used an existing Google Cloud project, then delete the resources
you created to avoid incurring charges to your account:
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the row containing the instance that you want to delete.
Click
delete
Delete
.
(Depending on the size of your window,
the
Delete
button might be in
the
more_vert
options menu.)
To confirm, click
Delete
.
What's next
Try one of the tutorials that is included
in your new managed notebooks instance.
In the JupyterLab
folder
File Browser
, open the
tutorials
folder,
and open one of the notebook files.
Read the
Introduction to managed notebooks
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/create-managed-notebooks-instance.txt
Quickstart: Create a managed notebooks instance by using the Google Cloud console  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create a managed notebooks instance
by using the Google Cloud console
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
Learn how to create a Vertex AI Workbench managed notebooks instance
and open JupyterLab by using the Google Cloud console.
This page also describes how to stop, start, reset, or delete
a managed notebooks instance.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Create an instance
In the Google Cloud console,
go to the
Managed notebooks
page.
Go to Managed notebooks
Click
add_box
Create new
.
In the
Create instance
window, in the
Name
field,
enter
my-instance
.
Click
Create
.
When you finish the tasks that are described in this document, you can avoid
   continued billing by deleting the resources that you created. For more information, see
Clean up
.
Open JupyterLab
After you create your instance, Vertex AI Workbench automatically starts
the instance. When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link.
Next to your managed notebooks instance's name,
click
Open JupyterLab
.
In the
Authenticate your managed notebook
dialog, click the button
to get an authentication code.
Choose an account and click
Allow
. Copy the authentication code.
In the
Authenticate your managed notebook
dialog,
paste the authentication code, and then click
Authenticate
.
Your managed notebooks instance opens JupyterLab.
Open a new notebook file
Select
File
>
New
>
Notebook
.
In the
Select kernel
dialog, select
Python
,
and then click
Select
.
Your new notebook file opens.
Change the kernel
You can change the kernel of your JupyterLab notebook file from the menu
or in the file.
Menu
In JupyterLab, on the
Kernel
menu, click
Change kernel
.
In the
Select kernel
dialog, select another kernel to use.
Click
Select
.
In the file
In your JupyterLab notebook file, click the kernel name.
In the
Select kernel
dialog, select another kernel to use.
Click
Select
.
Stop your instance
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the instance that you want to stop.
Click
square
Stop
.
Start your instance
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the instance that you want to start.
Click
arrow_right
Start
.
Reset your instance
Resetting an instance forcibly wipes the memory contents of your instance and
resets the instance to its initial state. To learn more about how resetting an
instance works, see
Resetting an instance
.
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the instance that you want to reset.
Click
Reset
, and then click
Reset
to confirm.
Clean up
To avoid incurring charges to your Google Cloud account for
          the resources used on this page, follow these steps.
If you created a new project to learn about
Vertex AI Workbench managed notebooks
and you no longer need the project, then
delete the project
.
If you used an existing Google Cloud project, then delete the resources
you created to avoid incurring charges to your account:
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the row containing the instance that you want to delete.
Click
delete
Delete
.
(Depending on the size of your window,
the
Delete
button might be in
the
more_vert
options menu.)
To confirm, click
Delete
.
What's next
Try one of the tutorials that is included
in your new managed notebooks instance.
In the JupyterLab
folder
File Browser
, open the
tutorials
folder,
and open one of the notebook files.
Read the
Introduction to managed notebooks
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/custom-container.txt
Create a managed notebooks instance with a custom container  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create a managed notebooks instance with a custom container
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page shows you how to add a custom container to
a Vertex AI Workbench managed notebooks instance
as a kernel that you can run your notebook files on.
Overview
You can add a custom container for use with your
managed notebooks instance. The custom container
is then available as a local kernel that you can run
your notebook file on.
Custom container requirements
Vertex AI Workbench managed notebooks supports
any of the current
Deep Learning Containers
container images
.
To create a custom container image of your own, you can modify one of the
Deep Learning Containers container images to
create a
derivative container image
.
To create a custom container image from scratch, make sure
the container image meets the following requirements:
Use a Docker container image with at least one valid Jupyter kernelspec.
This exposed kernelspec lets Vertex AI Workbench
managed notebooks load the container image as a kernel.
If your container image includes an installation of
JupyterLab
or
Jupyter Notebook
,
the installation will include the kernelspec by default. If your
container image doesn't have the kernelspec, you can
install the
kernelspec
directly.
Note:
If your container has a valid Jupyter kernelspec,
      it will respond to
jupyter kernelspec list --json
with a list of available kernelspecs.
The Docker container image must support
sleep infinity
.
To use your custom container with the
managed notebooks executor
,
ensure that your custom container has the
nbexecutor
extension.
Note:
Deep Learning Containers container images include
      a valid Jupyter kernelspec and the
nbexecutor
extension by default.
The following example Dockerfile text builds a custom Docker image
from scratch that is based on an Ubuntu image and includes
the latest Python version.
FROM
--platform=linux/amd64
ubuntu:22.04
RUN
apt-get
-y
update
RUN
apt-get
install
-y
--no-install-recommends
\
python3-pip
\
pipx
\
git
\
make
\
jq
RUN
pip
install
\
argcomplete
>
=
1
.9.4
\
poetry
==
1
.1.14
\
jupyterlab
==
3
.3.0
# Create a link that points to the right python bin directory
RUN
ln
-s
/usr/bin/python
VERSION_NUMBER
/usr/bin/python
Replace
VERSION_NUMBER
with the version of Python that you're using.
How a custom container becomes a kernel in managed notebooks
For each custom container image provided,
your managed notebooks instance identifies
the available Jupyter kernelspec on the container image
when the instance starts. The kernelspec appears as a local kernel
in the JupyterLab interface. When the kernelspec is selected,
the managed notebooks kernel manager runs
the custom container as a kernel and starts a Jupyter session
on that kernel.
How custom container kernels are updated
Vertex AI Workbench pulls the latest container image for your kernel:
When you create your instance.
When you upgrade your instance.
When you start your instance.
The custom container kernel doesn't persist when your instance is stopped,
so each time your instance is started, Vertex AI Workbench pulls
the latest version of the container image.
If your instance is running when a new version of a container is released,
your instance's kernel isn't updated until you stop and start your instance.
Custom container image availability
Deep Learning Containers container images are
available to all users. When you use
a Deep Learning Containers container image, you must grant
specific roles to your instance's service account so your instance
can load the Deep Learning Containers container image as
a kernel. Learn more about the required permissions and how to grant them
in the
Permissions
section.
If you want to use your own custom container image, it must be located in
Artifact Registry
and the container image must be
publicly available.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Artifact Registry APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Artifact Registry APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Add a custom container while creating an instance
To add a custom container to a managed notebooks instance,
the custom container image must be specified at instance creation.
To add a custom container while you create
a managed notebooks instance, complete the following steps.
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Click
add_box
Create new
.
In the
Name
field, enter a name for your instance.
Click the
Region
list, and select a region for your instance.
In the
Environment
section,
select
Provide custom Docker images
.
Add a Docker container image in one of the following ways:
Enter a Docker container image path. For example,
to use a TensorFlow 2.12 container image with accelerators from
Deep Learning Containers
,
enter
us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf-cpu.2-12.py310
.
Click
Select
to add a Docker container image from
Artifact Registry. Then on the
Artifact Registry
tab
where your container image is stored, change the project
to the project that includes your container image, and select
your container image.
Complete the rest of the
Create instance
dialog
according to your needs.
Click
Create
.
Vertex AI Workbench automatically starts the instance. When the
instance is ready to use, Vertex AI Workbench activates an
Open JupyterLab
link.
Grant permissions to Deep Learning Containers container images
If you aren't using a Deep Learning Containers container image,
skip this section.
To ensure that your instance's service account has the necessary
      permissions to load a Deep Learning Containers container
image from Artifact Registry,
    
      ask your administrator to grant your instance's service account the
    following IAM roles on your instance:
Important:
You must grant these roles
      to your instance's service account,
not
to your user account. Failure to grant the roles to the correct principal might result in permission errors.
Compute Instance Admin (v1)
(
roles/compute.instanceAdmin.v1
)
Artifact Registry Reader
(
roles/artifactregistry.reader
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
Your administrator might also be able to give your instance's service account
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Set up a notebook file to run in your custom container
To open JupyterLab, create a new notebook file, and set it up to run
on your custom container's kernel, complete the following steps.
Next to your managed notebooks instance's name,
click
Open JupyterLab
.
In the
Authenticate your managed notebook
dialog, click the button
to get an authentication code.
Choose an account and click
Allow
. Copy the authentication code.
In the
Authenticate your managed notebook
dialog,
paste the authentication code, and then click
Authenticate
.
Your managed notebooks instance opens JupyterLab.
Select
File
>
New
>
Notebook
.
In the
Select kernel
dialog, select the kernel for the
custom container image that you want to use,
and then click
Select
. Larger container images
may take some time to appear as a kernel. If the kernel that you want
isn't there yet, try again in a few minutes. You can
change the
kernel
whenever you want to run your notebook file on a different kernel.
Your new notebook file opens.
What's next
Learn how to
access Cloud Storage buckets and files
from within JupyterLab
.
Learn how to
query data in BigQuery tables
from within JupyterLab
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/dataproc.txt
Run a managed notebooks instance on a Dataproc cluster  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Run a managed notebooks instance on a Dataproc cluster
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page shows you how to run a managed notebooks instance's
notebook file on a Dataproc cluster.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Dataproc APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Dataproc APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Required roles
To ensure that the service account has the necessary
      permissions to run a notebook file on a Serverless for Apache Spark cluster,
    
      ask your administrator to grant the service account the
    following IAM roles:
Important:
You must grant these roles
      to the service account,
not
to your user account. Failure to grant the roles to the correct principal might result in permission errors.
Dataproc Worker
(
roles/dataproc.worker
)
    
              on your project
Dataproc Editor
(
roles/dataproc.editor
)
    
              on the cluster for the
dataproc.clusters.use
permission
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
These predefined roles contain
        
        the permissions required to run a notebook file on a Serverless for Apache Spark cluster. To see the exact permissions that are
        required, expand the
Required permissions
section:
Required permissions
The following permissions are required to run a notebook file on a Serverless for Apache Spark cluster:
dataproc.agents.create
dataproc.agents.delete
dataproc.agents.get
dataproc.agents.update
dataproc.tasks.lease
dataproc.tasks.listInvalidatedLeases
dataproc.tasks.reportStatus
dataproc.clusters.use
Your administrator might also be able to give the service account
          these permissions
        with
custom roles
or
        other
predefined roles
.
Create a Dataproc cluster
To run a managed notebooks instance's notebook file
in a Dataproc cluster, your cluster must meet the following
criteria:
The cluster's component gateway must be enabled.
The cluster must have
the
Jupyter component
.
The cluster must be in the same region as
your managed notebooks instance.
To create your Dataproc cluster,
enter the following command in either
Cloud Shell
or another
environment where the
Google Cloud CLI
is installed.
gcloud
dataproc
clusters
create
CLUSTER_NAME
\
--region
=
REGION
\
--enable-component-gateway
\
--optional-components
=
JUPYTER
Replace the following:
REGION
: the Google Cloud location of
your managed notebooks instance
CLUSTER_NAME
: the name of your new
cluster
After a few minutes, your Dataproc cluster
is available for use.
Learn more about creating Dataproc
clusters
.
Open JupyterLab
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Next to your managed notebooks instance's name,
click
Open JupyterLab
.
Run a notebook file in your Dataproc cluster
You can run a notebook file in your Dataproc cluster
from any managed notebooks instance in the same project and
region.
Run a new notebook file
In your managed notebooks instance's JupyterLab interface,
select
File
>
New
>
Notebook
.
Your Dataproc cluster's available kernels appear in
the
Select kernel
menu. Select the kernel that you want to use,
and then click
Select
.
Your new notebook file opens.
Add code to your new notebook file, and run the code.
To change the kernel that you want to use
after you've created your notebook file, see the following section.
Run an existing notebook file
In your managed notebooks instance's JupyterLab interface,
click the
folder
File Browser
button,
navigate to the notebook file that you want to run, and open it.
To open the
Select kernel
dialog, click the kernel name of your notebook
file, for example:
Python (Local)
.
To select a kernel from your Dataproc cluster,
select a kernel name that includes your cluster name at the end of it.
For example, a PySpark kernel on a Dataproc cluster
named
mycluster
is named
PySpark on mycluster
.
Click
Select
to close the dialog.
You can now run your notebook file's code
on the Dataproc cluster.
What's next
Learn more about
Dataproc
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/develop.txt
Model development in Vertex AI Workbench managed notebooks  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Model development in a managed notebooks instance
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page describes common ways to develop a machine learning (ML) model
in Vertex AI Workbench managed notebooks. You can use
pre-installed Python packages that are commonly used for ML model development,
Vertex AI custom training, and BigQuery ML.
Common Python packages
By default, managed notebooks instances are pre-installed
with Python packages that are commonly used for model development.
Import these packages into your notebook file and they are ready to use.
Vertex AI custom training
You can use
Vertex AI custom training
to create and train models from within
your managed notebooks instance.
Install one of the Vertex AI
client libraries
on your
instance, or use the
Vertex AI API
to send API requests from a Jupyter notebook file.
BigQuery ML
Using
BigQuery ML
, you can train models that use
your BigQuery data, all from within
your managed notebooks instance.
For example, by using the
Python client for
BigQuery
,
you can send SQL commands from your notebook file
to create a model, and then use the model to get batch predictions.
BigQuery ML leverages the BigQuery computational engine,
so you don't need to deploy the compute resources
required for batch predictions or model training.
This can reduce the time it takes to set up training, evaluation,
and prediction.
What's next
To learn more about Vertex AI custom training, see
Understand the custom training service
.
To learn more about BigQuery ML, see
What is BigQuery ML?
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/executor-parameters.txt
Run notebook executions with parameters  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Run notebook executions with parameters
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
Vertex AI Workbench managed notebooks instances
let you use parameter values in your notebook executions
to specify differences in how your notebook file's code runs.
This page describes how to set up your notebook file to use parameters
and how to run executions that specify different values
for your notebook parameters.
Use parameters to run different iterations of your notebook file
You can use notebook parameter values in your executions
to run the same notebook code while specifying differences like the following:
Specify a different dataset to use, or a different sample size
of the dataset.
Specify different model configurations such as learning rate or
optimizer type.
Run different models, or run different versions of the same model.
How to use parameters in a notebook execution
The process for executing a notebook with parameters has two main steps:
Add the
parameters
tag to one of your notebook file's cells
.
While this isn't a technical requirement, this cell
typically contains code that assigns values to your parameter
variables, though this is not a technical requirement.
If you don't assign different parameter values in your execution,
the execution uses the parameter values in your notebook file
as default values.
Create an execution for your notebook file that includes
new values for your parameters
. Use the
following pattern to format your parameters and their values:
parameter1=value1,parameter2=value2
. The format requires commas
between parameter-value pairs, no spaces, and no quotation marks.
When your execution runs,
the executor adds a cell to the notebook that updates the
values of your parameters directly following the cell that
is tagged
parameters
.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Required roles
To ensure that your instance's service account has the necessary
      permissions to interact with the Vertex AI Workbench executor,
    
      ask your administrator to grant your instance's service account the
    following IAM roles on the project:
Important:
You must grant these roles
      to your instance's service account,
not
to your user account. Failure to grant the roles to the correct principal might result in permission errors.
Notebooks Viewer (
roles/notebooks.viewer
)
Vertex AI User (
roles/aiplatform.user
)
Storage Admin (
roles/storage.admin
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
Your administrator might also be able to give your instance's service account
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Open JupyterLab
To open JupyterLab and prepare a notebook file to run,
complete the following steps.
Open JupyterLab
.
Upload a notebook (ipynb) file, open an existing file,
or
open a new notebook
file
and add the code that you want to run to the new notebook.
Make sure your notebook file's code meets the
requirements
for using the executor
.
Add the
parameters
tag to a notebook cell
In your managed notebooks instance's
JupyterLab user interface, open the notebook file that you want to run.
Write code in one cell that assigns values to
your parameter variables.
These are the values your notebook file uses if
you don't assign different parameter values in your execution.
Make sure your parameters cell is still selected, and then
in the right sidebar, click the
Property inspector
.
In the property inspector, in the
Cell Tags
section,
click
Add Tag
, enter
parameters
, and then press
Enter
.
Note:
If you tag more than one cell with
parameters
, the executor
adds only one parameters cell directly following the first cell
with the
parameters
tag.
Provide parameter values for your execution
In your managed notebooks instance's
JupyterLab user interface, click the
Executor
button.
In the
Submit notebooks to Executor
dialog,
enter a name for your execution in the
Execution name
field.
Select a
Machine type
and
Accelerator type
.
Select an
Environment
.
In the
Type
field,
select
One-time execution
, or
select
Schedule-based recurring executions
, and complete
the dialog for scheduling executions.
In
Advanced options
,
select the
Region
where you want to run your notebook.
In the
Cloud Storage bucket
field,
select an available Cloud Storage bucket or
enter a name for a new bucket and click
Create and select
.
The executor stores your notebook output
in this Cloud Storage bucket.
In the
Notebook parameterization
section
and the
Input parameters
text box,
add notebook parameters separated by commas, for example
optimizer=SGD,learning_rate=0.01
. The format requires
that there are no spaces and no quotation marks.
Configure the rest of your execution, and then click
Submit
.
What's next
Learn more about
how to run notebook code in
the executor
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/executor.txt
Run notebook files with the executor  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Run notebook files with the executor
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page describes how to use the executor in
a Vertex AI Workbench managed notebooks instance to run notebook files
as a one-time execution and on a schedule.
Overview
The executor lets you submit a notebook (ipynb) file to run on
Vertex AI custom training
.
You can set parameter values for each execution of a notebook file.
You can also run a notebook file on a recurring schedule.
After the execution completes, you can view the execution results and
share them with others.
When a notebook file is submitted to Vertex AI custom training,
Vertex AI creates a new custom training job
that executes your notebook file following the
lifecycle of a training
job
.
Requirements for notebook code run by the executor
When you write notebook code to run in the executor, keep in mind
that the code will run in a
tenant project
separate from your managed notebooks instance's project.
This section describes how this affects your code when it is run
in the executor.
Ensure package installations are available to the executor
If your notebook depends on package installations that are not already
included in the managed notebooks kernel that you are using,
make sure your packages are available to your notebook code
in the executor in one of the following ways:
Use a
custom container
that already has the package installed, and then execute your notebook
on that custom container. See the
requirements for using
a custom container with the executor
.
Install the package within your notebook file's code.
The package is installed every time the notebook file is executed,
but this ensures that the package is available on
the container that you select when you execute your notebook.
Use explicit project selection
When you access resources through code run by the executor,
the executor might not connect to the correct
Google Cloud project. If you encounter permission errors,
connecting to the wrong project might be the problem.
This problem occurs because the executor does not run your code
directly in your managed notebooks instance's
Google Cloud project. Instead, the executor
runs your code in Vertex AI custom training
within a tenant project managed by Google.
Therefore, don't try to infer a project ID from the environment in
your notebook code; specify project IDs explicitly.
If you don't want to hardcode a project ID in your code, you can
reference the
CLOUD_ML_PROJECT_ID
environment variable.
Vertex AI
sets this environment variable in every custom training container to contain the
project number
of the project where you initiated
custom training. Many Google Cloud tools can accept a project
number wherever they take a project ID.
For example, if you want to use the
Python Client for Google
BigQuery
to access a
BigQuery table in the same project, then do not infer the
project in your notebook code:
Implicit project selection
from
google.cloud
import
bigquery
client
=
bigquery
.
Client
()
Instead use code that explicitly selects a project:
Explicit project selection
import
os
from
google.cloud
import
bigquery
project_number
=
os
.
environ
[
"CLOUD_ML_PROJECT_ID"
]
client
=
bigquery
.
Client
(
project
=
project_number
)
Authenticate access using service accounts
By default, your managed notebooks instance
can have access to resources that exist in the same project.
Therefore, when you run your notebook file's code manually,
these resources do not need additional authentication. However,
because the executor runs in a separate tenant project, it does not
have the same default access.
Also, the executor cannot use end-user credentials to authenticate access
to resources, for example, the
gcloud auth login
command
.
To resolve these issues, in your notebook file's code,
authenticate access to resources through a service account.
Then when you create an execution or schedule, specify the
service account.
For example, while you
create an execution
,
complete these steps:
In the
Submit notebooks to Executor
dialog,
expand
Advanced options
.
In the
Identity and API access
section,
clear the check mark next to
Use Vertex AI Training's default service account
and enter the specific service account to use.
See the full list of
steps for creating an execution
.
Requirements when using a custom container
You can use the executor to run notebook code on a custom container.
Your custom container must include the
nbexecutor
extension, which
enables the executor to run notebook code as
a Vertex AI custom training job.
To ensure that your custom container has the
nbexecutor
extension,
you can modify one of the
Deep Learning Containers container images to
create a
derivative container image
.
Deep Learning Containers images
include the
nbexecutor
extension.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Required roles
To ensure that your instance's service account has the necessary
      permissions to interact with the Vertex AI Workbench executor,
    
      ask your administrator to grant your instance's service account the
    following IAM roles on the project:
Important:
You must grant these roles
      to your instance's service account,
not
to your user account. Failure to grant the roles to the correct principal might result in permission errors.
Notebooks Viewer (
roles/notebooks.viewer
)
Vertex AI User (
roles/aiplatform.user
)
Storage Admin (
roles/storage.admin
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
Your administrator might also be able to give your instance's service account
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Open JupyterLab
To open JupyterLab and prepare a notebook file to run,
complete the following steps.
Open JupyterLab
.
Upload a notebook (ipynb) file, open an existing file,
or
open a new notebook
file
and add the code that you want to run to the notebook.
Make sure your notebook file's code meets the
requirements
for using the executor
.
Create an execution
To create an execution that runs your notebook file,
complete the following steps. These steps cover both scheduling
executions and creating a one-time execution.
In your managed notebooks instance's
JupyterLab user interface, open the notebook file that you want to run.
Click the
Execute
button.
In the
Submit notebooks to Executor
dialog,
in the
Execution name
field, enter a name for your execution.
Select a
Machine type
and
Accelerator type
.
Select an
Environment
.
In the
Type
field,
select
One-time execution
or
select
Schedule-based recurring executions
and complete
the dialog for scheduling executions.
In
Advanced options
,
select the
Region
where you want to run your notebook.
In the
Cloud Storage bucket
field,
select an available Cloud Storage bucket or
enter a name for a new bucket and click
Create and select
.
The executor stores your notebook output
in this Cloud Storage bucket.
Optional: in the
Notebook parameterization
section,
in the
Input parameters
text box,
add notebook parameters separated by commas, for example
optimizer="SGD",learning_rate=0.01
.
Learn more about
how to use
notebook parameters
.
Optional: in the
Identity and API access
section,
select
Use Vertex AI Training's default service account
or clear the check mark and enter a specific service account to use.
Note:
If your notebook file's code authenticates access to services
by using end-user credentials or default access within
your managed notebooks instance's project,
see the
Requirements for notebook code run
by the executor
and consider specifying a service account.
Optional: in the
Networking
section,
specify a Virtual Private Cloud network. Using a VPC network
for your execution requires a
private services
access
connection.
Click
Submit
.
One-time executions begin immediately.
Scheduled executions run automatically
on the schedule that you set.
Note:
If your managed notebooks instance is shut down, the
executor still runs your notebook file on schedule.
In the Google Cloud console, on the
Vertex AI Workbench
page,
you can view your completed executions on
the
Executions
tab
and view your schedules on the
Schedules
tab
.
View, share, and import an executed notebook file
By using your managed notebooks instance's
JupyterLab user interface, you can view an executed notebook's output,
share the results with others,
and import the executed notebook file into JupyterLab.
View a notebook execution's results
You can view a notebook execution's results in the Google Cloud console
or in the JupyterLab user interface.
Console
In the Google Cloud console, go to the
Vertex AI Workbench
page
and click the
Executions
tab.
Go to Executions
Select the
Region
that contains your results.
Next to the execution that you want to view, click
View result
.
The result opens in a new browser tab.
JupyterLab
In JupyterLab's navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Under the execution that you want to view, click
View result
.
The result opens in a new browser tab.
Share a notebook execution's results
You can share execution results by providing access to the
Cloud Storage bucket that contains your notebook execution.
Providing this access also grants users access to any other resources
in the same Cloud Storage bucket.
To share execution results, complete the following steps.
Console
In the Google Cloud console, go to the
Vertex AI Workbench
page
and click the
Executions
tab.
Go to Executions
Select the
Region
that contains the execution.
Next to the execution that you want to share, click the
people
Share
button.
Follow the directions in the dialog
to grant users access to the Cloud Storage bucket
that contains your notebook execution.
JupyterLab
In your managed notebooks instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Next to the execution that you want to share,
click the
more_vert
options menu,
and select
Share execution result
.
Follow the directions in the dialog
to grant users access to the Cloud Storage bucket
that contains your notebook execution.
Import an executed notebook into JupyterLab
To import an executed notebook into JupyterLab, complete the following steps.
In your managed notebooks instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Select the
Region
that contains your executed notebook.
Next to the execution that you want to import,
click the
more_vert
options menu,
and select
Import executed notebook
.
Select the kernel that you want to open the notebook.
The executor opens the executed notebook file
in JupyterLab and stores this notebook file in
the JupyterLab File Browser in a folder named
imported_notebook_jobs
.
View or delete a schedule
You can view and delete schedules by using either the Google Cloud console or
your managed notebooks instance's JupyterLab user interface.
View a schedule
View a schedule to see the frequency settings of the schedule
or to view results of your notebook executions.
Console
In the Google Cloud console, go to the
Vertex AI Workbench
page
and click the
Schedules
tab.
Go to Schedules
Select the
Region
that contains your schedule.
Click a schedule name to open the
Schedule details
page.
Next to an execution name, click
View result
to open
the executed notebook file. The executor opens
your result in a new browser tab.
JupyterLab
In your managed notebooks instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Schedules
tab.
To view the latest execution, under the execution that you want to view,
click
View latest execution result
.
The executor opens your result in a new browser tab.
To see all of the executions, click the name of the schedule.
The executor opens the
Schedule details
page in
the Google Cloud console.
Next to an execution name, click
View result
to open
the executed notebook file. The executor opens
your result in a new browser tab.
Delete a schedule
Deleting a schedule does not delete the executions that were
generated from that schedule.
Console
In the Google Cloud console, go to the
Vertex AI Workbench
page
and click the
Schedules
tab.
Go to Schedules
Select the
Region
that contains your schedule.
Select the schedule you want to delete.
Click
delete
Delete
.
JupyterLab
In your managed notebooks instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Schedules
tab.
Click the name of the schedule that you want to delete.
The executor opens the
Schedule details
page in
the Google Cloud console.
Click
delete
Delete
.
Jobs on Vertex AI custom training
Because notebook executions are run on Vertex AI custom training,
they are exposed as custom training jobs in Vertex AI.
You can view these custom training jobs
in the Google Cloud console, on the
Custom jobs
tab of
the
Vertex AI Training
page
.
Learn more about
how to work with
Vertex AI custom training jobs
.
What's next
Learn how to
run notebook executions
with parameters
.
Learn more about
Vertex AI custom training
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/idle-shutdown.txt
Idle shutdown  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Idle shutdown
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
Vertex AI Workbench managed notebooks instances
shut down after a specified period of inactivity by default.
This page describes the idle shutdown feature and how to
change the default idle shutdown settings during instance creation.
Overview
To help manage costs,
managed notebooks instances
shut down after being idle for a specific time period by default.
You can change the amount of time or turn this feature off.
Billing
While your instance is shut down, there are no CPU or GPU
usage charges except for scheduled executions that run during
the shutdown. For more information about scheduled executions, see
Scheduled executions run while instance is shut down
on this page.
Disk storage charges still apply while
your instance is shut down.
For more information,
see
Pricing
.
Disable idle shutdown or change the default inactivity time period
Idle shutdown is enabled and set to shut down your instance after
180 inactive minutes by default.
To disable idle shutdown or to change the inactivity time period
on an existing instance:
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Click the instance name.
On the
Notebook details
page, click the
Software and security
tab.
In the
Modify software and security configuration
section:
To disable idle shutdown, clear the check mark next to
Enable Idle Shutdown
.
To change the inactivity time period,
in
Time of inactivity before shutdown (Minutes)
, change
the number to the number of minutes of inactivity that you want.
In the Google Cloud console, this setting
can be set to any integer value from 10 to 1440.
Click
Submit
.
How idle timeout works
Your instance shuts down when
there is no kernel activity for the specified time period.
For example, running a cell or new output printing to a notebook
is activity that resets the idle timeout timer. CPU usage
doesn't reset the idle timeout timer.
Scheduled executions run while instance is shut down
If you have scheduled an execution of a notebook file
in a managed notebooks instance that is shut down,
it will still run on schedule.
For more information,
see
Run notebook files with the executor
.
What's next
To run a notebook file on a schedule,
even when your instance is shut down,
schedule
an execution
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/introduction.txt
Introduction to Vertex AI Workbench managed notebooks  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Introduction to managed notebooks
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
Vertex AI Workbench managed notebooks instances
are Google-managed environments
with integrations and capabilities that help you set up and work in
an end-to-end Jupyter notebook-based production environment.
Managed notebooks instances are prepackaged with
JupyterLab
and have a preinstalled suite of deep learning packages,
including support for the TensorFlow and PyTorch
frameworks. Managed notebooks instances support GPU accelerators and
the ability to sync with a
GitHub
repository.
Your managed notebooks instances are protected
by Google Cloud authentication and authorization.
Google-managed compute infrastructure
A Vertex AI Workbench managed notebooks instance
is a Google-managed, Jupyter notebook-based, compute infrastructure.
When you create a managed notebooks instance,
it is deployed as a Google-managed virtual machine (VM) instance in a
tenant project
.
Your managed notebooks instance includes many common
data science framework environments, such as TensorFlow
and PyTorch. You can also add your own custom container images to
your managed notebooks instance. These environments
are available as kernels that you can run your
notebook file in.
When you run a notebook in one of the kernels, Vertex AI Workbench
starts the corresponding container, creates a Jupyter session on it, and
uses that Jupyter session to run your notebook on the container.
This Google-managed compute infrastructure includes integrations
and capabilities that help you implement data science and machine learning
workflows from start to finish. See the following sections for details.
Use custom containers
You can add custom Docker container images to
your managed notebooks instance
to run your notebook code in an environment customized for your needs.
These custom containers are available to use directly from the
JupyterLab user interface, alongside the preinstalled frameworks.
For more information, see
Add a custom container to
a managed notebooks instance
.
Notebook-based workflow
Managed notebooks instances let you
perform workflow-oriented tasks without leaving the JupyterLab user interface.
Control your hardware and framework from JupyterLab
In a managed notebooks instance, your JupyterLab user interface
is where you specify what compute resources your code will run on. For example,
you can configure how many vCPUs or GPUs you want, how much RAM you want, and
what framework you want to run the code in. You can write your code first, and
then choose how to run it without leaving JupyterLab
or restarting your instance.
For quick tests of your code, you can scale your hardware down and then scale it
back up to run your code against more data.
Access to data
You can access your data without leaving the JupyterLab user interface.
In JupyterLab's navigation menu on
a managed notebooks instance, you can use the
Cloud Storage integration
to browse data and other files that you have access to.
See
Access Cloud Storage buckets and files
from within JupyterLab
.
You can also use the
BigQuery integration
to browse tables that you have access to,
write queries, preview results, and load data into your notebook.
See
Query data in BigQuery tables
from within JupyterLab
.
Execute notebook runs
Use the executor to run a notebook file
as a one-time execution or on a schedule.
Choose the specific environment and hardware that you want
your execution to run on. Your notebook's code will run on
Vertex AI custom training, which can make it easier
to do distributed training, optimize hyperparameters, or
schedule continuous training jobs. See
Run notebook files
with the executor
.
You can
use parameters in
your execution
to make specific changes to each run.
For example, you might specify a different dataset to use,
change the learning rate on your model, or change the version
of the model.
You can also
set a notebook to run on a recurring
schedule
.
Even while your instance is shut down, Vertex AI Workbench will
run your notebook file and save the results
for you to look at and share with others.
Share insights
Executed notebook runs are stored in a Cloud Storage bucket,
so you can share your insights with others by granting access
to the results. See the
previous section on executing
notebook runs
.
Secure your instance
You can deploy your managed notebooks instance
with the default Google-managed network,
which uses a default VPC network and subnet.
Instead of the default network, you can specify a
VPC network to use with your instance.
For more information, see
Set up a network
. You can use
VPC Service Controls
to provide additional security for your
managed notebooks instances.
To use managed notebooks within a service perimeter, see
Use
a managed notebooks instance within a service
perimeter
.
By default, Google Cloud automatically
encrypts data when it is at
rest
using encryption keys managed by Google. If you have specific compliance or
regulatory requirements related to the keys that protect your data, you can
use customer-managed encryption keys (CMEK) with
your managed notebooks instances. For more information,
see
Use customer-managed encryption keys
.
Automated shutdown for idle instances
To help manage costs,
managed notebooks instances
shut down after being idle for a specific time period by default.
You can change the amount of time or turn this feature off.
For more information,
see
Idle shutdown
.
Dataproc integration
You can process data quickly by running a notebook
on a Dataproc cluster.
After your cluster is set up, you can run
a notebook file on it without leaving the JupyterLab user interface.
For more information, see
Run a managed notebooks instance
on a Dataproc cluster
.
Limitations
Consider the following limitations of
managed notebooks when planning your project:
Managed notebooks instances are Google-managed
and therefore less customizable than Vertex AI Workbench
user-managed notebooks instances.
User-managed notebooks instances can be
more ideal for users who need a lot of control over their environment.
For more information, see
Introduction to
user-managed notebooks
.
Third party JupyterLab extensions are not supported.
The Dataproc JupyterLab plugin isn't supported for
managed notebooks, but you can use the plugin in
Vertex AI Workbench instances. See
Create a
Dataproc-enabled
instance
.
Managed notebooks instances do not allow users to
have
sudo
access.
When you use
Access Context Manager
and
Chrome Enterprise Premium
to protect managed notebooks instances with
context-aware access controls, access is evaluated each time
the user authenticates to the instance. For example, access
is evaluated the first time the user accesses JupyterLab and
whenever they access it thereafter if their web browser's
cookie has expired.
To use accelerators with managed notebooks instances,
the accelerator type that you want must be available in your instance's
zone. To learn about accelerator availability by zone, see
GPU regions and zones availability
.
What's next
Learn more about the
networking options available for your
managed notebooks instance
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/manage-access-jupyterlab.txt
Manage access to a Vertex AI Workbench managed notebooks instance's JupyterLab interface  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Manage access to a managed notebooks instance's JupyterLab interface
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page describes how to grant access to the JupyterLab interface
of a Vertex AI Workbench managed notebooks instance.
You control access to a managed notebooks instance's
JupyterLab interface through the instance's access mode.
You set a JupyterLab access mode when you create
a managed notebooks instance.
The access mode can't be changed after the notebook is created.
The JupyterLab access mode determines who can use
the instance's JupyterLab interface.
The access mode also determines which credentials are used when
your instance interacts with other Google Cloud services.
Access limitations
Granting a principal access to
a managed notebooks instance's JupyterLab interface
doesn't grant access to the instance itself. For example,
to start, stop, or reset an instance, you must grant the principal
access to perform those operations by setting an
IAM policy
on the instance.
To grant access to the managed notebooks instance,
see
Manage access to
a managed notebooks instance
.
JupyterLab access modes
Managed notebooks instances support the
following access modes:
Single user only
: The
Single user only
access mode
grants access only to the user that you specify.
Service account
: The
Service account
access mode
grants access to a service account. You can grant access to one or more
users through this service account.
Note:
To grant access to the instance through the single user option
      or the service account, you must use an individual's
      user account email address. Group access is not supported.
Single user only
When you create a managed notebooks instance
with
Single user only
access, you specify a user account.
The specified user account is the only user with access to
the JupyterLab interface. If
the instance needs to access
other Google Cloud resources,
this user account must also have access
to those Google Cloud resources.
Note:
When you create a managed notebooks instance
with
Single user only
access, your instance completes the boot process
using the Compute Engine default service account.
Your specified user account can access the instance after the boot process
is finished.
Grant access to a single user
To grant access to a single user, complete the following steps.
Create
a managed notebooks instance
with the following specifications:
In the
Create instance
dialog, in
the
IAM and security
section, select the
Single user only
access mode.
In the
User email
field, enter the user account that you want
to grant access.
Complete the rest of the dialog, and then click
Create
.
Service account
When you create a managed notebooks instance
with
Service account
access, you specify a service account. If
the instance needs to access
other Google resources, this service account must have access to those
Google resources also.
When you specify a service account,
choose one of the following:
Select the Compute Engine default service account.
Specify a custom service account. The custom service account must be
in the same project as your managed notebooks instance.
To create the instance, you must have
the
iam.serviceAccounts.actAs
permission on the service account.
To grant access to users through a service account,
you grant the
iam.serviceAccounts.actAs
permission on
the specified service account for each user who needs
to access JupyterLab.
Grant access to multiple users through a service account
Create
a managed notebooks instance
with the following specifications:
In the
Create instance
dialog, in
the
IAM and security
section, select the
Service account
access mode.
Choose the Compute Engine default service account
or a
custom
service account
.
To use the Compute Engine default service account,
select
Use Compute Engine default service account
.
To use a custom service account, clear
Use Compute Engine default service account
, and then,
in the
Service account email
field, enter
your custom service account email address.
Complete the rest of the dialog, and then click
Create
.
For each user who needs to access JupyterLab,
grant the
iam.serviceAccounts.actAs
permission on your
service account
.
Access mode metadata
The access mode that you configure during
managed notebooks instance creation
is stored in the notebook metadata.
When you select the
Single user only
access mode,
Vertex AI Workbench stores a value for
proxy-mode
and
proxy-user-mail
.
The following are examples of single user access metadata entries:
proxy-mode=mail
proxy-user-mail=user@example.com
When you select the
Service account
access mode, Vertex AI Workbench
stores a
proxy-mode=service_account
metadata entry.
Caution:
Changing the access mode metadata is not supported and can make the
JupyterLab interface inaccessible.
What's next
Grant a principal access to
a managed notebooks instance.
To learn how to grant access to other Google resources, see
Manage access to
other resources
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/manage-access.txt
Manage access to a Vertex AI Workbench managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Manage access to a managed notebooks instance
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This guide describes how you can grant access to
a specific Vertex AI Workbench managed notebooks instance.
To manage access to Vertex AI resources, see
the
Vertex AI page on access control
.
You grant access to a managed notebooks instance by setting an
Identity and Access Management (IAM) policy
on the instance.
The policy binds one or more principals, such as a user or a
service account, to one or more
roles
.
Each role contains a list of permissions that let the principal interact
with the instance.
You can grant access to an instance, instead of to a parent resource
such as a project, folder, or organization, to exercise the
principle of
least privilege
.
If you grant access to a
parent resource
(for example, to a project), you implicitly grant access to all its child
resources (for example, to all instances in that project). To limit access to
resources, set IAM policies on lower-level resources when
possible, instead of at the project level or above.
For general information about how to grant, change, and revoke access to
resources unrelated to Vertex AI Workbench, for example, to grant access to
a Google Cloud project, see the IAM documentation for
Granting, changing, and revoking access
to resources
.
Access limitations
Access to an instance can include a broad range of abilities, depending
on the role you assign to the principal. For example,
you might grant a principal the ability to start, stop, upgrade, and
monitor the health status of an instance. For the complete list of
IAM permissions available, see
Predefined
managed notebooks IAM
roles
.
However, even granting a principal full access to
a managed notebooks instance doesn't grant
the ability to use the instance's JupyterLab interface.
To grant access to the JupyterLab interface, see
Manage access to a
managed notebooks instance's
JupyterLab interface
.
Grant access to managed notebooks instances
To grant users permission to access
a specific managed notebooks instance,
set an
IAM policy
on the instance.
To grant a role to a principal on
a managed notebooks instance, use the
getIamPolicy
method to retrieve the current policy,
edit the current policy's access, and then use the
setIamPolicy
method to update the policy on the instance.
Retrieve the current policy
Before using any of the request data,
  make the following replacements:
INSTANCE_NAME
: The name of your managed notebooks instance
HTTP method and URL:
GET https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:getIamPolicy
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Execute the following command:
curl -X GET \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:getIamPolicy"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method GET `
-Headers $headers `
-Uri "https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:getIamPolicy" | Select-Object -Expand Content
The response is the text of your instance's IAM policy.
See the following for an example.
{
  "bindings": [
    {
      "role": "roles/notebooks.viewer",
      "members": [
        "user:email@example.com"
      ]
    }
  ],
  "etag": "BwWWja0YfJA=",
  "version": 3
}
Edit the policy
Edit the policy with a text editor to add or remove principals and their
associated roles. For example, to grant the
notebooks.admin
role to
eve@example.com, add the following new binding to the policy
in the
"bindings"
section:
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
After adding the new binding, the policy might look like the following:
{
"bindings"
:
[
{
"role": "roles/notebooks.viewer",
"members": [
"user:email@example.com"
]
}
,
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
]
,
"etag"
:
"BwWWja0YfJA="
,
"version"
:
3
}
Update the policy on the instance
In the body of the request, provide the updated IAM
policy from the previous step, nested inside a
"policy"
section.
Before using any of the request data,
  make the following replacements:
INSTANCE_NAME
: The name of your managed notebooks instance
HTTP method and URL:
POST https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:setIamPolicy
Request JSON body:
{
  "policy": {
    "bindings": [
      {
        "role": "roles/notebooks.viewer",
        "members": [
          "user:email@example.com"
        ]
      },
      {
        "role": "roles/notebooks.admin",
        "members": [
          "user:eve@example.com"
        ]
      }
    ],
    "etag": "BwWWja0YfJA=",
    "version": 3
  }
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:setIamPolicy"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:setIamPolicy" | Select-Object -Expand Content
You should receive a successful status code (2xx) and an empty response.
Grant access to the JupyterLab interface
Granting a principal access to
a managed notebooks instance doesn't grant
the ability to use the instance's JupyterLab interface.
To grant access to the JupyterLab interface, see
Manage access to a
managed notebooks instance's
JupyterLab interface
.
What's next
Grant a principal access to
JupyterLab.
To learn about Identity and Access Management (IAM) and how
IAM roles can help grant and restrict access,
see the
IAM documentation
.
Learn about the
IAM roles available
to Vertex AI Workbench
managed notebooks
.
Learn how to create and manage
custom roles
.
To learn how to grant access to other Google resources, see
Manage access to
other resources
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/migrate-to-instances.txt
Migrate from managed notebooks to Vertex AI Workbench instances  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Migrate from managed notebooks to Vertex AI Workbench instances
Vertex AI Workbench managed notebooks is
deprecated
.
    On April 14, 2025, support for managed notebooks
    will end and the ability to create managed notebooks instances
    will be removed. Existing instances will continue to function
    but patches, updates, and upgrades won't be available. To continue using
    Vertex AI Workbench, complete the steps on this page to
migrate
    your managed notebooks instances to
    Vertex AI Workbench instances
.
This page describes how to migrate from
a managed notebooks instance to a Vertex AI Workbench instance.
You can migrate by using the Vertex AI Workbench migration tool or
migrate your instance's data and files manually.
Overview of the migration tool
Vertex AI Workbench provides a migration tool for migrating from
a managed notebooks instance to a Vertex AI Workbench instance.
The migration tool creates a Vertex AI Workbench instance with a
configuration similar to the managed notebooks instance
that you want to migrate. For example,
the migration tool creates an instance that has the same or similar
machine type, network configuration, idle shutdown settings,
and other specifications. Then, the files on your
managed notebooks instance's data disk are copied to the
Vertex AI Workbench instance.
Vertex AI Workbench doesn't delete or change your
managed notebooks instance, so after migration you can continue
to use it. If you don't need the managed notebooks instance
anymore,
delete it
to avoid further charges for that instance.
Billing
If your managed notebooks instance uses Extreme Persistent Disks,
then your migration generates charges for I/O operations. See "Extreme
provisioned IOPS" in the
Persistent Disk and Hyperdisk pricing section of
Disk pricing
.
After migration, the managed notebooks instance still exists
and generates charges as before. If you don't need
the managed notebooks instance
anymore,
delete it
to avoid further charges for that instance.
Default migration tool behaviors
The Vertex AI Workbench migration tool attempts to
migrate your managed notebooks instance to
a Vertex AI Workbench instance with matching specifications.
When a specification in your managed notebooks instance isn't
available in Vertex AI Workbench instances, Vertex AI Workbench
uses a default specification when possible. When the migration tool
can't migrate a specification of your managed notebooks instance,
it doesn't migrate the instance.
The following table lists some of the key default migration behaviors
for the migration tool.
Category
Managed notebooks specification
Migration result
OS
Any Ubuntu version
Debian 11
Any Debian version
Debian 11
Framework
Any CUDA version
CUDA 11.3
Any Python version
Python 3.10
Any PyTorch version
PyTorch 1.13
Any TensorFlow version
TensorFlow 2.11
Any R version
Not migrated; see
Add
        a conda environment
Any local PySpark version
Not migrated; see
Add
        a conda environment
Any XGBoost version
Not migrated; see
Add
        a conda environment
Any Kaggle Python version
Not migrated; see
Add
        a conda environment
Any Jax version
Not migrated; see
Add
        a conda environment
Any Apache Beam version
Not migrated; see
Add
        a conda environment
Machine type
A supported machine type
Identical machine type
An unsupported machine type
e2-standard-4
Accelerators
Supported accelerators
Identical accelerators
Unsupported accelerators
Migration doesn't include accelerators
Setting
Idle shutdown
Migrated
Delete to trash
Migrated
nbconvert
Migrated
File downloading
Migrated
Terminal access
Migrated
Other
Identity and Access Management permissions
Migrated, though new permissions might be required to use
        the Vertex AI Workbench instance
Access mode
Migrated; instances that use the single user JupyterLab access mode must
specify the
serviceAccount
option
Network
Migrated; instances that use a Google-managed Virtual Private Cloud must
specify the
network
and
subnet
options
Post-startup script
When using the Google Cloud console, the instance is migrated without
        the post-startup script; to migrate the instance with the
        post-startup script, use the Google Cloud CLI or REST API to
specify the
PostStartupScriptOption
option
Dataproc Hub
Not migrated; must
migrate manually
Specifying options
The following sections describe cases where specifying an option
is required to migrate your managed notebooks instance
to a Vertex AI Workbench instance.
Instances that use the single user access mode
Managed notebooks instances that use the
single user access mode must be migrated to an
instance with the
serviceAccount
option specified.
The Vertex AI Workbench instance that you migrate to restricts
access to JupyterLab to the single user, but it uses a service account
to interact with Google Cloud services and APIs.
Instances that use a Google-managed VPC
Managed notebooks instances that use a Google-managed
VPC must be migrated to an instance with the
network
and
subnet
options specified.
The option to use a Google-managed VPC isn't supported
in Vertex AI Workbench instances, so a different network must be
specified.
Instances that use a post-startup script
Managed notebooks instances that use a post-startup script
must be migrated to an instance with the
PostStartupScriptOption
option specified. Use this option to indicate whether you want to
skip or rerun the post-startup script in your new
Vertex AI Workbench instance.
Specifying the
PostStartupScriptOption
option isn't supported
in the Google Cloud console. To specify the
PostStartupScriptOption
option when you migrate
your managed notebooks instance, you must
use the Google Cloud CLI or REST API.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Required roles
To get the permissions that
      you need to migrate a managed notebooks instance to a Vertex AI Workbench instance,
    
      ask your administrator to grant you the
    
  
  Notebooks Runner (
roles/notebooks.runner
)
   IAM role on the project.
  

  

  
  
  For more information about granting roles, see
Manage access to projects, folders, and organizations
.
This predefined role contains
        
        the permissions required to migrate a managed notebooks instance to a Vertex AI Workbench instance. To see the exact permissions that are
        required, expand the
Required permissions
section:
Required permissions
The following permissions are required to migrate a managed notebooks instance to a Vertex AI Workbench instance:
notebooks.runtimes.create
notebooks.runtimes.get
You might also be able to get
          these permissions
        with
custom roles
or
        other
predefined roles
.
Pre-migration check
Before you migrate, check your managed notebooks instance's
migration eligibility by listing your instances and checking
the output for any migration warnings or errors.
List your instances
To list your managed notebooks instances that aren't
migrated yet, use the
projects.locations.runtimes.list
method with the filter
migrated:false
. You can list them by using the
gcloud CLI or REST API:
gcloud
Before using any of the command data below,
  make the following replacements:
PROJECT_ID
: Your project ID
LOCATION
: The region where your managed notebooks instance is located, or use
-
to list instances from all regions
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
runtimes
list
--project
=
PROJECT_ID
\
--location
=
LOCATION
--filter
=
migrated:false
--format
=
default
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
runtimes
list
--project
=
PROJECT_ID
`
--location
=
LOCATION
--filter
=
migrated:false
--format
=
default
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
runtimes
list
--project
=
PROJECT_ID
^
--location
=
LOCATION
--filter
=
migrated:false
--format
=
default
REST
Before using any of the request data,
  make the following replacements:
PROJECT_ID
: Your project ID
LOCATION
: The region where your managed notebooks instance is located, or use
-
to list instances from all regions
HTTP method and URL:
GET https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes?filter=migrated:false
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Execute the following command:
curl -X GET \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes?filter=migrated:false"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method GET `
-Headers $headers `
-Uri "https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes?filter=migrated:false" | Select-Object -Expand Content
Check the output for warnings or errors
If migration warnings or errors are detected, the output of
the
projects.locations.runtimes.list
method includes this information.
Warnings appear when specific components in
your managed notebooks instance's configuration won't migrate
to the same specification in a Vertex AI Workbench instance.
For example, if your managed notebooks instance uses an
unsupported accelerator, a warning appears in the output. In this case,
the instance is migrated without any accelerators. You can attach
accelerators after migration. Review the warnings in
the output, consider the
migration tool's default behaviors
,
and assess whether the migration tool is acceptable for your migration.
One or more errors in the output means that you can't migrate
the managed notebooks instance by using the migration tool.
You must
migrate the instance manually
.
For more information about migration warnings and errors, see
warnings
and
errors
in the
RuntimeMigrationEligibility
documentation.
Migrate by using the migration tool
You can migrate your managed notebooks instance by using
the Google Cloud console, the gcloud CLI, or REST API.
Console
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Click the
Migrate
button. The
Migrate managed notebooks to instances
page opens.
To migrate instances that don't need options specified,
click the
Ready
tab, select the instances that you want to migrate,
and then click
Migrate
.
To migrate instances that need options specified,
click the
Needs input
tab, select the instances that you want to
migrate, and then click
Migrate
.
In the
Provide input for migration
dialog,
specify a network and service account to use for the
new Vertex AI Workbench instances that you selected.
Click
Submit
.
After your migrations are finished, go to the
Instances
page
to view your new Vertex AI Workbench instances.
Go to Instances
gcloud
Before using any of the command data below,
  make the following replacements:
PROJECT_ID
: Your project ID
LOCATION
: The region where your managed notebooks instance is located
RUNTIME_ID
: The ID of the managed notebooks instance
NETWORK
: The network that you want to migrate the instance to
SUBNET
: The subnet that you want to migrate the instance to
SUBNET_REGION
: The subnet's region
SERVICE_ACCOUNT
: Optional: The email address of the service account that you want to use
POST_STARTUP_SCRIPT_OPTION
: Optional: One of the
post-startup script options
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
runtimes
migrate
RUNTIME_ID
\
--project
=
PROJECT_ID
\
--location
=
LOCATION
\
--network
=
NETWORK
\
--subnet
=
SUBNET
\
--subnet-region
=
SUBNET_REGION
\
--service-account
=
SERVICE_ACCOUNT
\
--post-startup-script-option
=
POST_STARTUP_SCRIPT_OPTION
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
runtimes
migrate
RUNTIME_ID
`
--project
=
PROJECT_ID
`
--location
=
LOCATION
`
--network
=
NETWORK
`
--subnet
=
SUBNET
`
--subnet-region
=
SUBNET_REGION
`
--service-account
=
SERVICE_ACCOUNT
`
--post-startup-script-option
=
POST_STARTUP_SCRIPT_OPTION
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
runtimes
migrate
RUNTIME_ID
^
--project
=
PROJECT_ID
^
--location
=
LOCATION
^
--network
=
NETWORK
^
--subnet
=
SUBNET
^
--subnet-region
=
SUBNET_REGION
^
--service-account
=
SERVICE_ACCOUNT
^
--post-startup-script-option
=
POST_STARTUP_SCRIPT_OPTION
REST
Before using any of the request data,
  make the following replacements:
PROJECT_ID
: Your project ID
LOCATION
: The region where your managed notebooks instance is located
RUNTIME_ID
: The ID of the managed notebooks instance
NETWORK
: The network that you want to migrate the instance to
SUBNET
: The subnet that you want to migrate the instance to
SERVICE_ACCOUNT
: Optional: The email address of the service account that you want to use
POST_STARTUP_SCRIPT_OPTION
: Optional: One of the
post-startup script options
HTTP method and URL:
POST https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes/
RUNTIME_ID
:migrate
Request JSON body:
{
  "network":
NETWORK
,
  "subnet":
SUBNET
,
  "serviceAccount":
SERVICE_ACCOUNT_EMAIL_ADDRESS
,
  "postStartupScriptOption": (
POST_STARTUP_SCRIPT_OPTION
)
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes/
RUNTIME_ID
:migrate"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes/
RUNTIME_ID
:migrate" | Select-Object -Expand Content
Migrate manually
To migrate your instance to a Vertex AI Workbench instance manually,
consider using the following methods:
Use Cloud Storage and the terminal
: Copy your data and files
to Cloud Storage and then to another instance by using the terminal.
Use GitHub
: Copy your data and files to a GitHub repository
by using the Git extension for JupyterLab.
This guide describes how to migrate data and files by using
Cloud Storage and the terminal.
Requirements
You must have terminal access to your managed notebooks instance.
Terminal access is manually set when you create an instance. The
terminal access setting cannot be changed after the instance is created.
Migrate manually by using Cloud Storage and the terminal
To migrate data and files to a new Vertex AI Workbench instance
by using Cloud Storage and the terminal, do the following.
Create a Cloud Storage bucket
in the same project where your managed notebooks instance
is located.
In that same project,
Create
a Vertex AI Workbench instance
to migrate your data to.
When you create this instance:
Enable terminal access.
Specify the machine type, network,
and other characteristics to match what you need.
In your managed notebooks instance's
JupyterLab interface, select
File
>
New
>
Terminal
to open a terminal window.
Use the
gcloud CLI
to copy your user data
to a Cloud Storage bucket. The following example command
copies all of the files from your instance's
/home/jupyter/
directory
to a directory in a Cloud Storage bucket.
gcloud
storage
cp
/home/jupyter/*
gs://
BUCKET_NAME
PATH
--recursive
Replace the following:
BUCKET_NAME
: The name of your
Cloud Storage bucket
PATH
: The path to the directory
where you want to copy your files, for example:
/copy/jupyter/
In your new Vertex AI Workbench instance's
JupyterLab interface, select
File
>
New
>
Terminal
to open a terminal window.
Use the gcloud CLI to copy your data to the new instance.
The following example command copies all of
the files from a Cloud Storage directory to the
your new instance's
/home/jupyter/
directory.
gcloud
storage
cp
gs://
BUCKET_NAME
PATH
*
/home/jupyter/
--recursive
Confirm the migration
After the migration, the original managed notebooks instance
continues to work as before. Confirm that your migration was a success
before you delete the original instance.
Delete the managed notebooks instance
If you don't need the managed notebooks instance that
you migrated from, delete it to avoid further charges for that instance.
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the instance that you want to delete.
Click
delete
Delete
.
(Depending on the size of your window,
the
Delete
button might be in
the
more_vert
options menu.)
To confirm, click
Delete
.
Troubleshoot
To find methods for diagnosing and resolving migration issues, see
Troubleshooting
Vertex AI Workbench
.
What's next
Learn more about
Vertex AI Workbench instances
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/migrate.txt
Migrate data to a new Vertex AI Workbench managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Migrate data to a new managed notebooks instance
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page describes how to migrate data and files from
an existing managed notebooks instance to a new one.
When to migrate
You might want to migrate to a new managed notebooks instance
for any of the following reasons:
To use new capabilities that are only available in a newer version.
To benefit from framework updates, package updates, and bug fixes
that have been implemented in a newer version.
If you're unable to upgrade the environment of an existing instance.
See the
requirements for upgrading the environment of
a managed notebooks
instance
.
Migration options
To migrate data and files from one managed notebooks instance to another,
consider using the following methods:
Use GitHub
: Copy your data and files to a GitHub repository
by using the Git extension for JupyterLab.
Use Cloud Storage and the terminal
: Copy your data and files
to Cloud Storage and then to another instance by using the terminal.
Use Cloud Storage within JupyterLab notebooks
:
Copy your data and files to Cloud Storage and then to another instance
by running commands within your respective instances' notebook cells.
This guide describes how to migrate data and files by using
Cloud Storage and the terminal.
Requirements
You must have terminal access to your managed notebooks instance.
Terminal access is manually set when you create an instance. The
terminal access setting cannot be changed after the instance is created.
Before you begin
Create a Cloud Storage bucket
in the same project where your managed notebooks instance
is located.
Migrate your data to a new managed notebooks instance
To migrate data and files to a new managed notebooks instance
by using Cloud Storage and the terminal, complete the following steps.
In your managed notebooks instance's
JupyterLab interface, select
File
>
New
>
Terminal
to open a terminal window.
Use the
gcloud CLI
to copy your user data
to a Cloud Storage bucket. The following example command
copies all of the files from your instance's
/home/jupyter/
directory
to a directory in a Cloud Storage bucket.
gcloud
storage
cp
/home/jupyter/*
gs://
BUCKET_NAME
PATH
--recursive
Replace the following:
BUCKET_NAME
: the name of your
Cloud Storage bucket
PATH
: the path to the directory
where you want to copy your files, for example:
/copy/jupyter/
Open your managed notebooks instance's JupyterLab interface.
In your managed notebooks instance's
JupyterLab interface, select
File
>
New
>
Terminal
to open a terminal window.
Use the gcloud CLI to restore your data on the new instance.
The following example command copies all of
the files from a Cloud Storage directory to the
your new instance's
/home/jupyter/
directory.
gcloud
storage
cp
gs://
BUCKET_NAME
PATH
*
/home/jupyter/
What's next
Learn how to
manually upgrade the environment of
managed notebooks instances
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/networking.txt
Set up a network  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Set up a network
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page describes networking options for Vertex AI Workbench
managed notebooks instances and shows you how to
set up a network.
This guide is recommended for networking administrators
who are already familiar with Google Cloud networking concepts.
Overview
This guide describes how to configure each of the following network options:
Google-managed network
VPC network in the same project as your managed notebooks instance
Shared VPC network
By default, your managed notebooks instance uses
a Google-managed network. If you want to,
you can instead specify a Virtual Private Cloud network located within your
project or a Shared VPC network that you have access to.
If you specify a VPC or Shared VPC network,
the network requires a
private services
access
connection.
Supported feature comparison
The following table describes which common features are supported for each
networking option.
Feature
Google-managed network
VPC network in your instance's project
Shared VPC network
External IP
Supported
Supported
Supported
Internal IP
Supported
Supported
Supported
Private Google Access
Not supported
Supported
Supported
VPC
Supported
Supported
Supported
VPC Network Peering (requires Service Networking)
Not supported
Supported
Supported
Note:
Because VPC Network Peering is not supported when using
the default Google-managed network, an external IP address is required
in order to download additional resources such as Python or Conda packages.
Use the default Google-managed network
The default network is Google-managed and requires no additional setup
to configure.
When you create a managed notebooks instance
with the default Google-managed network,
the instance is deployed in a
tenant project
and uses a default VPC and subnet.
To download additional resources such as Python or Conda packages,
a managed notebooks instance using the
default Google-managed network requires an external IP address.
Connect your instance to a VPC network in the same project
To connect a managed notebooks instance
to a VPC network in the same project
as your managed notebooks instance,
complete the following steps.
This option requires you to configure
private services access
.
Before you begin
Select or
create a Google Cloud
project
where your
managed notebooks instance will be.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Compute Engine, Notebooks, and Service Networking APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Install the
gcloud CLI
to run the
gcloud
command-line examples in this guide.
Set up private services access for your VPC
When you set up private services access,
you establish a private connection between your network
and a network owned by Google or a third party service (
service producers
).
In this case, your managed notebooks instance
is a service producer. To
set up
private services access
, you
reserve an IP range
for the service producer, and then
create
a peering connection
with your managed notebooks instance.
Configure your project ID
To configure your project ID, use the following command.
gcloud
config
set
project
PROJECT_ID
Replace
PROJECT_ID
with the
project ID
of the Google Cloud project where
your managed notebooks instance will be.
You'll create the instance later.
Enable the APIs
Make sure you have
enabled the required APIs
.
Create or select a VPC
Create or select an existing VPC in a
supported
managed notebooks region
to use with your managed notebooks instance.
If you already have a VPC
with private services access configured,
and you want to use that VPC to peer with
your managed notebooks instance,
skip to
Create a managed notebooks instance
.
If you need to create a new VPC,
run the following gcloud CLI commands:
gcloud
compute
networks
create
VPC_NAME
\
--project
=
PROJECT_ID
--subnet-mode
=
auto
\
--mtu
=
1460
--bgp-routing-mode
=
regional

gcloud
compute
firewall-rules
create
VPC_NAME
-allow-icmp
\
--project
=
PROJECT_ID
\
--network
=
projects/
PROJECT_ID
/global/networks/
VPC_NAME
\
--description
=
Allows
\
ICMP
\
connections
\
from
\
any
\
source
\
to
\
any
\
instance
\
on
\
the
\
network.
\
--direction
=
INGRESS
--priority
=
65534
--source-ranges
=
0
.0.0.0/0
\
--action
=
ALLOW
--rules
=
icmp

gcloud
compute
firewall-rules
create
VPC_NAME
-allow-internal
\
--project
=
PROJECT_ID
\
--network
=
projects/
PROJECT_ID
/global/networks/
VPC_NAME
\
--description
=
Allows
\
connections
\
from
\
any
\
source
\
in
\
the
\
network
\
IP
\
range
\
to
\
any
\
instance
\
on
\
the
\
network
\
using
\
all
\
protocols.
\
--direction
=
INGRESS
--priority
=
65534
--source-ranges
=
10
.128.0.0/9
\
--action
=
ALLOW
--rules
=
all

gcloud
compute
firewall-rules
create
VPC_NAME
-allow-rdp
\
--project
=
PROJECT_ID
\
--network
=
projects/
PROJECT_ID
/global/networks/
VPC_NAME
\
--description
=
Allows
\
RDP
\
connections
\
from
\
any
\
source
\
to
\
any
\
instance
\
on
\
the
\
network
\
using
\
port
\
3389
.
\
--direction
=
INGRESS
--priority
=
65534
--source-ranges
=
0
.0.0.0/0
\
--action
=
ALLOW
--rules
=
tcp:3389

gcloud
compute
firewall-rules
create
VPC_NAME
-allow-ssh
\
--project
=
PROJECT_ID
\
--network
=
projects/
PROJECT_ID
/global/networks/
VPC_NAME
\
--description
=
Allows
\
TCP
\
connections
\
from
\
any
\
source
\
to
\
any
\
instance
\
on
\
the
\
network
\
using
\
port
\
22
.
\
--direction
=
INGRESS
--priority
=
65534
--source-ranges
=
0
.0.0.0/0
\
--action
=
ALLOW
--rules
=
tcp:22
Replace
VPC_NAME
with a name for
your VPC.
Create and configure DNS entries
Vertex AI Workbench managed notebooks instances use several domains that a
  Virtual Private Cloud network doesn't handle by default.
  To ensure that your VPC network correctly handles requests sent
  to those domains, use Cloud DNS to add DNS records. For more
  information about VPC routes, see
Routes
.
To create a
managed zone
for
  a domain, add a DNS entry that will route the request, and execute
  the transaction, complete the following steps.
  Repeat these steps for each of
several
  domains
that you need to handle requests for, starting
  with
*.notebooks.googleapis.com
.
In
Cloud Shell
or any environment where the
Google Cloud CLI
is installed, enter the following
Google Cloud CLI
commands.
To create a private managed zone
      for one of the domains that your
      VPC network needs to handle:
gcloud
dns
managed-zones
create
ZONE_NAME
\
--visibility
=
private
\
--networks
=
https://www.googleapis.com/compute/v1/projects/
PROJECT_ID
/global/networks/
NETWORK_NAME
\
--dns-name
=
DNS_NAME
\
--description
=
"Description of your managed zone"
Replace the following:
ZONE_NAME
: a name for the zone to create.
        You must use a separate zone for each domain. This zone name is used in
        each of the following steps.
PROJECT_ID
: the ID of the project that hosts your
        VPC network
NETWORK_NAME
: the name of the VPC
        network that you created earlier
DNS_NAME
: the part of the domain name that comes
        after the
*.
, with a period on the end.
        For example,
*.notebooks.googleapis.com
has a
DNS_NAME
of
notebooks.googleapis.com.
Start a transaction.
gcloud
dns
record-sets
transaction
start
--zone
=
ZONE_NAME
Add the following DNS A record. This reroutes traffic to
      Google's restricted IP addresses.
gcloud
dns
record-sets
transaction
add
\
--name
=
DNS_NAME
.
\
--type
=
A
199
.36.153.4
199
.36.153.5
199
.36.153.6
199
.36.153.7
\
--zone
=
ZONE_NAME
\
--ttl
=
300
Add the following DNS CNAME record to point to the A record
      that you just added. This redirects all traffic matching the
      domain to the IP addresses listed in the previous step.
gcloud
dns
record-sets
transaction
add
\
--name
=
\*
.
DNS_NAME
.
\
--type
=
CNAME
DNS_NAME
.
\
--zone
=
ZONE_NAME
\
--ttl
=
300
Execute the transaction.
gcloud
dns
record-sets
transaction
execute
--zone
=
ZONE_NAME
Repeat these steps for each of the following domains. For each
      repetition, change
ZONE_NAME
and
DNS_NAME
to the appropriate values for that
      domain. Keep
PROJECT_ID
and
NETWORK_NAME
the same each time. You already
      completed these steps for
*.notebooks.googleapis.com
.
*.notebooks.googleapis.com
*.notebooks.cloud.google.com
*.notebooks.googleusercontent.com
*.googleapis.com
to run code that interacts with other Google APIs
        and services
Reserve IP ranges for your managed notebooks instance
When you reserve an IP range for service producers, the range can be used by
your managed notebooks instance and other services. If you
plan to connect with other service producers using the same range,
you might want to allocate a larger range to accommodate them,
to avoid IP exhaustion.
Use the following command to set a reserved range using
gcloud compute addresses create
.
gcloud
compute
addresses
create
PEERING_RANGE_NAME
\
--global
\
--prefix-length
=
16
\
--description
=
"Managed notebooks range"
\
--network
=
NETWORK_NAME
\
--purpose
=
VPC_PEERING
Replace the following:
PEERING_RANGE_NAME
: the name of your range
NETWORK_NAME
: the name of your network
A
prefix-length
value of
16
means that a CIDR block
with a subnet mask of
/16
will be
reserved for use by Google Cloud services
such as Vertex AI Workbench managed notebooks.
To avoid an invalid service networking configuration, use a subnet mask of
/24
or lower.
Use the following command to verify the addresses.
gcloud
compute
addresses
list
Establish a peering connection
Establish a peering connection between your
VPC host project and Google's Service Networking, using
gcloud services vpc-peerings connect
.
gcloud
services
vpc-peerings
connect
\
--service
=
servicenetworking.googleapis.com
\
--network
=
NETWORK_NAME
\
--ranges
=
PEERING_RANGE_NAME
\
--project
=
PROJECT_ID
Note:
The
--ranges
flag accepts a list of ranges so that you can
specify multiple ranges if necessary.
To list the
peerings, use the following command.
gcloud
services
vpc-peerings
list
--network
=
NETWORK_NAME
Create a managed notebooks instance
Before using any of the request data,
  make the following replacements:
USER_ACCOUNT
: The user account in the form of an email address.
MACHINE_TYPE
: The
machine type
,
    for example
n1-standard-1
.
PROJECT_ID
: The project ID of your managed notebooks instance.
NETWORK_NAME
: The VPC network name.
LOCATION
: The region of your VPC network.
NOTEBOOK_NAME
: The name of your managed notebooks instance.
SUBNET_NAME
: The subnet name for your VPC network.
PEERING_RANGE_NAME
: Optional. The name of the peering range
    if you want to specify one.
HTTP method and URL:
POST https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes?runtimeId=
NOTEBOOK_NAME
Request JSON body:
{
  "access_config": {
    "access_type": "SINGLE_USER",
    "runtime_owner": "
USER_ACCOUNT
"
  },
  "virtual_machine": {
    "virtual_machine_config": {
      "machine_type": "
MACHINE_TYPE
",
      "network": "projects/
PROJECT_ID
/global/networks/
NETWORK_NAME
",
      "subnet":  "projects/
PROJECT_ID
/regions/
LOCATION
/subnetworks/
SUBNET_NAME
",
      "internal_ip_only": true,
      "reserved_ip_range": "
PEERING_RANGE_NAME
" # Optional
    }
  }
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes?runtimeId=
NOTEBOOK_NAME
"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes?runtimeId=
NOTEBOOK_NAME
" | Select-Object -Expand Content
Verify connectivity
To verify that your managed notebooks instance is connected
to the VPC
 network,
complete these steps.
In the Google Cloud console,
go to the
VPC network peering
page.
Go to VPC network peering
On the
VPC network peering
page, find your connection.
Export custom routes
If you use custom routes, you need to export them so that
Vertex AI Workbench managed notebooks can import them.
To export custom routes, you
update
the peering connection
in your VPC. Exporting custom routes sends all
eligible
static and dynamic routes
that are
in your VPC network, such as routes to your on-premises network,
to service producers' networks (in this case, managed notebooks).
This establishes the necessary connections and lets
managed notebooks instances send traffic back
to your on-premises network.
To list the name of the peering connection to update,
use the following command.
If you have multiple peering connections, omit the
--format
flag.
gcloud
services
vpc-peerings
list
\
--network
=
NETWORK_NAME
\
--service
=
servicenetworking.googleapis.com
\
--project
=
PROJECT_ID
\
--format
"value(peering)"
To update the peering connection to export custom routes,
use the following command.
gcloud
compute
networks
peerings
update
PEERING_NAME
\
--network
=
NETWORK_NAME
\
--export-custom-routes
\
--project
=
PROJECT_ID
Replace
PEERING_NAME
with the name of your peering connection.
Check the state of your peering connections
To check whether your peering connections are active,
you can list them using the following command.
gcloud
compute
networks
peerings
list
--network
NETWORK_NAME
Verify that the state of the peering connection that
you just created is
ACTIVE
.
Learn more about
active
peering connections
.
Connect your instance to a Shared VPC network
To connect a managed notebooks instance
to a Shared VPC network that you have access to,
complete the following steps.
This option requires you to configure
private services access
.
Before you begin
Select or
create a Google Cloud
project
where your
managed notebooks instance will be.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Compute Engine, Notebooks, and Service Networking APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
When you use
Shared VPC
, you run
your managed notebooks instance in
a separate Google Cloud project than
your VPC host project.
Repeat the previous steps to enable the Compute Engine, Notebooks,
and Service Networking APIs
in your VPC host project.
Learn more about how to
provision Shared VPC
.
Install the
gcloud CLI
to run the
gcloud
command-line examples in this guide.
Set up private services access for your VPC
When you set up private services access,
you establish a private connection between your network
and a network owned by Google or a third party service (
service producers
).
In this case, your managed notebooks instance
is a service producer. To
set up
private services access
, you
reserve an IP range
for the service producer, and then
create
a peering connection
with your managed notebooks instance.
Configure your project ID
To configure your project ID, use the following command.
gcloud
config
set
project
PROJECT_ID
Replace
PROJECT_ID
with the
project ID
of your VPC host project. If you haven't created
the VPC yet, use the project ID where it will be after
it is created.
Enable the APIs
Make sure you have
enabled the required APIs
in both your VPC host project and
the Google Cloud project where
your managed notebooks instance will be.
Create or select a VPC
Create or select an existing VPC in a
supported
managed notebooks region
to use with your managed notebooks instance.
If you already have a VPC
with private services access configured,
and you want to use that VPC to peer with
your managed notebooks instance,
skip to
Create a managed notebooks instance
.
If you need to create a new VPC,
run the following gcloud CLI commands:
gcloud
compute
networks
create
VPC_NAME
\
--project
=
PROJECT_ID
--subnet-mode
=
auto
\
--mtu
=
1460
--bgp-routing-mode
=
regional

gcloud
compute
firewall-rules
create
VPC_NAME
-allow-icmp
\
--project
=
PROJECT_ID
\
--network
=
projects/
PROJECT_ID
/global/networks/
VPC_NAME
\
--description
=
Allows
\
ICMP
\
connections
\
from
\
any
\
source
\
to
\
any
\
instance
\
on
\
the
\
network.
\
--direction
=
INGRESS
--priority
=
65534
--source-ranges
=
0
.0.0.0/0
\
--action
=
ALLOW
--rules
=
icmp

gcloud
compute
firewall-rules
create
VPC_NAME
-allow-internal
\
--project
=
PROJECT_ID
\
--network
=
projects/
PROJECT_ID
/global/networks/
VPC_NAME
\
--description
=
Allows
\
connections
\
from
\
any
\
source
\
in
\
the
\
network
\
IP
\
range
\
to
\
any
\
instance
\
on
\
the
\
network
\
using
\
all
\
protocols.
\
--direction
=
INGRESS
--priority
=
65534
--source-ranges
=
10
.128.0.0/9
\
--action
=
ALLOW
--rules
=
all

gcloud
compute
firewall-rules
create
VPC_NAME
-allow-rdp
\
--project
=
PROJECT_ID
\
--network
=
projects/
PROJECT_ID
/global/networks/
VPC_NAME
\
--description
=
Allows
\
RDP
\
connections
\
from
\
any
\
source
\
to
\
any
\
instance
\
on
\
the
\
network
\
using
\
port
\
3389
.
\
--direction
=
INGRESS
--priority
=
65534
--source-ranges
=
0
.0.0.0/0
\
--action
=
ALLOW
--rules
=
tcp:3389

gcloud
compute
firewall-rules
create
VPC_NAME
-allow-ssh
\
--project
=
PROJECT_ID
\
--network
=
projects/
PROJECT_ID
/global/networks/
VPC_NAME
\
--description
=
Allows
\
TCP
\
connections
\
from
\
any
\
source
\
to
\
any
\
instance
\
on
\
the
\
network
\
using
\
port
\
22
.
\
--direction
=
INGRESS
--priority
=
65534
--source-ranges
=
0
.0.0.0/0
\
--action
=
ALLOW
--rules
=
tcp:22
Replace
VPC_NAME
with a name for
your VPC.
Create and configure DNS entries
Vertex AI Workbench managed notebooks instances use several domains that a
  Virtual Private Cloud network doesn't handle by default.
  To ensure that your VPC network correctly handles requests sent
  to those domains, use Cloud DNS to add DNS records. For more
  information about VPC routes, see
Routes
.
To create a
managed zone
for
  a domain, add a DNS entry that will route the request, and execute
  the transaction, complete the following steps.
  Repeat these steps for each of
several
  domains
that you need to handle requests for, starting
  with
*.notebooks.googleapis.com
.
In
Cloud Shell
or any environment where the
Google Cloud CLI
is installed, enter the following
Google Cloud CLI
commands.
To create a private managed zone
      for one of the domains that your
      VPC network needs to handle:
gcloud
dns
managed-zones
create
ZONE_NAME
\
--visibility
=
private
\
--networks
=
https://www.googleapis.com/compute/v1/projects/
PROJECT_ID
/global/networks/
NETWORK_NAME
\
--dns-name
=
DNS_NAME
\
--description
=
"Description of your managed zone"
Replace the following:
ZONE_NAME
: a name for the zone to create.
        You must use a separate zone for each domain. This zone name is used in
        each of the following steps.
PROJECT_ID
: the ID of the project that hosts your
        VPC network
NETWORK_NAME
: the name of the VPC
        network that you created earlier
DNS_NAME
: the part of the domain name that comes
        after the
*.
, with a period on the end.
        For example,
*.notebooks.googleapis.com
has a
DNS_NAME
of
notebooks.googleapis.com.
Start a transaction.
gcloud
dns
record-sets
transaction
start
--zone
=
ZONE_NAME
Add the following DNS A record. This reroutes traffic to
      Google's restricted IP addresses.
gcloud
dns
record-sets
transaction
add
\
--name
=
DNS_NAME
.
\
--type
=
A
199
.36.153.4
199
.36.153.5
199
.36.153.6
199
.36.153.7
\
--zone
=
ZONE_NAME
\
--ttl
=
300
Add the following DNS CNAME record to point to the A record
      that you just added. This redirects all traffic matching the
      domain to the IP addresses listed in the previous step.
gcloud
dns
record-sets
transaction
add
\
--name
=
\*
.
DNS_NAME
.
\
--type
=
CNAME
DNS_NAME
.
\
--zone
=
ZONE_NAME
\
--ttl
=
300
Execute the transaction.
gcloud
dns
record-sets
transaction
execute
--zone
=
ZONE_NAME
Repeat these steps for each of the following domains. For each
      repetition, change
ZONE_NAME
and
DNS_NAME
to the appropriate values for that
      domain. Keep
PROJECT_ID
and
NETWORK_NAME
the same each time. You already
      completed these steps for
*.notebooks.googleapis.com
.
*.notebooks.googleapis.com
*.notebooks.cloud.google.com
*.notebooks.googleusercontent.com
*.googleapis.com
to run code that interacts with other Google APIs
        and services
Reserve IP ranges for your managed notebooks instance
When you reserve an IP range for service producers, the range can be used by
your managed notebooks instance and other services. If you
plan to connect with other service producers using the same range,
you might want to allocate a larger range to accommodate them,
to avoid IP exhaustion.
Use the following command to set a reserved range using
gcloud compute addresses create
.
gcloud
compute
addresses
create
PEERING_RANGE_NAME
\
--global
\
--prefix-length
=
16
\
--description
=
"Managed notebooks range"
\
--network
=
NETWORK_NAME
\
--purpose
=
VPC_PEERING
Replace the following:
PEERING_RANGE_NAME
: the name of your range
NETWORK_NAME
: the name of your network
A
prefix-length
value of
16
means that a CIDR block
with a subnet mask of
/16
will be
reserved for use by Google Cloud services
such as Vertex AI Workbench managed notebooks.
To avoid an invalid service networking configuration, use a subnet mask of
/24
or lower.
Use the following command to verify the addresses.
gcloud
compute
addresses
list
Establish a peering connection
Establish a peering connection between your
VPC host project and Google's Service Networking, using
gcloud services vpc-peerings connect
.
gcloud
services
vpc-peerings
connect
\
--service
=
servicenetworking.googleapis.com
\
--network
=
NETWORK_NAME
\
--ranges
=
PEERING_RANGE_NAME
\
--project
=
PROJECT_ID
Note:
The
--ranges
flag accepts a list of ranges so that you can
specify multiple ranges if necessary.
To list the
peerings, use the following command.
gcloud
services
vpc-peerings
list
--network
=
NETWORK_NAME
Create a managed notebooks instance
Before using any of the request data,
  make the following replacements:
USER_ACCOUNT
: The user account in the form of an email address.
MACHINE_TYPE
: The
machine type
,
    for example
n1-standard-1
.
PROJECT_ID
: The project ID of your managed notebooks instance.
NETWORK_NAME
: The VPC network name.
LOCATION
: The region of your VPC network.
NOTEBOOK_NAME
: The name of your managed notebooks instance.
SUBNET_NAME
: The subnet name for your VPC network.
PEERING_RANGE_NAME
: Optional. The name of the peering range
    if you want to specify one.
HTTP method and URL:
POST https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes?runtimeId=
NOTEBOOK_NAME
Request JSON body:
{
  "access_config": {
    "access_type": "SINGLE_USER",
    "runtime_owner": "
USER_ACCOUNT
"
  },
  "virtual_machine": {
    "virtual_machine_config": {
      "machine_type": "
MACHINE_TYPE
",
      "network": "projects/
PROJECT_ID
/global/networks/
NETWORK_NAME
",
      "subnet":  "projects/
PROJECT_ID
/regions/
LOCATION
/subnetworks/
SUBNET_NAME
",
      "internal_ip_only": true,
      "reserved_ip_range": "
PEERING_RANGE_NAME
" # Optional
    }
  }
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes?runtimeId=
NOTEBOOK_NAME
"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/runtimes?runtimeId=
NOTEBOOK_NAME
" | Select-Object -Expand Content
Verify connectivity
To verify that your managed notebooks instance is connected
to the Shared VPC network,
complete these steps.
In the Google Cloud console,
go to the
VPC network peering
page.
Go to VPC network peering
On the
VPC network peering
page, find your connection.
Export custom routes
If you use custom routes, you need to export them so that
Vertex AI Workbench managed notebooks can import them.
To export custom routes, you
update
the peering connection
in your VPC. Exporting custom routes sends all
eligible
static and dynamic routes
that are
in your VPC network, such as routes to your on-premises network,
to service producers' networks (in this case, managed notebooks).
This establishes the necessary connections and lets
managed notebooks instances send traffic back
to your on-premises network.
To list the name of the peering connection to update,
use the following command.
If you have multiple peering connections, omit the
--format
flag.
gcloud
services
vpc-peerings
list
\
--network
=
NETWORK_NAME
\
--service
=
servicenetworking.googleapis.com
\
--project
=
PROJECT_ID
\
--format
"value(peering)"
To update the peering connection to export custom routes,
use the following command.
gcloud
compute
networks
peerings
update
PEERING_NAME
\
--network
=
NETWORK_NAME
\
--export-custom-routes
\
--project
=
PROJECT_ID
Replace
PEERING_NAME
with the name of your peering connection.
Check the state of your peering connections
To check whether your peering connections are active,
you can list them using the following command.
gcloud
compute
networks
peerings
list
--network
NETWORK_NAME
Verify that the state of the peering connection that
you just created is
ACTIVE
.
Learn more about
active
peering connections
.
What's next
Learn more about
VPC Network Peering
.
See
reference architectures and
best practices
for VPC design.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/quickstart-schedule-execution-console.txt
Quickstart: Schedule a managed notebooks run  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Schedule a managed notebooks run
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page shows you how to use
the Vertex AI Workbench managed notebooks executor
to run a Python notebook file on an hourly schedule.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Required roles
To ensure that your instance's service account has the necessary
      permissions to interact with the Vertex AI Workbench executor,
    
      ask your administrator to grant your instance's service account the
    following IAM roles on the project:
Important:
You must grant these roles
      to your instance's service account,
not
to your user account. Failure to grant the roles to the correct principal might result in permission errors.
Notebooks Viewer (
roles/notebooks.viewer
)
Vertex AI User (
roles/aiplatform.user
)
Storage Admin (
roles/storage.admin
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
Your administrator might also be able to give your instance's service account
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create a managed notebooks instance and example notebook file
Create a managed notebooks
instance
.
Open JupyterLab
.
Open a new notebook file
.
In the first cell of the notebook file, enter the following:
# Import datetime
import
datetime
# Get the time and print it
datetime
.
datetime
.
now
()
print
(
datetime
.
datetime
.
now
())
To make sure your notebook file is saved, select
File
>
Save Notebook
.
Schedule a run
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Next to the managed notebooks instance
that you want to use,
click
Open JupyterLab
.
Your managed notebooks instance opens JupyterLab.
In the
folder
File Browser
,
double-click the example notebook file to open it.
Click the
Execute
button.
In the
Submit notebooks to Executor
dialog, in the
Type
field,
select
Schedule-based recurring executions
.
By default, the executor runs your notebook file
every hour at the
00
minute of the hour.
In
Advanced options
,
select the
Region
where you want to run your notebook.
In the
Cloud Storage bucket
field, enter a name for your bucket,
and then click
Create and select
.
The executor stores your notebook output
in the Cloud Storage bucket.
Click
Submit
.
Your notebook file runs automatically
on the schedule that you set.
Note:
If your managed notebooks instance is shut down, the
executor still runs your notebook file on schedule.
When you finish the tasks that are described in this document, you can avoid
   continued billing by deleting the resources that you created. For more information, see
Clean up
.
View, share, and import an executed notebook file
By using your managed notebooks instance's JupyterLab interface,
you can view your notebook output, share the results with others,
and import the executed notebook file into JupyterLab.
Note:
To use the Google Cloud console to view and share execution results,
on the
Executions
page,
click
Executions
.
View the execution results
In JupyterLab's navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Under the execution that you want to view, click
View result
.
Executor opens your result in a new browser tab.
Share the execution results
In your managed notebooks instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Next to the execution that you want to share,
click the
more_vert
options menu,
and select
Share execution result
.
Follow the directions in the dialog
to grant users access to the execution result.
Import the executed notebook into JupyterLab
In your managed notebooks instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Next to the execution that you want to import,
click the
more_vert
options menu,
and select
Import executed notebook
.
If the
Select Kernel
dialog appears,
select the kernel that you want to open the notebook.
The executor opens the executed notebook file
in JupyterLab, and stores this notebook file in
the JupyterLab File Browser in a folder named
imported_notebook_jobs
.
View or delete a schedule
You can view and delete schedules by using either the Google Cloud console or
your managed notebooks instance's JupyterLab user interface.
View a schedule
View a schedule to see the frequency settings of the schedule
or to view the five most recent results of the notebook file execution.
Console
In the Google Cloud console, go to the
Schedules
page.
Go to Schedules
Select the
Region
where you want to see schedules.
For the
Schedule details
page that you want to open,
 click its schedule name.
On the
Schedule details
page, you can view the schedule's last
 five executions.
Next to an execution name, click
View result
to open
 the executed notebook file.
Executor opens your result in a new browser tab.
JupyterLab
In your managed notebooks instance's
 JupyterLab user interface,
 in the navigation menu, click the
Notebook Executor
button.
Click the
Schedules
tab.
Under the execution that you want to view, click
View latest
 execution result
.
Executor opens your result in a new browser tab.
Delete a schedule
Deleting a schedule doesn't delete the executions that were
generated from that schedule.
Console
In the Google Cloud console, go to the
Schedules
page.
Go to Schedules
Select the
Region
that contains the schedule
 that you want to delete.
Select the schedule that you want to delete.
Click
delete
Delete
.
JupyterLab
In your managed notebooks instance's
 JupyterLab user interface,
 in the navigation menu, click the
Notebook Executor
button.
Click the
Schedules
tab.
At the end of the schedule name, click the
open_in_new
Open in
 new icon. The
Schedule details
page for that schedule opens in the
 Google Cloud console.
Click
delete
Delete
.
Clean up
To avoid incurring charges to your Google Cloud account for
          the resources used on this page, follow these steps.
Delete the instance
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the
Region
that contains your instance.
Select the managed notebooks instance that you want
to delete.
Click
delete
Delete
.
Delete the project
If you used resources outside of
your managed notebooks instance,
such as the Cloud Storage bucket required
for creating a schedule,
you might want to delete your project to avoid incurring additional charges.
Caution
: Deleting a project has the following effects:
Everything in the project is deleted.
If you used an existing project for
      the tasks in this document, when you delete it, you also delete any other work you've
      done in the project.
Custom project IDs are lost.
When you created this project, you might have created a custom project ID that you want to use in
      the future. To preserve the URLs that use the project ID, such as an
appspot.com
URL, delete selected resources inside the project instead of deleting the whole project.
If you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects
    can help you avoid exceeding project quota limits.
In the Google Cloud console, go to the
Manage resources
page.
Go to Manage resources
In the project list, select the project that you
    want to delete, and then click
Delete
.
In the dialog, type the project ID, and then click
Shut down
to delete the project.
What's next
Vertex AI Workbench managed notebooks instances are deprecated. To
schedule a notebook run in a Vertex AI Workbench instance, see
Schedule a notebook run
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/save-to-github.txt
Save a notebook to GitHub  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Save a notebook to GitHub
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
To back up your managed notebooks instance and make it available
to others, save the contents of your managed notebooks instance
to GitHub.
Create a GitHub repository
If you don't already have a
GitHub
repository, you must create one.
When you create your GitHub repository make sure that your GitHub repository
can be cloned by selecting the
Initialize this repository with a README
checkbox.
Clone your GitHub repository in your managed notebooks instance
To clone your GitHub repository in your managed notebooks
instance, complete the following steps:
In your GitHub repository, click the
Code
button,
and then click the
Local
tab.
Copy the
HTTPS
URL.
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Click
Open JupyterLab
to open
your managed notebooks instance.
In the JupyterLab
folder
File Browser
, select
the folder where you want to clone the GitHub repository. For example,
the home folder.
In JupyterLab, select
Git
>
Clone a Repository
.
If prompted, enter your credentials.
If you use a GitHub username and password, enter your
GitHub username and password.
If you use two-factor authentication with GitHub,
create and use a
personal access token
.
In the
Clone a repo
dialog, paste the HTTPS URL for your GitHub repository.
Click
Clone
.
Configure your managed notebooks instance with your GitHub user information
In JupyterLab, select
Git
>
Open Git Repository in Terminal
to open a Git terminal window.
In the Git terminal window, enter the following commands to configure
your Git username and email:
git config --global user.name "
YOUR_NAME
"
git config --global user.email "
YOUR_EMAIL
"
If your GitHub account requires SSH authentication, complete
the following steps to connect your account:
From your Git terminal in your managed notebooks
instance, follow GitHub's
instructions for generating a new SSH key
.
Follow the
instructions for adding that SSH key to your GitHub
account
.
Close the Git terminal window.
Add your committed files to your GitHub repository
Your managed notebooks instance shows your repository
as a new folder. If you don't
see your cloned GitHub repository as a folder, click the
Refresh File
List
button.
Double-click your repository folder to open it.
Add a new notebook to your managed notebooks instance.
To add a notebook file, you can use the menu or the Launcher.
Menu
To add a new notebook file from the menu, select
File
>
New
>
Notebook
.
In the
Select kernel
dialog, select the kernel for your new
notebook, for example,
Python 3
, and then click
Select
.
Your new notebook file opens.
Launcher
To add a new notebook file from the Launcher, select
File
>
New
>
Launcher
.
Click the tile for the kernel you want to use.
Your new notebook file opens.
Rename your new notebook file.
Menu
Select
File
>
Rename notebook
. The
Rename file
dialog opens.
In the
New name
field, change
Untitled.ipynb
to something
meaningful, such as
install.ipynb
.
Click
Rename
.
Launcher
Right-click the
Untitled.ipynb
tab and then click
Rename notebook
. The
Rename file
dialog opens.
In the
New name
field, change
Untitled.ipynb
to something
meaningful, such as
install.ipynb
.
Click
Rename
.
Select the
Git
tab. Your new notebook is listed in the
Untracked
grouping.
To add the new notebook as a file for your GitHub repository, right-click
the new notebook and select
Track
. On the
Git
tab, your notebook
is now added to the
Staged
grouping.
To commit your new notebook to your GitHub repository, on the
Git
tab,
add a commit comment and click
Commit
.
To open a Git terminal window, select
Git
>
Open Git repository in terminal
.
In the Git terminal window, enter the
git push
command.
If you use a GitHub username and password, when prompted, enter your
GitHub username and password.
If you use two-factor authentication with GitHub,
create a personal access token
to use.
When the
git push
command completes, your committed files are in
your GitHub repository.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/schedule-managed-notebooks-run-quickstart.txt
Quickstart: Schedule a managed notebooks run  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Schedule a managed notebooks run
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page shows you how to use
the Vertex AI Workbench managed notebooks executor
to run a Python notebook file on an hourly schedule.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Required roles
To ensure that your instance's service account has the necessary
      permissions to interact with the Vertex AI Workbench executor,
    
      ask your administrator to grant your instance's service account the
    following IAM roles on the project:
Important:
You must grant these roles
      to your instance's service account,
not
to your user account. Failure to grant the roles to the correct principal might result in permission errors.
Notebooks Viewer (
roles/notebooks.viewer
)
Vertex AI User (
roles/aiplatform.user
)
Storage Admin (
roles/storage.admin
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
Your administrator might also be able to give your instance's service account
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create a managed notebooks instance and example notebook file
Create a managed notebooks
instance
.
Open JupyterLab
.
Open a new notebook file
.
In the first cell of the notebook file, enter the following:
# Import datetime
import
datetime
# Get the time and print it
datetime
.
datetime
.
now
()
print
(
datetime
.
datetime
.
now
())
To make sure your notebook file is saved, select
File
>
Save Notebook
.
Schedule a run
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Next to the managed notebooks instance
that you want to use,
click
Open JupyterLab
.
Your managed notebooks instance opens JupyterLab.
In the
folder
File Browser
,
double-click the example notebook file to open it.
Click the
Execute
button.
In the
Submit notebooks to Executor
dialog, in the
Type
field,
select
Schedule-based recurring executions
.
By default, the executor runs your notebook file
every hour at the
00
minute of the hour.
In
Advanced options
,
select the
Region
where you want to run your notebook.
In the
Cloud Storage bucket
field, enter a name for your bucket,
and then click
Create and select
.
The executor stores your notebook output
in the Cloud Storage bucket.
Click
Submit
.
Your notebook file runs automatically
on the schedule that you set.
Note:
If your managed notebooks instance is shut down, the
executor still runs your notebook file on schedule.
When you finish the tasks that are described in this document, you can avoid
   continued billing by deleting the resources that you created. For more information, see
Clean up
.
View, share, and import an executed notebook file
By using your managed notebooks instance's JupyterLab interface,
you can view your notebook output, share the results with others,
and import the executed notebook file into JupyterLab.
Note:
To use the Google Cloud console to view and share execution results,
on the
Executions
page,
click
Executions
.
View the execution results
In JupyterLab's navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Under the execution that you want to view, click
View result
.
Executor opens your result in a new browser tab.
Share the execution results
In your managed notebooks instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Next to the execution that you want to share,
click the
more_vert
options menu,
and select
Share execution result
.
Follow the directions in the dialog
to grant users access to the execution result.
Import the executed notebook into JupyterLab
In your managed notebooks instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Next to the execution that you want to import,
click the
more_vert
options menu,
and select
Import executed notebook
.
If the
Select Kernel
dialog appears,
select the kernel that you want to open the notebook.
The executor opens the executed notebook file
in JupyterLab, and stores this notebook file in
the JupyterLab File Browser in a folder named
imported_notebook_jobs
.
View or delete a schedule
You can view and delete schedules by using either the Google Cloud console or
your managed notebooks instance's JupyterLab user interface.
View a schedule
View a schedule to see the frequency settings of the schedule
or to view the five most recent results of the notebook file execution.
Console
In the Google Cloud console, go to the
Schedules
page.
Go to Schedules
Select the
Region
where you want to see schedules.
For the
Schedule details
page that you want to open,
 click its schedule name.
On the
Schedule details
page, you can view the schedule's last
 five executions.
Next to an execution name, click
View result
to open
 the executed notebook file.
Executor opens your result in a new browser tab.
JupyterLab
In your managed notebooks instance's
 JupyterLab user interface,
 in the navigation menu, click the
Notebook Executor
button.
Click the
Schedules
tab.
Under the execution that you want to view, click
View latest
 execution result
.
Executor opens your result in a new browser tab.
Delete a schedule
Deleting a schedule doesn't delete the executions that were
generated from that schedule.
Console
In the Google Cloud console, go to the
Schedules
page.
Go to Schedules
Select the
Region
that contains the schedule
 that you want to delete.
Select the schedule that you want to delete.
Click
delete
Delete
.
JupyterLab
In your managed notebooks instance's
 JupyterLab user interface,
 in the navigation menu, click the
Notebook Executor
button.
Click the
Schedules
tab.
At the end of the schedule name, click the
open_in_new
Open in
 new icon. The
Schedule details
page for that schedule opens in the
 Google Cloud console.
Click
delete
Delete
.
Clean up
To avoid incurring charges to your Google Cloud account for
          the resources used on this page, follow these steps.
Delete the instance
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the
Region
that contains your instance.
Select the managed notebooks instance that you want
to delete.
Click
delete
Delete
.
Delete the project
If you used resources outside of
your managed notebooks instance,
such as the Cloud Storage bucket required
for creating a schedule,
you might want to delete your project to avoid incurring additional charges.
Caution
: Deleting a project has the following effects:
Everything in the project is deleted.
If you used an existing project for
      the tasks in this document, when you delete it, you also delete any other work you've
      done in the project.
Custom project IDs are lost.
When you created this project, you might have created a custom project ID that you want to use in
      the future. To preserve the URLs that use the project ID, such as an
appspot.com
URL, delete selected resources inside the project instead of deleting the whole project.
If you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects
    can help you avoid exceeding project quota limits.
In the Google Cloud console, go to the
Manage resources
page.
Go to Manage resources
In the project list, select the project that you
    want to delete, and then click
Delete
.
In the dialog, type the project ID, and then click
Shut down
to delete the project.
What's next
Vertex AI Workbench managed notebooks instances are deprecated. To
schedule a notebook run in a Vertex AI Workbench instance, see
Schedule a notebook run
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/schedule-managed-notebooks-run.txt
Quickstart: Schedule a managed notebooks run  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Schedule a managed notebooks run
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page shows you how to use
the Vertex AI Workbench managed notebooks executor
to run a Python notebook file on an hourly schedule.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Required roles
To ensure that your instance's service account has the necessary
      permissions to interact with the Vertex AI Workbench executor,
    
      ask your administrator to grant your instance's service account the
    following IAM roles on the project:
Important:
You must grant these roles
      to your instance's service account,
not
to your user account. Failure to grant the roles to the correct principal might result in permission errors.
Notebooks Viewer (
roles/notebooks.viewer
)
Vertex AI User (
roles/aiplatform.user
)
Storage Admin (
roles/storage.admin
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
Your administrator might also be able to give your instance's service account
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create a managed notebooks instance and example notebook file
Create a managed notebooks
instance
.
Open JupyterLab
.
Open a new notebook file
.
In the first cell of the notebook file, enter the following:
# Import datetime
import
datetime
# Get the time and print it
datetime
.
datetime
.
now
()
print
(
datetime
.
datetime
.
now
())
To make sure your notebook file is saved, select
File
>
Save Notebook
.
Schedule a run
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Next to the managed notebooks instance
that you want to use,
click
Open JupyterLab
.
Your managed notebooks instance opens JupyterLab.
In the
folder
File Browser
,
double-click the example notebook file to open it.
Click the
Execute
button.
In the
Submit notebooks to Executor
dialog, in the
Type
field,
select
Schedule-based recurring executions
.
By default, the executor runs your notebook file
every hour at the
00
minute of the hour.
In
Advanced options
,
select the
Region
where you want to run your notebook.
In the
Cloud Storage bucket
field, enter a name for your bucket,
and then click
Create and select
.
The executor stores your notebook output
in the Cloud Storage bucket.
Click
Submit
.
Your notebook file runs automatically
on the schedule that you set.
Note:
If your managed notebooks instance is shut down, the
executor still runs your notebook file on schedule.
When you finish the tasks that are described in this document, you can avoid
   continued billing by deleting the resources that you created. For more information, see
Clean up
.
View, share, and import an executed notebook file
By using your managed notebooks instance's JupyterLab interface,
you can view your notebook output, share the results with others,
and import the executed notebook file into JupyterLab.
Note:
To use the Google Cloud console to view and share execution results,
on the
Executions
page,
click
Executions
.
View the execution results
In JupyterLab's navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Under the execution that you want to view, click
View result
.
Executor opens your result in a new browser tab.
Share the execution results
In your managed notebooks instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Next to the execution that you want to share,
click the
more_vert
options menu,
and select
Share execution result
.
Follow the directions in the dialog
to grant users access to the execution result.
Import the executed notebook into JupyterLab
In your managed notebooks instance's
JupyterLab user interface,
in the navigation menu, click the
Notebook Executor
button.
Click the
Executions
tab.
Next to the execution that you want to import,
click the
more_vert
options menu,
and select
Import executed notebook
.
If the
Select Kernel
dialog appears,
select the kernel that you want to open the notebook.
The executor opens the executed notebook file
in JupyterLab, and stores this notebook file in
the JupyterLab File Browser in a folder named
imported_notebook_jobs
.
View or delete a schedule
You can view and delete schedules by using either the Google Cloud console or
your managed notebooks instance's JupyterLab user interface.
View a schedule
View a schedule to see the frequency settings of the schedule
or to view the five most recent results of the notebook file execution.
Console
In the Google Cloud console, go to the
Schedules
page.
Go to Schedules
Select the
Region
where you want to see schedules.
For the
Schedule details
page that you want to open,
 click its schedule name.
On the
Schedule details
page, you can view the schedule's last
 five executions.
Next to an execution name, click
View result
to open
 the executed notebook file.
Executor opens your result in a new browser tab.
JupyterLab
In your managed notebooks instance's
 JupyterLab user interface,
 in the navigation menu, click the
Notebook Executor
button.
Click the
Schedules
tab.
Under the execution that you want to view, click
View latest
 execution result
.
Executor opens your result in a new browser tab.
Delete a schedule
Deleting a schedule doesn't delete the executions that were
generated from that schedule.
Console
In the Google Cloud console, go to the
Schedules
page.
Go to Schedules
Select the
Region
that contains the schedule
 that you want to delete.
Select the schedule that you want to delete.
Click
delete
Delete
.
JupyterLab
In your managed notebooks instance's
 JupyterLab user interface,
 in the navigation menu, click the
Notebook Executor
button.
Click the
Schedules
tab.
At the end of the schedule name, click the
open_in_new
Open in
 new icon. The
Schedule details
page for that schedule opens in the
 Google Cloud console.
Click
delete
Delete
.
Clean up
To avoid incurring charges to your Google Cloud account for
          the resources used on this page, follow these steps.
Delete the instance
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Select the
Region
that contains your instance.
Select the managed notebooks instance that you want
to delete.
Click
delete
Delete
.
Delete the project
If you used resources outside of
your managed notebooks instance,
such as the Cloud Storage bucket required
for creating a schedule,
you might want to delete your project to avoid incurring additional charges.
Caution
: Deleting a project has the following effects:
Everything in the project is deleted.
If you used an existing project for
      the tasks in this document, when you delete it, you also delete any other work you've
      done in the project.
Custom project IDs are lost.
When you created this project, you might have created a custom project ID that you want to use in
      the future. To preserve the URLs that use the project ID, such as an
appspot.com
URL, delete selected resources inside the project instead of deleting the whole project.
If you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects
    can help you avoid exceeding project quota limits.
In the Google Cloud console, go to the
Manage resources
page.
Go to Manage resources
In the project list, select the project that you
    want to delete, and then click
Delete
.
In the dialog, type the project ID, and then click
Shut down
to delete the project.
What's next
Vertex AI Workbench managed notebooks instances are deprecated. To
schedule a notebook run in a Vertex AI Workbench instance, see
Schedule a notebook run
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/serverless-spark.txt
Use Serverless for Apache Spark with managed notebooks  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Use Serverless for Apache Spark with managed notebooks
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
Preview
This feature is
        
        subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the
Service Specific
        Terms
.
        
        Pre-GA features are available "as is" and might have limited support.
      
      For more information, see the
launch stage descriptions
.
This page shows you how to run a notebook file on serverless Spark
in a Vertex AI Workbench managed notebooks instance
by using
Google Cloud Serverless for Apache Spark
.
Your managed notebooks instance
can submit a notebook file's code to run on
the Serverless for Apache Spark service. The service runs
the code on a managed compute infrastructure that automatically
scales resources as needed. Therefore,
you don't need to provision and manage your own cluster.
Serverless for Apache Spark charges
apply only to the time when the workload is executing.
Requirements
To run a notebook file on Serverless for Apache Spark,
see the following requirements.
Your Serverless for Apache Spark session must run in the same
region as your managed notebooks instance.
The Require OS Login (
constraints/compute.requireOsLogin
) constraint
must not be enabled for your project. See
Manage OS Login in
an organization
.
To run a notebook file on Serverless for Apache Spark,
you must provide a
service account
that has specific permissions. You can grant these permissions
to the default service account or provide a custom service account.
See the
Permissions section of this page
.
Your Serverless for Apache Spark session uses
a Virtual Private Cloud (VPC) network to execute workloads.
The VPC subnetwork must meet specific requirements.
See the requirements in
Google Cloud Serverless for Apache Spark for
Spark network configuration
.
Permissions
To ensure that the service account has the necessary
      permissions to run a notebook file on Serverless for Apache Spark,
    
      ask your administrator to grant the service account the
Dataproc Editor
(
roles/dataproc.editor
)
     IAM role on your project.
Important:
You must grant this role
      to the service account,
not
to your user account. Failure to grant the role to the correct principal might result in permission errors.
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
This predefined role contains
        
        the permissions required to run a notebook file on Serverless for Apache Spark. To see the exact permissions that are
        required, expand the
Required permissions
section:
Required permissions
The following permissions are required to run a notebook file on Serverless for Apache Spark:
dataproc.agents.create
dataproc.agents.delete
dataproc.agents.get
dataproc.agents.update
dataproc.session.create
dataproc.sessions.get
dataproc.sessions.list
dataproc.sessions.terminate
dataproc.sessions.delete
dataproc.tasks.lease
dataproc.tasks.listInvalidatedLeases
dataproc.tasks.reportStatus
Your administrator might also be able to give the service account
          these permissions
        with
custom roles
or
        other
predefined roles
.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks, Vertex AI, and Dataproc APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks, Vertex AI, and Dataproc APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
If you haven't already,
    configure a VPC network that meets the requirements listed in
Google Cloud Serverless for Apache Spark
    network configuration
.
Open JupyterLab
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Next to your managed notebooks instance's name,
click
Open JupyterLab
.
Start a Serverless for Apache Spark session
To start a Serverless for Apache Spark session,
complete the following steps.
In your managed notebooks instance's JupyterLab interface,
select the
Launcher
tab, and then select
Serverless Spark
.
If the
Launcher
tab is not open,
select
File
>
New Launcher
to open it.
The
Create Serverless Spark session
dialog appears.
In the
Session name
field, enter a name for your session.
In the
Execution configuration
section, enter
the
Service account
that you want to use. If you don't enter
a service account, your session will use the
Compute Engine default
service account
.
In the
Network configuration
section, select the
Network
and
Subnetwork
of a network that meets the requirements
listed in
Google Cloud Serverless for Apache Spark
network configuration
.
Click
Create
.
A new notebook file opens.
The Serverless for Apache Spark session that you created is
the kernel that runs your notebook file's code.
Run your code on Serverless for Apache Spark and other kernels
Add code to your new notebook file, and run the code.
To run code on a different kernel,
change the kernel
.
When you want to run the code on
your Serverless for Apache Spark session again,
change the kernel back to
the Serverless for Apache Spark kernel.
Terminate your Serverless for Apache Spark session
You can terminate a Serverless for Apache Spark session
in the JupyterLab interface or in the Google Cloud console.
The code in your notebook file is preserved.
JupyterLab
In JupyterLab, close the notebook file that was created when you
created your Serverless for Apache Spark session.
In the dialog that appears, click
Terminate session
.
Google Cloud console
In the Google Cloud console, go to the
Dataproc sessions
page.
Go to Dataproc sessions
Select the session that you want to terminate,
and then click
Terminate
.
Delete your Serverless for Apache Spark session
You can delete a Serverless for Apache Spark session
by using the Google Cloud console.
The code in your notebook file is preserved.
In the Google Cloud console, go to the
Dataproc sessions
page.
Go to Dataproc sessions
Select the session that you want to delete,
and then click
Delete
.
What's next
Learn more about
Serverless for Apache Spark
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/service-perimeter.txt
Use a managed notebooks instance within a service perimeter  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Use a managed notebooks instance within a service perimeter
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page describes how to use VPC Service Controls to set up
a managed notebooks instance within a service perimeter.
Before you begin
Read the
Overview of
VPC Service Controls
.
Create a VPC network
or use your project's
default VPC network
.
Create and configure the service perimeter
To create and configure the service perimeter, do the following:
Create a service perimeter using
VPC Service Controls
.
This service perimeter protects the Google-managed resources of services
that you specify. While creating your service perimeter, do the following:
When it's time to add projects to your service perimeter, add the
project that contains your managed notebooks instance.
When it's time to add services to your service perimeter, add the
Notebooks API
.
If you have created your service perimeter without adding the
projects and services you need, see
Managing service
perimeters
to learn how to update your service perimeter.
Configure your DNS entries using Cloud DNS
Vertex AI Workbench managed notebooks instances use several domains that a
  Virtual Private Cloud network doesn't handle by default.
  To ensure that your VPC network correctly handles requests sent
  to those domains, use Cloud DNS to add DNS records. For more
  information about VPC routes, see
Routes
.
To create a
managed zone
for
  a domain, add a DNS entry that will route the request, and execute
  the transaction, complete the following steps.
  Repeat these steps for each of
several
  domains
that you need to handle requests for, starting
  with
*.notebooks.googleapis.com
.
In
Cloud Shell
or any environment where the
Google Cloud CLI
is installed, enter the following
Google Cloud CLI
commands.
To create a private managed zone
      for one of the domains that your
      VPC network needs to handle:
gcloud
dns
managed-zones
create
ZONE_NAME
\
--visibility
=
private
\
--networks
=
https://www.googleapis.com/compute/v1/projects/
PROJECT_ID
/global/networks/
NETWORK_NAME
\
--dns-name
=
DNS_NAME
\
--description
=
"Description of your managed zone"
Replace the following:
ZONE_NAME
: a name for the zone to create.
        You must use a separate zone for each domain. This zone name is used in
        each of the following steps.
PROJECT_ID
: the ID of the project that hosts your
        VPC network
NETWORK_NAME
: the name of the VPC
        network that you created earlier
DNS_NAME
: the part of the domain name that comes
        after the
*.
, with a period on the end.
        For example,
*.notebooks.googleapis.com
has a
DNS_NAME
of
notebooks.googleapis.com.
Start a transaction.
gcloud
dns
record-sets
transaction
start
--zone
=
ZONE_NAME
Add the following DNS A record. This reroutes traffic to
      Google's restricted IP addresses.
gcloud
dns
record-sets
transaction
add
\
--name
=
DNS_NAME
.
\
--type
=
A
199
.36.153.4
199
.36.153.5
199
.36.153.6
199
.36.153.7
\
--zone
=
ZONE_NAME
\
--ttl
=
300
Add the following DNS CNAME record to point to the A record
      that you just added. This redirects all traffic matching the
      domain to the IP addresses listed in the previous step.
gcloud
dns
record-sets
transaction
add
\
--name
=
\*
.
DNS_NAME
.
\
--type
=
CNAME
DNS_NAME
.
\
--zone
=
ZONE_NAME
\
--ttl
=
300
Execute the transaction.
gcloud
dns
record-sets
transaction
execute
--zone
=
ZONE_NAME
Repeat these steps for each of the following domains. For each
      repetition, change
ZONE_NAME
and
DNS_NAME
to the appropriate values for that
      domain. Keep
PROJECT_ID
and
NETWORK_NAME
the same each time. You already
      completed these steps for
*.notebooks.googleapis.com
.
*.notebooks.googleapis.com
*.notebooks.cloud.google.com
*.notebooks.googleusercontent.com
*.googleapis.com
to run code that interacts with other Google APIs
        and services
Use Artifact Registry within your service perimeter
If you want to use Artifact Registry in your service perimeter,
see
Configure restricted access for GKE
private clusters
.
Use Shared VPC
If you are using
Shared VPC
,
you must add the host and the service projects to the service
perimeter. In the host project, you must also grant the
Compute Network User role
(
roles/compute.networkUser
) to the
Notebooks Service
Agent
from the service project. For more information, see
Managing
service perimeters
.
Access your managed notebooks instance
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Next to your managed notebooks instance's name,
click
Open JupyterLab
.
If it's the first time you have accessed the
managed notebooks instance's JupyterLab user interface,
you must grant permission to access your data and authenticate
your managed notebooks instance.
In the
Authenticate your managed notebook
dialog, click the button
to get an authentication code.
Choose an account and click
Allow
. Copy the authentication code.
In the
Authenticate your managed notebook
dialog,
paste the authentication code, and then click
Authenticate
.
Your managed notebooks instance opens JupyterLab.
Limitations
Identity type for ingress and egress policies
When you specify an ingress or egress policy for a service perimeter,
you can't use
ANY_SERVICE_ACCOUNT
or
ANY_USER_ACCOUNT
as an identity type for
all
Vertex AI Workbench
operations.
Instead, use
ANY_IDENTITY
as the identity type.
Accessing the managed notebooks proxy from a workstation without internet
To access managed notebooks instances
from a workstation with limited internet access,
verify with your IT administrator that you can access the following domains:
*.accounts.google.com
*.accounts.youtube.com
*.googleusercontent.com
*.kernels.googleusercontent.com
*.gstatic.com
*.notebooks.cloud.google.com
*.notebooks.googleapis.com
You must have access to these domains for authentication to
Google Cloud. See the previous section,
Configure your DNS entries using Cloud DNS
,
for further configuration information.
What's next
Learn more about
VPC Service Controls
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/upgrade.txt
Upgrade the environment of a Vertex AI Workbench managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Upgrade the environment of a managed notebooks instance
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
Vertex AI Workbench managed notebooks instances are
Google-managed environments with integrations and
features that help you set up and work in
an end-to-end notebook-based production environment.
This page describes how to upgrade the environment of
a managed notebooks instance.
Upgrade methods
There is one way to upgrade a managed notebooks instance:
Manual upgrade
: If
an existing managed notebooks instance
meets the
requirements
for upgrading, you can upgrade the instance manually.
Requirements and limitations
Backward compatibility with your managed notebooks isn't
guaranteed.
Make a copy of
your data
before upgrading a managed notebooks instance.
To determine whether you can upgrade
a specific managed notebooks instance,
see the following requirements and limitations:
The Notebooks API must be
enabled in the instance's Google Cloud project
. For more information, see
List enabled services
and
Enable an API
.
After upgrading, an instance cannot be rolled back to the previous version.
Instances that are version M112 or earlier use Python 3.7, and therefore
can't be upgraded to a more recent version than M112. This is because M112
is the latest version to support Python 3.7.
How the upgrade works
Managed notebooks instances are dual-disk, with
one boot disk and one data disk. The upgrade process upgrades the boot disk
to a new image while preserving your data on the data disk.
Which components are upgraded
The following table shows which components of
your managed notebooks
instance are upgraded, which are preserved,
and which need to be reinstalled.
Component
Upgrade result
Machine learning frameworks
Upgraded
Machine learning data
Preserved
Preinstalled Python dependencies
Upgraded
User-installed Python packages
Must be reinstalled
Preinstalled operating system packages
Upgraded
GPU drivers
Upgraded
Notebooks
Preserved
User configurations
Preserved
Before you begin
Before you upgrade, complete the following steps.
Check the
release notes
to learn about
updates to newer versions.
Make a copy of your data
as a backup.
Check for a newer version of your instance's environment
To check whether a newer version of your instance's environment is available,
access your instance from the Google Cloud console.
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
On the
Managed notebooks
tab, "Upgrade available" will appear in
each upgradeable instance's row.
There will also be a message at the top saying
how many notebooks have available upgrades.
Upgrade your instance's environment to a newer version
You can manually upgrade a managed notebooks instance in
the Google Cloud console.
Note:
Upgrades can affect user operations that are running.
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
If the instances that you want to upgrade aren't running,
start the
instances
.
Vertex AI Workbench can only upgrade instances when
they're running.
If you want to upgrade all upgradeable
managed notebooks instances,
then on the
Managed notebooks
tab, in the message at the top
of the page, click the
Upgrade all
button. If there
is no message at the top, then there are no
managed notebooks instances that can be upgraded.
If you want to upgrade an individual notebook,
then on the
Managed notebooks
tab, select
the managed notebooks instance that
you want to upgrade. There will be an "Upgrade available" message
in the instance's row.
Make sure you have
made a copy of the data on your
instance
before
continuing.
On the
Managed notebooks
tab, click
Upgrade
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/versions.txt
Managed notebooks versions  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Managed notebooks versions
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page describes how Vertex AI Workbench managed notebooks versions
are named and how frequently they are updated.
Version names
Managed notebooks version names
match the naming convention of
Deep Learning VM Images versions
.
Both are named with the letter M followed by a version number.
For example, the M93 versions of managed notebooks
and Deep Learning VM were released on May 27, 2022.
Managed notebooks and Deep Learning VM
releases that have the same version number also have matching
versions of frameworks and pre-installed packages.
For example, the M93 version of managed notebooks
contains a TensorFlow kernel that runs TensorFlow 2.9,
and the M93 version of Deep Learning VM
includes an image with a TensorFlow 2.9 environment.
Update frequency
Managed notebooks versions are not necessarily updated
at the same time as Deep Learning VM.
For example, the M91 version of managed notebooks
was released after the M91 version of Deep Learning VM.
Managed notebooks versions are not necessarily updated
as frequently as Deep Learning VM.
As a result, managed notebooks versions can skip
version numbers. For example,
Deep Learning VM released versions M91, M92, and M93,
while managed notebooks released versions M91 and M93.
What's next
Browse the
Vertex AI release notes
,
which include the release notes
for Vertex AI Workbench managed notebooks.
Browse the
Deep Learning VM
release notes
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/visualize-data-bigquery.txt
Explore and visualize data in BigQuery from within JupyterLab  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Explore and visualize data in BigQuery from within JupyterLab
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page shows you some examples of how to explore and visualize data
that is stored in BigQuery from within the JupyterLab interface
of your Vertex AI Workbench managed notebooks instance.
Open JupyterLab
In the Google Cloud console, go to the
Managed notebooks
page.
Go to Managed notebooks
Next to your managed notebooks instance's name,
click
Open JupyterLab
.
Your managed notebooks instance opens JupyterLab.
Read data from BigQuery
In the next two sections, you read data from BigQuery
that you will use to visualize later. These steps are identical to those
in
Query data in BigQuery from
within JupyterLab
, so if you've completed
them already, you can skip to
Get a summary of data in a BigQuery table
.
Query data by using the %%bigquery magic command
In this section, you write SQL directly in notebook cells and read data from
BigQuery into the Python notebook.
Magic commands that use a single or double percentage character (
%
or
%%
)
let you use minimal syntax to interact with BigQuery within the
notebook. The BigQuery client library for Python is automatically
installed in a managed notebooks instance. Behind the scenes, the
%%bigquery
magic
command uses the BigQuery client library for Python to run the
given query, convert the results to a pandas DataFrame, optionally save the
results to a variable, and then display the results.
Note
: As of version 1.26.0 of the
google-cloud-bigquery
Python package,
the
BigQuery Storage API
is used by default to download results from the
%%bigquery
magics.
To open a notebook file, select
File
>
New
>
Notebook
.
In the
Select Kernel
dialog, select
Python (Local)
, and then click
Select
.
Your new IPYNB file opens.
To get the number of regions by country in the
international_top_terms
dataset, enter the following statement:
%%
bigquery
SELECT
country_code
,
country_name
,
COUNT
(
DISTINCT
region_code
)
AS
num_regions
FROM
`
bigquery
-
public
-
data
.
google_trends
.
international_top_terms
`
WHERE
refresh_date
=
DATE_SUB
(
CURRENT_DATE
,
INTERVAL
1
DAY
)
GROUP
BY
country_code
,
country_name
ORDER
BY
num_regions
DESC
;
Click
play_circle_filled
Run cell
.
The output is similar to the following:
Query complete after 0.07s: 100%|██████████| 4/4 [00:00<00:00, 1440.60query/s]
Downloading: 100%|██████████| 41/41 [00:02<00:00, 20.21rows/s]
country_code      country_name    num_regions
0   TR  Turkey         81
1   TH  Thailand       77
2   VN  Vietnam        63
3   JP  Japan          47
4   RO  Romania        42
5   NG  Nigeria        37
6   IN  India          36
7   ID  Indonesia      34
8   CO  Colombia       33
9   MX  Mexico         32
10  BR  Brazil         27
11  EG  Egypt          27
12  UA  Ukraine        27
13  CH  Switzerland    26
14  AR  Argentina      24
15  FR  France         22
16  SE  Sweden         21
17  HU  Hungary        20
18  IT  Italy          20
19  PT  Portugal       20
20  NO  Norway         19
21  FI  Finland        18
22  NZ  New Zealand    17
23  PH  Philippines    17
...
Note:
Your results might differ from what is above as the
google_trends
dataset being queried is refreshed with new data on an ongoing basis.
In the next cell (below the output from the previous cell), enter the
following command to run the same query, but this time save the results to
a new pandas DataFrame that's named
regions_by_country
. You provide that
name by using an argument with the
%%bigquery
magic command.
%%
bigquery
regions_by_country
SELECT
country_code
,
country_name
,
COUNT
(
DISTINCT
region_code
)
AS
num_regions
FROM
`
bigquery
-
public
-
data
.
google_trends
.
international_top_terms
`
WHERE
refresh_date
=
DATE_SUB
(
CURRENT_DATE
,
INTERVAL
1
DAY
)
GROUP
BY
country_code
,
country_name
ORDER
BY
num_regions
DESC
;
Note:
For more information about available arguments for the
%%bigquery
command, see the
client library magics documentation
.
Click
play_circle_filled
Run cell
.
In the next cell, enter the following command to look at the first few
rows of the query results that you just read in:
regions_by_country
.
head
()
Click
play_circle_filled
Run cell
.
The pandas DataFrame
regions_by_country
is ready to plot.
Query data by using the BigQuery client library directly
<
Get a summary of data in a BigQuery table
In this section, you use a notebook shortcut to get summary statistics and
visualizations for all fields of a BigQuery table. This can
be a fast way to profile your data before exploring further.
The BigQuery client library provides a magic command,
%bigquery_stats
, that you can call with a specific table name to provide an
overview of the table and detailed statistics on each of the table's
columns.
In the next cell, enter the following code to run that analysis on the US
top_terms
table
:
%bigquery_stats
bigquery
-
public
-
data
.
google_trends
.
top_terms
Click
play_circle_filled
Run cell
.
After running for some time, an image appears with various statistics on
each of the 7 variables in the
top_terms
table. The following image shows
part of some example output:
Note:
Your results might differ from what is above as the
google_trends
dataset being queried is refreshed with new data on an ongoing basis.
Visualize BigQuery data
In this section, you use plotting capabilities to visualize the results from
the queries that you previously ran in your Jupyter notebook.
In the next cell, enter the following code to use the pandas
DataFrame.plot()
method to create a bar chart that visualizes the results
of the query that returns the number of regions by country:
regions_by_country
.
plot
(
kind
=
"bar"
,
x
=
"country_name"
,
y
=
"num_regions"
,
figsize
=
(
15
,
10
))
Click
play_circle_filled
Run cell
.
The chart is similar to the following:
In the next cell, enter the following code to use the pandas
DataFrame.plot()
method to create a scatter plot that visualizes the
results from the query for the percentage of overlap in the top search terms
by days apart:
pct_overlap_terms_by_days_apart.plot(
  kind="scatter",
  x="days_apart",
  y="pct_overlap_terms",
  s=len(pct_overlap_terms_by_days_apart["num_date_pairs"]) * 20,
  figsize=(15, 10)
  )
Click
play_circle_filled
Run cell
.
The chart is similar to the following. The size of each point reflects
the number of date pairs that are that many days apart in the data. For
example, there are more pairs that are 1 day apart than 30 days apart
because the top search terms are surfaced daily over about a month's time.
For more information about data visualization, see the
pandas documentation
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/managed/visualize-data.txt
Explore and visualize data in Vertex AI Workbench managed notebooks  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Explore and visualize data: Overview
Vertex AI Workbench managed notebooks is
deprecated
. On
    April 14, 2025, support for
    managed notebooks ended and the ability to create managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your managed notebooks instances to Vertex AI Workbench instances
.
This page identifies common ways to explore and visualize data
in Vertex AI Workbench managed notebooks. You can use
pre-installed Python packages and R libraries that are commonly used
for data visualization. To explore and visualize BigQuery data,
you can use the BigQuery client library with
packages that visualize data.
Common Python packages and R libraries
By default, managed notebooks instances are pre-installed
with common Python packages and R libraries for data visualization, for example
matplotlib
,
seaborn
,
and
ggplot2
. Import or load these packages
and libraries into your notebook file and they are ready to use.
BigQuery data
You can use common Python packages to visualize
BigQuery data. The BigQuery client library
provides additional methods, and by default the
BigQuery client library is pre-installed in
managed notebooks instances.
For an example of how to use the BigQuery client library
with Python plotting capabilities, see
Explore and visualize data in
BigQuery tables
.
What's next
Explore and visualize data in
BigQuery tables.
To learn more about BigQuery, see
What is BigQuery?
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/notebook-solution.txt
Choose a notebook solution  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Choose a notebook solution
This page describes the differences between Vertex AI's notebook
environment options so that you can choose the best one for your project.
Vertex AI provides two notebook environment solutions:
Colab Enterprise:
A collaborative,
managed notebook environment with the security and compliance capabilities
of Google Cloud. If your project's priorities are to collaborate
with others and to avoid spending time managing infrastructure,
Colab Enterprise might be the best option for you.
See the following
Colab Enterprise
section.
Vertex AI Workbench:
A Jupyter notebook-based environment
provided through virtual machine (VM) instances with features that support
the entire data science workflow. If your project's priorities are control
and customizability, Vertex AI Workbench might be the best option
for you. See the following
Vertex AI Workbench
section.
Colab Enterprise
Learn about a few of Colab Enterprise's strengths in the
sections that follow. For more information, see
Introduction to
Colab Enterprise
.
Share and collaborate
Colab Enterprise lets you share notebooks and collaborate
with others. You can share a notebook with a single user, Google group,
or Google Workspace domain. You control this access
through Identity and Access Management (IAM).
Managed compute
Colab Enterprise lets you work in notebooks without having
to manage infrastructure. Colab Enterprise provisions
a runtime for you when you need it. If you want to, you can configure
runtimes for specific needs, but Colab Enterprise starts
them for you and shuts them down when you no longer need them.
Integrated into the Google Cloud console
Colab Enterprise's integrations with Google Cloud services
make it easier to use notebooks that interact with those services.
You can use Colab Enterprise from within the Google Cloud console,
with features built into both Vertex AI and
BigQuery.
Write code with Gemini assistance
Preview
This feature is
        
        subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the
Service Specific
        Terms
.
        
        Pre-GA features are available "as is" and might have limited support.
      
      For more information, see the
launch stage descriptions
.
You can use Gemini in Vertex AI, which is a product in
the
Gemini for Google Cloud
portfolio,
to help you write and generate code in a Vertex AI notebook.
Gemini in Vertex AI can generate code completion
suggestions while you type in a code cell. You can also use the
Help me code
tool to generate code based on a description
of what you want. To learn more, see
Write code
with Gemini assistance
.
Vertex AI Workbench
Learn about a few of Vertex AI Workbench's strengths in the
sections that follow. For more information, see
Introduction to
Vertex AI Workbench
.
Overview
All Vertex AI Workbench instances provide the following:
Prepackaged with
JupyterLab
.
A preinstalled suite of deep learning packages, including support for
the TensorFlow and PyTorch frameworks.
Support for GPU accelerators.
The ability to sync with a
GitHub
repository.
Google Cloud authentication and authorization.
Add conda environments
Vertex AI Workbench instances use
kernels
based on conda environments.
You can add a conda environment to your Vertex AI Workbench instance,
and the environment appears as a kernel in your instance's JupyterLab interface.
Adding conda environments lets you use kernels that aren't available in the
default Vertex AI Workbench instance.
For example, you can add conda environments for R and Apache Beam. Or you
can add conda environments for specific earlier versions of the available
frameworks, such as TensorFlow, PyTorch, or Python.
For more information, see
Add a conda environment
.
Access to data
You can work more efficiently by accessing your data without leaving
the JupyterLab interface.
From within JupyterLab's navigation menu on
a Vertex AI Workbench instance, you can
use the
Cloud Storage integration
to browse data and other files that you have access to.
Also from within the navigation menu, you can
use the
BigQuery integration
to browse tables that you have access to,
write queries, preview results, and load data into your notebook.
Automated notebook runs
You can
set a notebook to run on a recurring
schedule
.
Even while your instance is shut down, Vertex AI Workbench will
run your notebook file and save the results
for you to look at and share with others.
Automated shutdown for idle instances
To help manage costs, you can set
your Vertex AI Workbench instance
to shut down after being idle for a specific time period.
For more information,
see
Idle shutdown
.
Custom containers
You can create a Vertex AI Workbench instance based on a custom container.
Start with a Google-provided base container image, and modify it for
your needs. Then create an instance based on your custom container.
For more information, see
Create an instance using a
custom container
.
Use third party credentials
You can create and manage Vertex AI Workbench instances with
third party credentials provided by Workforce Identity Federation.
Workforce Identity Federation uses your external identity provider (IdP)
to grant a group of users access to Vertex AI Workbench instances
through a proxy.
For more information, see
Create an instance with
third party credentials
.
Health status monitoring
To help ensure that your Vertex AI Workbench instance
is working properly, you can
monitor the health
status
.
Editable Deep Learning VM instances
Vertex AI Workbench provides API methods for modifying the
underlying VM through the Notebooks API.
Note:
You can't edit the underlying VM of an instance by using
      the Google Cloud console or the Compute Engine API.
What's next
To get started:
Create a Colab Enterprise
notebook
.
Create a Vertex AI Workbench instance
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/notebooks.txt
Vertex AI Workbench notebook tutorials  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Vertex AI Workbench notebook tutorials
This document contains a list of available Vertex AI Workbench
  notebook tutorials. These end-to-end tutorials help you get started
  using Vertex AI Workbench and can give you ideas for how to
  implement a specific project.
These tutorials highlight specific Vertex AI Workbench capabilities
  but you can also use them in other environments, such as
Colab Enterprise
and
Colaboratory
.
List of notebooks
Select a service
AutoML
BigQuery
BigQuery ML
Custom training
Image
Ray on Vertex AI
Tabular
Text
Vector Search
Vertex AI Experiments
Vertex AI Feature Store
Vertex AI Inference
Vertex AI model evaluation
Vertex AI Model Monitoring
Vertex AI Model Registry
Vertex AI Pipelines
Vertex AI TensorBoard
Vertex AI Vizier
Vertex AI Workbench
Vertex Explainable AI
Vertex ML Metadata
Services
Description
Open in
Classification for tabular data
AutoML tabular training and prediction
.
Learn how to train and make predictions on an AutoML model based on a tabular dataset.


 Learn more about
Classification for tabular data
.
Tutorial steps
Create a Vertex AI model training job.
Train an AutoML Tabular model.
Deploy the model resource to a serving endpoint resource.
Make a prediction by sending data.
Undeploy the model resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Get predictions from an image classification model
AutoML training image classification model for batch prediction
.
In this tutorial, you create an AutoML image classification model from a Python script, and then do a batch prediction using the Vertex SDK.


 Learn more about
Get predictions from an image classification model
.
Tutorial steps
Create a Vertex dataset resource.
Train the model.
View the model evaluation.
Make a batch prediction.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Get predictions from an image classification model
AutoML training image classification model for online prediction
.
In this tutorial, you create an AutoML image classification model and deploy for online prediction from a Python script using the Vertex SDK.


 Learn more about
Get predictions from an image classification model
.
Tutorial steps
Create a Vertex
Dataset
resource.
Train the model.
View the model evaluation.
Deploy the
Model
resource to a serving
Endpoint
resource.
Make a prediction.
Undeploy the
Model
.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
AutoML
AutoML training image object detection model for export to edge
.
In this tutorial, you create an AutoML image object detection model from a Python script using the Vertex SDK, and then export the model as an Edge model in TFLite format.
Tutorial steps
Create a Vertex dataset resource.
Train the model.
Export the edge model from the model resource to Cloud Storage.
Download the model locally.
Make a local prediction.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Object detection for image data
AutoML training image object detection model for online prediction
.
In this tutorial, you create an AutoML image object detection model and deploy for online prediction from a Python script using the Vertex AI SDK.


 Learn more about
Object detection for image data
.
Tutorial steps
Create a Vertex AI dataset resource.
Train the model.
View the model evaluation.
Deploy the model resource to a serving endpoint resource.
Make a prediction.
Undeploy the model.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Tabular Workflow for E2E AutoML
AutoML Tabular Workflow pipelines
.
Learn how to create two regression models using Vertex AI Pipelines downloaded from Google Cloud Pipeline Components .


 Learn more about
Tabular Workflow for E2E AutoML
.
Tutorial steps
Create a training pipeline that reduces the search space from the default to save time.
Create a training pipeline that reuses the architecture search results from the previous pipeline to save time.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
AutoML training
Get started with AutoML training
.
Learn how to use
AutoML
for training with
Vertex AI
.


 Learn more about
AutoML training
.
Tutorial steps
Train an image model
Export the image model as an edge model
Train a tabular model
Export the tabular model as a cloud model
Train a text model
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Hierarchical forecasting for tabular data
Vertex AI AutoML training hierarchical forecasting for batch prediction
.
In this tutorial, you create an AutoML hierarchical forecasting model and deploy it for batch prediction using the Vertex AI SDK for Python.


 Learn more about
Hierarchical forecasting for tabular data
.
Tutorial steps
Create a Vertex AI TimeSeriesDataset resource.
Train the model.
View the model evaluation.
Make a batch prediction.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Object detection for image data
AutoML training image object detection model for batch prediction
.
In this tutorial, you create an AutoML image object detection model from a Python script, and then do a batch prediction using the Vertex AI SDK for Python.


 Learn more about
Object detection for image data
.
Tutorial steps
Create a Vertex dataset resource.
Train the model.
View the model evaluation.
Make a batch prediction.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Forecasting with AutoML
AutoML tabular forecasting model for batch prediction
.
Learn how to create an AutoML tabular forecasting model from a Python script, and then generate batch prediction using the Vertex AI SDK.


 Learn more about
Forecasting with AutoML
.
Tutorial steps
Create a Vertex AI dataset resource.
Train an AutoML tabular forecasting model resource.
Obtain the evaluation metrics for the model resource.
Make a batch prediction.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Regression for tabular data
AutoML training tabular regression model for batch prediction using BigQuery
.
Learn how to create an AutoML tabular regression model and deploy it for batch prediction using the Vertex AI SDK for Python.


 Learn more about
Regression for tabular data
.
Tutorial steps
Create a Vertex AI dataset resource.
Train an AutoML tabular regression model resource.
Obtain the evaluation metrics for the model resource.
Make a batch prediction.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Regression for tabular data
AutoML training tabular regression model for online prediction using BigQuery
.
Learn how to create an AutoML tabular regression model and deploy for online prediction from a Python script using the Vertex AI SDK.


 Learn more about
Regression for tabular data
.
Tutorial steps
Create a Vertex dataset resource.
Train the model.
View the model evaluation.
Deploy the model resource to a serving Endpoint resource.
Make a prediction.
Undeploy the model.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
BigQuery ML
Get started with BigQuery ML Training
.
Learn how to use BigQuery ML for training with Vertex AI.


 Learn more about
BigQuery ML
.
Tutorial steps
Create a local BigQuery table in your project
Train a BigQuery ML model
Evaluate the BigQuery ML model
Export the BigQuery ML model as a cloud model
Upload the exported model as a Vertex AI model resource
Hyperparameter tune a BigQuery ML model with Vertex AI Vizier
Automatically register a BigQuery ML model to Vertex AI Model Registry
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Custom training
Vertex AI Inference
Deploying Iris-detection model using FastAPI and Vertex AI custom container serving
.
Learn how to create, deploy and serve a custom classification model on Vertex AI.


 Learn more about
Custom training
.

 Learn more about
Vertex AI Inference
.
Tutorial steps
Train a model that uses flower's measurements as input to predict the class of iris.
Save the model and its serialized preprocessor.
Build a FastAPI server to handle predictions and health checks.
Build a custom container with model artifacts.
Upload and deploy custom container to Vertex AI Endpoints.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Training
Training a TensorFlow model on BigQuery data
.
Learn how to create a custom-trained model from a Python script in a Docker container using the Vertex AI SDK for Python, and then get a prediction from the deployed model by sending data.


 Learn more about
Vertex AI Training
.
Tutorial steps
Create a Vertex AI custom
TrainingPipeline
for training a model.
Train a TensorFlow model.
Deploy the
Model
resource to a serving
Endpoint
resource.
Make a prediction.
Undeploy the
Model
resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Custom training
Custom training with custom container image and automatic model upload to Vertex AI Model Registry
.
In this tutorial, you train a machine learning model custom container image approach for custom training in Vertex AI.


 Learn more about
Custom training
.
Tutorial steps
Create a Vertex AI custom job for training a model.
Train and register a TensorFlow model using a custom container.
List the registered model in the Vertex AI Model Registry.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Cloud Profiler
Profile model training performance using Cloud Profiler
.
Learn how to enable Cloud Profiler for custom training jobs.


 Learn more about
Cloud Profiler
.
Tutorial steps
Setup a service account and a Cloud Storage bucket
Create a Vertex AI TensorBoard instance
Create and run a custom training job
View the Cloud Profiler dashboard
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Custom training
Get started with Vertex AI Training for XGBoost
.
Learn how to use Vertex AI Training for training a XGBoost custom model.


 Learn more about
Custom training
.
Tutorial steps
Training using a Python package.
Report accuracy when hyperparameter tuning.
Save the model artifacts to Cloud Storage using Cloud StorageFuse.
Create a Vertex AI model resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Shared resources across deployments
Get started with Endpoint and shared VM
.
Learn how to use deployment resource pools for deploying models.


 Learn more about
Shared resources across deployments
.
Tutorial steps
Upload a pretrained image classification model as a
Model
resource (model A).
Upload a pretrained text sentence encoder model as a
Model
resource (model B).
Create a shared VM deployment resource pool.
List shared VM deployment resource pools.
Create two
Endpoint
resources.
Deploy first model (model A) to first
Endpoint
resource using deployment resource pool.
Deploy second model (model B) to second
Endpoint
resource using deployment resource pool.
Make a prediction request with first deployed model (model A).
Make a prediction request with second deployed model (model B).
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Custom training
Vertex AI Batch Prediction
Custom training and batch prediction
.
Learn to use Vertex AI Training to create a custom trained model and use Vertex AI Batch Prediction to do a batch prediction on the trained model.


 Learn more about
Custom training
.

 Learn more about
Vertex AI Batch Prediction
.
Tutorial steps
Create a Vertex AI custom job for training a TensorFlow model.
Upload the trained model artifacts as a model resource.
Make a batch prediction.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Custom training
Vertex AI Inference
Custom training and online prediction
.
Learn to use
Vertex AI Training
to create a custom-trained model from a Python script in a Docker container, and learn to use
Vertex AI Inference
to do a prediction on the deployed model by sending data.


 Learn more about
Custom training
.

 Learn more about
Vertex AI Inference
.
Tutorial steps
Create a
Vertex AI
custom job for training a TensorFlow model.
Upload the trained model artifacts to a
Model
resource.
Create a serving
Endpoint
resource.
Deploy the
Model
resource to a serving
Endpoint
resource.
Make a prediction.
Undeploy the
Model
resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
BigQuery datasets
Vertex AI for BigQuery users
Get started with BigQuery datasets
.
Learn how to use BigQuery as a dataset for training with Vertex AI.


 Learn more about
BigQuery datasets
.

 Learn more about
Vertex AI for BigQuery users
.
Tutorial steps
Create a Vertex AI dataset resource from BigQuery table  compatible for AutoML training.
Extract a copy of the dataset from BigQuery to a CSV file in Cloud Storage  compatible for AutoML or custom training.
Select rows from a BigQuery dataset into a pandas dataframe  compatible for custom training.
Select rows from a BigQuery dataset into a
tf.data.Dataset
compatible for custom training TensorFlow models.
Select rows from extracted CSV files into a
tf.data.Dataset
compatible for custom training TensorFlow models.
Create a BigQuery dataset from CSV files.
Extract data from BigQuery table into a DMatrix  compatible for custom training XGBoost models.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Experiments
Vertex ML Metadata
Build Vertex AI Experiment lineage for custom training
.
Learn how to integrate preprocessing code in a Vertex AI experiments.


 Learn more about
Vertex AI Experiments
.

 Learn more about
Vertex ML Metadata
.
Tutorial steps
Execute module for preprocessing data
Create a dataset artifact
Log parameters
Execute module for training the model
Log parameters
Create model artifact
Assign tracking lineage to dataset, model and parameters
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Experiments
Track parameters and metrics for locally trained models
.
Learn how to use Vertex AI Experiments to compare and evaluate model experiments.


 Learn more about
Vertex AI Experiments
.
Tutorial steps
log the model parameters
log the loss and metrics on every epoch to Vertex AI TensorBoard
log the evaluation metrics
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Experiments
Vertex AI Pipelines
Compare pipeline runs with Vertex AI Experiments
.
Learn how to use Vertex AI Experiments to log a pipeline job and then compare different pipeline jobs.


 Learn more about
Vertex AI Experiments
.

 Learn more about
Vertex AI Pipelines
.
Tutorial steps
Formalize a training component
Build a training pipeline
Run several pipeline jobs and log their results
Compare different pipeline jobs
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI TensorBoard
Delete Outdated Experiments in Vertex AI TensorBoard
.
Learn how to delete outdated Vertex AI TensorBoard Experiments to avoid unnecessary storage costs.


 Learn more about
Vertex AI TensorBoard
.
Tutorial steps
How to delete the TB Experiment with a predefined keyvalue label pair
How to delete the TB Experiments created before the
create_time
How to delete the TB Experiments created before the
update_time
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Experiments
Custom training autologging - Local script
.
Learn how to autolog parameters and metrics of an ML experiment running on Vertex AI Training by leveraging the integration with Vertex AI Experiments.
Tutorial steps
Formalize model experiment in a script
Run model traning using local script on Vertex AI Training
Check out ML experiment parameters and metrics in Vertex AI Experiments
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Experiments
Vertex ML Metadata
Custom training
Get started with Vertex AI Experiments
.
Learn how to use Vertex AI Experiments when training with Vertex AI.


 Learn more about
Vertex AI Experiments
.

 Learn more about
Vertex ML Metadata
.

 Learn more about
Custom training
.
Tutorial steps
Local (notebook) training
Create an experiment.
Create a first run in the experiment.
Log parameters and metrics.
Create artifact lineage.
Visualize the experiment results.
Execute a second run.
Compare the two runs in the experiment.
Cloud (Vertex AI) training
Within the training script
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Experiments
Autologging
.
Learn how to use Vertex AI Autologging.
Tutorial steps
Enable autologging in the Vertex AI SDK.
Train scikitlearn model and see the resulting experiment run with metrics and parameters autologged to Vertex AI Experiments without setting an experiment run.
Train Tensorflow model, check autologged metrics and parameters to Vertex AI Experiments by manually setting an experiment run with
aiplatform.start_run()
and
aiplatform.end_run()
.
Disable autologging in the Vertex AI SDK, train a PyTorch model and check that none of the parameters or metrics are logged.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Classification for tabular data
Vertex Explainable AI
Batch explanation for AutoML tabular binary classification model
.
Learn to use
AutoML
to create a tabular binary classification model from a Python script, and then learn to use
Vertex AI Batch Prediction
to make predictions with explanations.


 Learn more about
Classification for tabular data
.

 Learn more about
Vertex Explainable AI
.
Tutorial steps
Create a Vertex AI managed dataset resource.
Train an AutoML tabular binary classification model.
View the model evaluation metrics for the trained model.
Make a batch prediction request with explainability.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Classification for tabular data
Vertex Explainable AI
AutoML training tabular classification model for online explanation
.
Learn how to use AutoML to create a tabular binary classification model from a Python script.


 Learn more about
Classification for tabular data
.

 Learn more about
Vertex Explainable AI
.
Tutorial steps
Create a Vertex AI dataset resource.
Train an AutoML tabular binary classification model.
View the model evaluation metrics for the trained model.
Create a serving endpoint resource.
Deploy the Model resource to a serving endpoint resource.
Make an online prediction request with explainability.
Undeploy the Model resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex Explainable AI
Vertex AI Batch Prediction
Custom training image classification model for batch prediction with explainabilty
.
Learn to use
Vertex AI Training and Vertex Explainable AI
to create a custom image classification model with explanations, and then you learn to use
Vertex AI Batch Prediction
to make a batch prediction request with explanations.


 Learn more about
Vertex Explainable AI
.

 Learn more about
Vertex AI Batch Prediction
.
Tutorial steps
Create a
Vertex AI
custom job for training a TensorFlow model.
View the model evaluation for the trained model.
Set explanation parameters for when the model is deployed.
Upload the trained model artifacts and explanation parameters as a
Model
resource.
Make a batch prediction with explanations.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex Explainable AI
Vertex AI Inference
Custom training image classification model for online prediction with explainability
.
Learn how to use Vertex AI training and Vertex Explainable AI to create a custom image classification model with explanations.


 Learn more about
Vertex Explainable AI
.

 Learn more about
Vertex AI Inference
.
Tutorial steps
Create a Vertex AI custom job for training a TensorFlow model.
View the model evaluation for the trained model.
Set explanation parameters for when the model is deployed.
Upload the trained model artifacts and explanations as a model resource.
Create a serving endpoint resource.
Deploy the model resource to a serving endpoint resource.
Make a prediction with explanation.
Undeploy the model resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex Explainable AI
Vertex AI Batch Prediction
Custom training tabular regression model for batch prediction with explainabilty
.
Learn how to use Vertex AI training and Vertex Explainable AI to create a custom image classification model with explanations.


 Learn more about
Vertex Explainable AI
.

 Learn more about
Vertex AI Batch Prediction
.
Tutorial steps
Create a Vertex AI custom job for training a TensorFlow model.
View the model evaluation for the trained model.
Set explanation parameters for the model.
Upload the trained model artifacts as a model resource.
Make a batch prediction with explanations.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex Explainable AI
Vertex AI Inference
Custom training tabular regression model for online prediction with explainabilty
.
Learn how to use Vertex AI training and Vertex Explainable AI to create a custom tabular regression model with explanations.


 Learn more about
Vertex Explainable AI
.

 Learn more about
Vertex AI Inference
.
Tutorial steps
Create a Vertex AI custom job for training a TensorFlow model.
View the model evaluation for the trained model.
Set explanation parameters for when the model is deployed.
Upload the trained model artifacts and explanations as a model resource.
Create a serving endpoint resource.
Deploy the model resource to a serving endpoint resource.
Make a prediction with explanation.
Undeploy the model resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex Explainable AI
Vertex AI Inference
Custom training tabular regression model for online prediction with explainabilty using get_metadata
.
Learn how to create a custom model from a Python script in a Google prebuilt Docker container using the Vertex AI SDK.


 Learn more about
Vertex Explainable AI
.

 Learn more about
Vertex AI Inference
.
Tutorial steps
Create a Vertex AI custom job for training a TensorFLow model.
Train a TensorFlow model.
Retrieve and load the model artifacts.
View the model evaluation for the trained model.
Set explanation parameters.
Upload the model as a Vertex AI model resource.
Deploy the Model resource to a serving endpoint resource.
Make a prediction with explanation.
Undeploy the Model resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex Explainable AI
Vertex AI Inference
Explaining image classification with Vertex Explainable AI
.
Learn how to configure feature-based explanations on a pre-trained image classification model and make online and batch predictions with explanations.


 Learn more about
Vertex Explainable AI
.

 Learn more about
Vertex AI Inference
.
Tutorial steps
Download pretrained model from TensorFlow Hub
Upload model for deployment
Deploy model for online prediction
Make online prediction with explanations
Make batch predictions with explanations
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex Explainable AI
Explaining text classification with Vertex Explainable AI
.
Learn how to configure feature-based explanations using the sampled Shapley method on a TensorFlow text classification model for online predictions with explanations.


 Learn more about
Vertex Explainable AI
.
Tutorial steps
Build and train a TensorFlow text classification model
Upload model for deployment
Deploy model for online prediction
Make online prediction with explanations
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Feature Store
Online feature serving and fetching of BigQuery data with Vertex AI Feature Store
.
Learn how to create and use an online feature store instance to host and serve data in BigQuery with Vertex AI Feature Store in an end to end workflow of feature values serving and fetching user journey.


 Learn more about
Vertex AI Feature Store
.
Tutorial steps
Provision an online feature store instance to host and serve data.
Register a BigQuery view with the online feature store instance and set up the sync job.
Use the online server to fetch feature values for online prediction.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Feature Store
Online feature serving and fetching of BigQuery data with Vertex AI Feature Store Optimized Serving
.
Learn how to create and use an online feature store instance to host and serve data in BigQuery with Vertex AI Feature Store in an end-to-end workflow of serving and fetching feature values.


 Learn more about
Vertex AI Feature Store
.
Tutorial steps
Provision an online feature store instance to host and serve data using Optimized online serving with Public or Private endpoint.
Register a BigQuery view with the online feature store instance and set up the sync job.
Use the online server to fetch feature values for online prediction.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Feature Store
Online feature serving and vector retrieval of BigQuery data with Vertex AI Feature Store
.
Learn how to create and use an online feature store instance to host and serve data in BigQuery with Vertex AI Feature Store in an end to end workflow of features serving and vector retrieval user journey.


 Learn more about
Vertex AI Feature Store
.
Tutorial steps
Provision an online feature store instance to host and serve data.
Create an online feature store instance to serve a BigQuery table.
Use the online server to search nearest neighbors.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Feature Store
Vertex AI Feature Store Based LLM Grounding tutorial
.
Learn how to create and use an online feature store instance to host and serve data in BigQuery with Vertex AI Feature Store in an end to end workflow of features serving and vector retrieval user journey.


 Learn more about
Vertex AI Feature Store
.
Tutorial steps
Provision an online feature store instance to host and serve data.
Create an online feature store instance to serve a BigQuery table.
Use the online server to search nearest neighbors.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Feature Store
Vertex AI Feature Store Feature View Service Agents Tutorial
.
Learn how to use a dedicated service agent for a feature view in Vertex AI Feature Store.


 Learn more about
Vertex AI Feature Store
.
Tutorial steps
Create a feature view configured to use a dedicated service account.
A service account is created for each feature view. Such service account is used to sync data from BigQuery.
Get/List feature view API returns the autocreated service account. Users need to call
bq addiampolicybinding
command to grant
roles/bigquery.dataViewer
to the service account.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Feature Store
Streaming import SDK in Vertex AI Feature Store (Legacy)
.
Learn how to import features from a
Pandas DataFrame
into Vertex AI Feature Store  using
write_feature_values
method from the Vertex AI SDK.


 Learn more about
Vertex AI Feature Store
.
Tutorial steps
Create a featurestore.
Create a new entity type for your featurestore.
Import feature values from
Pandas DataFrame
into the entity type in the featurestore.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Feature Store
Using Vertex AI Feature Store (Legacy) with Pandas Dataframe
.
Learn how to use
Vertex AI Feature Store
with pandas Dataframe.


 Learn more about
Vertex AI Feature Store
.
Tutorial steps
Create
Featurestore
,
EntityType
, and
Feature
resources.
Import feature values from Pandas DataFrame into the entity type.
Read entity feature values from the online feature store into Pandas DataFrame.
Batch serve feature values from your featurestore into Pandas DataFrame.
Online serving with updated feature values.
Pointintime correctness to fetch feature values for training.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Feature Store
Online and Batch predictions using Vertex AI Feature Store (Legacy)
.
Learn how to use
Vertex AI Feature Store
to import feature data, and to access the feature data for both online serving and offline tasks, such as training.


 Learn more about
Vertex AI Feature Store
.
Tutorial steps
Create
Featurestore
,
EntityType
, and
Feature
resources.
Import feature data into the
Featurestore
resource.
Serve online prediction requests using the imported features.
Access imported features in offline jobs, such as training jobs.
Use streaming import to import small amount of data.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Overview of Generative AI support on Vertex AI
Vertex AI LLM Batch Inference with RLHF-tuned Models
.
In this tutorial, you use Vertex AI to get predictions from an RLHF-tuned large-language model.


 Learn more about
Overview of Generative AI support on Vertex AI
.
Tutorial steps
Create Vertex AI Pipeline job using a predefined template for bulk inference.
Execute the pipeline using Vertex AI Pipelines.
Produce prediction results against a model for a given dataset.
Colab Enterprise
GitHub
Vertex AI Workbench
generative_ai
Distill a large language model
.
Learn how to distill and deploy a large language model using Vertex AI LLM.
Tutorial steps
Get the Vertex AI LLM model.
Distill the model(this automatically creates a Vertex AI endpoint and deploys the model to the endpoint).
Make a prediction using Vertex AI LLM.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Tune text models by using RLHF tuning
Vertex AI LLM Reinforcement Learning from Human Feedback
.
In this tutorial, you use Vertex AI RLHF to tune and deploy a large language model model.


 Learn more about
Tune text models by using RLHF tuning
.
Tutorial steps
Set the number of model tuning steps.
Create a Vertex AI Pipeline job using a predefined tuning template.
Execute the pipeline using Vertex AI Pipelines.
Get predictions from the tuned model.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
text embedding
Semantic Search using Embeddings
.
In this tutorial, we demonstrate how to create an embedding generated from text and perform a semantic search.


 Learn more about
text embedding
.
Tutorial steps
Installation and imports
Create embedding dataset
Create an index
Query the index
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
generative_ai
Getting Text Embeddings on Vertex AI
.
Learn how to get a text embedding given a text-embedding model and a text.
Tutorial steps
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
generative_ai
Getting Text Embeddings on Vertex AI
.
Learn how to get a text embedding given a text-embedding model and a text.
Tutorial steps
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Tune text models by using supervised tuning
Vertex AI Tuning a PEFT model
.
Learn to use Vertex AI LLM to tune and deploy a PEFT large language model.


 Learn more about
Tune text models by using supervised tuning
.
Tutorial steps
Get the Vertex AI LLM model.
Tune the model.
This automatically creates a Vertex AI endpoint and deploy the model to it.
Make a prediction using Vertex AI LLM.
Make a prediction using Vertex AI Inference.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
generative_ai
Getting Tuned Text-Embeddings on Vertex AI
.
Learn how to tune a text-embedding model.
Tutorial steps
Colab
GitHub
Vertex AI Workbench
PaLM API
Using the Vertex AI SDK with Large Language Models
.
Learn how to provide text input to Large Language Models  available on Vertex AI to test, tune, and deploy generative AI language models.


 Learn more about
PaLM API
.
Tutorial steps
Use the predict endpoints of Vertex AI PaLM API to receive generative AI responses to a message.
Use the text embedding endpoint to receive a vector representation of a message.
Perform prompt tuning of an LLM, based on input/output training data.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Migrate to Vertex AI
Classification for image data
AutoML Image Classification
.
Learn to use
AutoML
to train an image model and use
Vertex AI Inference
and
Vertex AI batch inference
to do online and batch predictions.


 Learn more about
Migrate to Vertex AI
.

 Learn more about
Classification for image data
.
Tutorial steps
Train an AutoML image classification model.
Make a batch prediction.
Deploy model to a endpoint
Make a online prediction
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Migrate to Vertex AI
Object detection for image data
AutoML image object detection
.
Learn to use
AutoML
to train an image model and use
Vertex AI Inference
and
Vertex AI Batch Prediction
to do online and batch predictions.


 Learn more about
Migrate to Vertex AI
.

 Learn more about
Object detection for image data
.
Tutorial steps
Train an AutoML object detection model.
Make a batch prediction.
Deploy model to a endpoint
Make a online prediction
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Migrate to Vertex AI
Classification for tabular data
AutoML tabular binary classification
.
In this tutorial, you create an AutoML tabular binary classification model and deploy for online prediction from a Python script using the Vertex AI SDK.


 Learn more about
Migrate to Vertex AI
.

 Learn more about
Classification for tabular data
.
Tutorial steps
Create a Vertex AI dataset resource.
Train the model.
View the model evaluation.
Deploy the model resource to a serving endpoint resource.
Make a prediction.
Undeploy the model
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Migrate to Vertex AI
Custom training
Custom image classification with a custom training container
.
Learn how to train a tensorflow image classification model using a custom container and Vertex AI training.


 Learn more about
Migrate to Vertex AI
.

 Learn more about
Custom training
.
Tutorial steps
Package the training code into a python application.
Containerize the training application using Cloud Build and Artifact Registry.
Create a custom container training job in Vertex AI and run it.
Evaluate the model generated from the training job.
Create a model resource for the trained model in Vertex AI Model Registry.
Run a Vertex AI batch inference job.
Deploy the model resource to a Vertex AI endpoint.
Run a online prediction job on the model resource.
Clean up the resources created.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Migrate to Vertex AI
Custom training overview
Custom image classification with a pre-built training container
.
Learn how to train a tensorflow image classification model using a prebuilt container and Vertex AI training.


 Learn more about
Migrate to Vertex AI
.

 Learn more about
Custom training overview
.
Tutorial steps
Package the training code into a python application.
Containerize the training application using Cloud Build and Artifact Registry.
Create a custom container training job in Vertex AI and run it.
Evaluate the model generated from the training job.
Create a model resource for the trained model in Vertex AI Model Registry.
Run a Vertex AI batch inference job.
Deploy the model resource to a Vertex AI endpoint.
Run a online prediction job on the model resource.
Clean up the resources created.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Migrate to Vertex AI
Custom training overview
Custom Scikit-Learn model with pre-built training container
.
Learn how to use Vertex AI Training to create a custom trained model.


 Learn more about
Migrate to Vertex AI
.

 Learn more about
Custom training overview
.
Tutorial steps
Create a Vertex AI custom job for training a scikitlearn model.
Upload the trained model artifacts as a model resource.
Generate batch predictions.
Deploy the model resource to a serving endpoint resource.
Generate online predictions.
Undeploy the model resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Migrate to Vertex AI
Custom training overview
Custom XGBoost model with pre-built training container
.
Learn to use Vertex AI Training to create a custom trained model.


 Learn more about
Migrate to Vertex AI
.

 Learn more about
Custom training overview
.
Tutorial steps
Create a Vertex AI custom job for training a xgboost model.
Upload the trained model artifacts as a model resource.
Generate batch predictions.
Deploy the model resource to a serving endpoint resource.
Generate online predictions.
Undeploy the model resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI hyperparameter tuning
Custom training
Hyperparameter Tuning
.
Learn to use Vertex AI hyperparameter to create and tune a custom trained model.


 Learn more about
Vertex AI hyperparameter tuning
.

 Learn more about
Custom training
.
Tutorial steps
Create a Vertex AI hyperparameter tuning job for training a TensorFlow model.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Google Artifact Registry documentation
Get started with Google Artifact Registry
.
Learn how to use Google Artifact Registry.


 Learn more about
Google Artifact Registry documentation
.
Tutorial steps
Creating a private Docker repository.
Tagging a container image, specific to the private Docker repository.
Pushing a container image to the private Docker repository.
Pulling a container image from the private Docker repository.
Deleting a private Docker repository.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex ML Metadata
Track parameters and metrics for custom training jobs
.
Learn how to use Vertex AI SDK for Python to:
Tutorial steps
Track training parameters and prediction metrics for a custom training job.
Extract and perform analysis for all parameters and metrics within an Experiment.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex ML Metadata
Track parameters and metrics for locally trained models
.
Learn how to use Vertex ML Metadata to track training parameters and evaluation metrics.


 Learn more about
Vertex ML Metadata
.
Tutorial steps
Track parameters and metrics for a locally trained model.
Extract and perform analysis for all parameters and metrics within an experiment.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex ML Metadata
Vertex AI Pipelines
Track artifacts and metrics across Vertex AI Pipelines runs using Vertex ML Metadata
.
Learn how to track artifacts and metrics with Vertex ML Metadata in Vertex AI Pipeline runs.


 Learn more about
Vertex ML Metadata
.

 Learn more about
Vertex AI Pipelines
.
Tutorial steps
Use the Kubeflow Pipelines SDK to build an ML pipeline that runs on Vertex AI.
The pipeline creates a dataset, trains a scikitlearn model, and deploys the model to an endpoint.
Write custom pipeline components that generate artifacts and metadata.
Compare Vertex AI Pipeline runs, both in the Google Cloud console and programmatically.
Trace the lineage for pipelinegenerated artifacts.
Query your pipeline run metadata.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Evaluation
Classification for tabular data
Evaluating batch prediction results from an AutoML Tabular classification model
.
Learn how to train a Vertex AI AutoML Tabular classification model and learn how to evaluate it through a Vertex AI pipeline job using
google_cloud_pipeline_components
:


 Learn more about
Vertex AI Model Evaluation
.

 Learn more about
Classification for tabular data
.
Tutorial steps
Create a Vertex AI
Dataset
.
Train an Automl Tabular classification model on the
Dataset
resource.
Import the trained
AutoML model resource
into the pipeline.
Run a
Batch Prediction
job.
Evaluate the AutoML model using the
Classification Evaluation component
.
Import the classification metrics to the AutoML model resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Evaluation
Regression for tabular data
Evaluating batch prediction results from AutoML tabular regression model
.
Learn how to evaluate a Vertex AI model resource through a Vertex AI pipeline job using
google_cloud_pipeline_components
:


 Learn more about
Vertex AI Model Evaluation
.

 Learn more about
Regression for tabular data
.
Tutorial steps
Create a Vertex AI dataset.
Configure an
AutoMLTabularTrainingJob
class.
Run the
AutoMLTabularTrainingJob
which returns a model.
Import a pretrained
AutoML model resource
into the pipeline.
Run a
batch prediction
job in the pipeline.
Evaluate the AutoML model using the
regression evaluation component
.
Import the generated regression metrics into the AutoML model resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI custom training
Vertex AI model evaluation
Evaluating BatchPrediction results from a custom tabular classification model
.
In this tutorial, you train a scikit-learn RandomForest model, save the model in Vertex AI Model Registry and learn how to evaluate the model through a Vertex AI pipeline job using Google Cloud Pipeline Components Python SDK.


 Learn more about
Vertex AI custom training
.

 Learn more about
Vertex AI model evaluation
.
Tutorial steps
Fetch the dataset from the public source.
Preprocess the data locally and save test data in BigQuery.
Train a RandomForest classification model locally using scikitlearn Python package.
Create a custom container in Artifact Registry for predictions.
Upload the model in Vertex AI Model Registry.
Create and run a Vertex AI Pipeline that
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Evaluation
Custom training
Evaluating batch prediction results from custom tabular regression model
.
Learn how to evaluate a Vertex AI model resource through a Vertex AI pipeline job using google cloud pipeline components.


 Learn more about
Vertex AI Model Evaluation
.

 Learn more about
Custom training
.
Tutorial steps
Create a Vertex AI Custom Training Job to train a TensorFlow model.
Run the custom training job.
Retrieve and load the model artifacts.
View the model evaluation.
Upload the model as a Vertex AI model resource.
Import a pretrained Vertex AI model resource into the pipeline.
Run a batch prediction job in the pipeline.
Evaluate the model using the regression evaluation component.
Import the Regression Metrics to the Vertex AI model resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI AutoSxS Model Evaluation
Check autorater alignment against a human-preference dataset
.
Learn how to use
Vertex AI Pipelines
and
google_cloud_pipeline_components
to check autorater alignment using human-preference data:


 Learn more about
Vertex AI AutoSxS Model Evaluation
.
Tutorial steps
Create a evaluation dataset with predictions and human preference data.
Preprocess the data locally and save it in Cloud Storage.
Create and run a Vertex AI AutoSxS Pipeline that generates the judgments and a set of AutoSxS metrics using the generated judgments.
Print the judgments and AutoSxS metrics.
Clean up the resources created in this notebook.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI AutoSxS Model Evaluation
Evaluate a LLM in Vertex AI Model Registry against a third-party model
.
Learn how to use
Vertex AI Pipelines
and
google_cloud_pipeline_components
to evaluate the performance between two LLM models:


 Learn more about
Vertex AI AutoSxS Model Evaluation
.
Tutorial steps
Fetch the dataset from the public source.
Preprocess the data locally and save test data in Cloud Storage.
Create and run a Vertex AI AutoSxS Pipeline that generates the judgments and evaluates the two candidate models using the generated judgments.
Print the judgments and evaluation metrics.
Clean up the resources created in this notebook.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Monitoring for batch predictions
Vertex AI Batch Prediction with Model Monitoring
.
Learn to use the Vertex AI model monitoring service to detect drift and anomalies in batch prediction.


 Learn more about
Vertex AI Model Monitoring for batch predictions
.
Tutorial steps
Upload a pretrained model as a Vertex AI model resource.
Generate batch prediction requests.
Interpret the statistics, visualizations, other data reported by the model monitoring feature.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Monitoring
Vertex AI Model Monitoring for AutoML tabular models
.
Learn to use the Vertex AI Model Monitoring service to detect feature skew and drift in the input predict requests, for AutoML tabular models.


 Learn more about
Vertex AI Model Monitoring
.
Tutorial steps
Train an AutoML model.
Deploy the model resource to a Vertex AI endpoint resource.
Configure the endpoint resource for model monitoring.
Generate synthetic prediction requests for skew.
Generate synthetic prediction requests for drift.
Wait for email alert notification.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Monitoring
Vertex AI Model Monitoring for online prediction in AutoML image models
.
Learn how to use
Vertex AI Model Monitoring
with
Vertex AI Online Prediction
with an AutoML image classification model to detect an out of distribution image.


 Learn more about
Vertex AI Model Monitoring
.
Tutorial steps
1. Train an AutoML image classification model.
2. Create an endpoint.
3. Deploy the model to the endpoint, and configure for model monitoring.
4. Submit a online prediction containing both in and out of distribution images.
5. Use Model Monitoring to calculate anomaly score on each image.
6. Identify the images in the online prediction request that are out of distribution.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Monitoring
Vertex AI Model Monitoring for custom tabular models
.
Learn to use the Vertex AI Model Monitoring service to detect feature skewness and drift in the input predict requests, for custom tabular models.


 Learn more about
Vertex AI Model Monitoring
.
Tutorial steps
Download a pretrained custom tabular model.
Upload the pretrained model to Vertex AI Model Registry.
Deploy the model resource to a Vertex AI endpoint resource.
Configure the endpoint resource for model monitoring.
Generate synthetic prediction requests to simulate skewness.
Wait for email alert notifications.
Generate synthetic prediction requests to simulate drift.
Wait for email alert notifications.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Monitoring
Vertex AI Model Monitoring for custom tabular models with TensorFlow Serving container
.
Learn to use the Vertex AI Model Monitoring service to detect feature skew and drift in the input predict requests, for custom tabular models, using a custom deployment container.


 Learn more about
Vertex AI Model Monitoring
.
Tutorial steps
Download a pretrained custom tabular model.
Upload the pretrained model as a model resource.
Deploying the model resource to an endpoint resource with "TensorFlow Serving" serving binary.
Configure the Endpoint resource for model monitoring.
Generate synthetic prediction requests for skew.
Wait for email alert notification.
Generate synthetic prediction requests for drift.
Wait for email alert notification.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Monitoring
Vertex AI Model Monitoring for setup for tabular models
.
Learn to setup the Vertex AI Model Monitoring service to detect feature skew and drift in the input predict requests.


 Learn more about
Vertex AI Model Monitoring
.
Tutorial steps
Download a pretrained custom tabular model.
Upload the pretrained model as a model resource.
Deploy the model resource to the endpoint resource.
Configure the endpoint resource for model monitoring.
Skew and drift detection for feature inputs.
Skew and drift detection for feature attributions.
Automatic generation of the input schema by sending 1000 prediction request.
List, pause, resume and delete monitoring jobs.
Restart monitoring job with predefined input schema.
View logged monitored data.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Monitoring
Vertex AI Model Monitoring for XGBoost models
.
Learn to use the Vertex AI Model Monitoring service to detect feature skew and drift in the input predict requests for XGBoost models.


 Learn more about
Vertex AI Model Monitoring
.
Tutorial steps
Download a pretrained XGBoost model.
Upload the pretrained model to Vertex AI Model Registry.
Deploy the model resource to a Vertex AI endpoint resource.
Configure the endpoint resource for model monitoring
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Monitoring
Vertex AI Model Monitoring with Vertex Explainable AI Feature Attributions
.
Learn to use the Vertex AI Model Monitoring service to detect drift and anomalies in prediction requests from a deployed Vertex AI model resource.


 Learn more about
Vertex AI Model Monitoring
.
Tutorial steps
Upload a pretrained model as a Vertex AI model resource.
Create an Vertex AI endpoint resource.
Deploy the model resource to the endpoint resource.
Configure the endpoint resource for model monitoring.
Initialize the baseline distribution for model monitoring.
Generate synthetic prediction requests.
Understand how to interpret the statistics, visualizations, other data reported by the model monitoring feature.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
model_monitoring_v2
Model Monitoring for Vertex AI Custom Model Batch Prediction Job
.
In this tutorial, you'll complete the following steps:
Tutorial steps
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
model_monitoring_v2
Model Monitoring for Vertex AI Custom Model Online Prediction
.
In this tutorial, you'll complete the following steps:
Tutorial steps
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Model Registry
Get started with Vertex AI Model Registry
.
Learn how to use Vertex AI Model Registry to create and register multiple versions of a model.


 Learn more about
Vertex AI Model Registry
.
Tutorial steps
Create and register a first version of a model to Vertex AI Model Registry.
Create and register a second version of a model to Vertex AI Model Registry.
Updating the model version which is the default.
Deleting a model version.
Retraining the next model version.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
AutoML components
Classification for tabular data
AutoML Tabular pipelines using google-cloud-pipeline-components
.
Learn to use Vertex AI Pipelines and Google Cloud Pipeline Components to build an AutoML tabular classification model.


 Learn more about
Vertex AI Pipelines
.

 Learn more about
AutoML components
.

 Learn more about
Classification for tabular data
.
Tutorial steps
Create a KFP pipeline that creates a Vertex AI Dataset.
Add a component to the pipeline that trains an AutoML tabular classification model resource.
Add a component that creates a Vertex AI endpoint resource.
Add a component that deploys the model resource to the endpoint resource.
Compile the KFP pipeline.
Execute the KFP pipeline using Vertex AI Pipelines.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Challenger vs Blessed methodology for model deployment into production
.
Learn how to construct a Vertex AI pipeline, which trains a new challenger version of a model, evaluates the model and compares the evaluation to the existing blessed model in production.
Tutorial steps
Import a pretrained (blessed) model to the Vertex AI Model Registry.
Import synthetic model evaluation metrics to the corresponding (blessed) model.
Create a Vertex AI endpoint resource
Deploy the blessed model to the endpoint resource.
Create a Vertex AI Pipeline that runs the following steps
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Pipeline control structures using the KFP SDK
.
Learn how to use the KFP SDK, which uses loops and conditionals including nested examples, to build pipelines.


 Learn more about
Vertex AI Pipelines
.
Tutorial steps
Create a KFP pipeline using control flow components
Compile the KFP pipeline
Execute the KFP pipeline using Vertex AI Pipelines
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Custom training components
Custom training with pre-built Google Cloud Pipeline Components
.
Learn to use Vertex AI Pipelines and Google Cloud Pipeline Components to build a custom model.


 Learn more about
Vertex AI Pipelines
.

 Learn more about
Custom training components
.
Tutorial steps
Create a KFP pipeline
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Vertex AI Batch Prediction components
Training and batch prediction with BigQuery source and destination for a custom tabular classification model
.
In this tutorial, you train a scikit-learn tabular classification model and create a batch prediction job for it through a Vertex AI pipeline using google_cloud_pipeline_components.


 Learn more about
Vertex AI Pipelines
.

 Learn more about
Vertex AI Batch Prediction components
.
Tutorial steps
Create a dataset in BigQuery.
Set some data aside from the source dataset for batch prediction.
Create a custom python package for training application.
Upload the python package to Cloud Storage.
Create a Vertex AI Pipeline that
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Vertex AI hyperparameter tuning
Get started with Vertex AI hyperparameter tuning pipeline components
.
Learn how to use prebuilt Google Cloud Pipeline Components for Vertex AI hyperparameter tuning.


 Learn more about
Vertex AI Pipelines
.

 Learn more about
Vertex AI hyperparameter tuning
.
Tutorial steps
Construct a pipeline for
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Get started with machine management for Vertex AI Pipelines
.
Learn how to convert a self-contained custom training component into a
Vertex AI CustomJob
, whereby:
Tutorial steps
Create a custom component with a selfcontained training job.
Execute pipeline using componentlevel settings for machine resources
Convert the selfcontained training component into a
Vertex AI CustomJob
.
Execute pipeline using customjoblevel settings for machine resources
Colab
GitHub
Vertex AI Workbench
Vertex AI Pipelines
AutoML components
AutoML image classification pipelines using google-cloud-pipeline-components
.
Learn how to use Vertex AI Pipelines and Google Cloud pipeline components to build an AutoML image classification model.


 Learn more about
Vertex AI Pipelines
.

 Learn more about
AutoML components
.
Tutorial steps
Create a KFP pipeline
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
AutoML components
Regression for tabular data
AutoML tabular regression pipelines using google-cloud-pipeline-components
.
Learn to use
Vertex AI Pipelines
and
Google Cloud Pipeline Components
to build an
AutoML
tabular regression model.


 Learn more about
Vertex AI Pipelines
.

 Learn more about
AutoML components
.

 Learn more about
Regression for tabular data
.
Tutorial steps
Create a KFP pipeline that creates a
Dataset
resource.
Add a component to the pipeline that trains an AutoML tabular regression
Model
resource.
Add a component that creates an
Endpoint
resource.
Add a component that deploys the
Model
resource to the
Endpoint
resource.
Compile the KFP pipeline.
Execute the KFP pipeline using
Vertex AI Pipelines
.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
BigQuery ML components
Training an acquisition-prediction model using Swivel, BigQuery ML and Vertex AI Pipelines
.
Learn how to build a simple BigQuery ML pipeline using Vertex AI pipelines in order to calculate text embeddings of content from articles and classify them
into the *corporate acquisitions* category.


 Learn more about
Vertex AI Pipelines
.

 Learn more about
BigQuery ML components
.
Tutorial steps
Creating a component for Dataflow job that ingests data to BigQuery.
Creating a component for preprocessing steps to run on the data in BigQuery.
Creating a component for training a logistic regression model using BigQuery ML.
Building and configuring a Kubeflow DSL pipeline with all the created components.
Compiling and running the pipeline in Vertex AI Pipelines.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Custom training components
Model train, upload, and deploy using Google Cloud Pipeline Components
.
Learn how to use Vertex AI Pipelines and Google Cloud pipeline component to build and deploy a custom model.


 Learn more about
Vertex AI Pipelines
.

 Learn more about
Custom training components
.
Tutorial steps
Create a KFP pipeline
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Vertex AI Pipelines with KFP 2.x
.
Learn to use
Vertex AI Pipelines
and KFP 2.
Tutorial steps
Create a KFP pipeline
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Lightweight Python function-based components, and component I/O
.
Learn to use the KFP SDK to build lightweight Python function-based components, and then you learn to use Vertex AI Pipelines to execute the pipeline.


 Learn more about
Vertex AI Pipelines
.
Tutorial steps
Build Python functionbased KFP components.
Construct a KFP pipeline.
Pass Artifacts and parameters between components, both by path reference and by value.
Use the kfp.dsl.importer method.
Compile the KFP pipeline.
Execute the KFP pipeline using Vertex AI Pipelines
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Metrics visualization and run comparison using the KFP SDK
.
Learn how to use the KFP SDK for Python to build pipelines that generate evaluation metrics.


 Learn more about
Vertex AI Pipelines
.
Tutorial steps
Create KFP components
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Multicontender vs Champion methodology for model deployment into production
.
Learn how to construct a Vertex AI pipeline, which evaluates new production data from a deployed  model against other versions  of the model, to determine if a contender model becomes the champion model for replacement in production.
Tutorial steps
Import a pretrained (champion) model to the Vertex AI Model Registry.
Import synthetic model training evaluation metrics to the corresponding (champion) model.
Create a Vertex AI endpoint resource.
Deploy the champion model to the endpoint resource.
Import additional (contender) versions of the deployed model.
Import synthetic model training evaluation metrics to the corresponding (contender) models.
Create a Vertex AI Pipeline that runs the following steps
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Pipelines
Pipelines introduction for KFP
.
Learn how to use the KFP SDK for Python to build pipelines that generate evaluation metrics.


 Learn more about
Vertex AI Pipelines
.
Tutorial steps
Define and compile a Vertex AI pipeline.
Specify which service account to use for a pipeline run.
Run the pipeline using Vertex AI SDK for Python and REST API.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
AutoML components
BigQuery ML components
BigQuery ML and AutoML - Rapid Prototyping with Vertex AI
.
Learn how to use Vertex AI Pipelines for rapid prototyping a model.


 Learn more about
AutoML components
.

 Learn more about
BigQuery ML components
.
Tutorial steps
Creating a BigQuery and Vertex AI training dataset.
Training a BigQuery ML and AutoML model.
Extracting evaluation metrics from the BigQueryML and AutoML models.
Selecting the best trained model.
Deploying the best trained model.
Testing the deployed model infrastructure.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Batch Inference
Custom model batch inference with feature filtering
.
Learn how to create a custom-trained model from a Python script in a Docker container using the Vertex AI SDK for Python, and then run a batch inference job by including or excluding a list of features.


 Learn more about
Vertex AI Batch Inference
.
Tutorial steps
Create a Vertex AI custom
TrainingPipeline
for training a model.
Train a TensorFlow model.
Send batch prediction job.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Inference
Get started with NVIDIA Triton server
.
Learn how to deploy a container running Nvidia Triton Server with a Vertex AI model resource to a Vertex AI endpoint for making online predictions.


 Learn more about
Vertex AI Inference
.
Tutorial steps
Download the model artifacts from TensorFlow Hub.
Create Triton serving configuration file for the model.
Construct a custom container, with Triton serving image, for model deployment.
Upload the model as a Vertex AI model resource.
Deploy the Vertex AI model resource to a Vertex AI endpoint resource.
Make a prediction request.
Undeploy the model resource and delete the endpoint.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Raw Predict
Get started with TensorFlow serving functions with Vertex AI Raw Prediction
.
Learn how to use
Vertex AI Raw Prediction
on a
Vertex AI Endpoint
resource.


 Learn more about
Raw Predict
.
Tutorial steps
Download pretrained tabular classification model artifacts for a TensorFlow 1.x estimator.
Upload the TensorFlow estimator model as a
Vertex AI Model
resource.
Create an
Endpoint
resource.
Deploy the
Model
resource to an
Endpoint
resource.
Make an online raw prediction to the
Model
resource instance deployed to the
Endpoint
resource.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
getting predictions from a custom trained model
Get started with TensorFlow Serving with Vertex AI Inference
.
Learn how to use
Vertex AI Inference
on a
Vertex AI Endpoint
resource with
TensorFlow Serving
serving binary.


 Learn more about
getting predictions from a custom trained model
.
Tutorial steps
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Private Endpoints
Get started with Vertex AI Private Endpoints
.
Learn how to use
Vertex AI Private Endpoint
resources.


 Learn more about
Private Endpoints
.
Tutorial steps
Creating a
Private Endpoint
resource.
Configure a VPC peering connection.
Configuring the serving binary of a
Model
resource for deployment to a
Private Endpoint
resource.
Deploying a
Model
resource to a
Private Endpoint
resource.
Send a prediction request to a
Private Endpoint
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Language Models
Vertex AI LLM and streaming prediction
.
Learn how to use Vertex AI LLM to download pretrained LLM model, make predictions and finetuning the model.


 Learn more about
Vertex AI Language Models
.
Tutorial steps
Load a pretrained text generation model.
Make a nonstreaming prediction
Load a pretrained text generation model, which supports streaming.
Make a streaming prediction
Load a pretrained chat model.
Do a local interactive chat session.
Do a batch prediction with a text generation model.
Do a batch prediction with a text embedding model.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Pre-built containers for prediction
Serving PyTorch image models with prebuilt containers on Vertex AI
.
Learn how to package and deploy a PyTorch image classification model using a prebuilt Vertex AI container with TorchServe for serving online and batch predictions.


 Learn more about
Pre-built containers for prediction
.
Tutorial steps
Download a pretrained image model from PyTorch
Create a custom model handler
Package model artifacts in a model archive file
Upload model for deployment
Deploy model for prediction
Make online predictions
Make batch predictions
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Inference
Train and deploy PyTorch models with prebuilt containers on Vertex AI
.
Learn how to build, train and deploy a PyTorch image classification model using prebuilt containers for custom training and prediction.
Tutorial steps
Package training application into a Python source distribution
Configure and run training job in a prebuilt container
Package model artifacts in a model archive file
Upload model for deployment
Deploy model using a prebuilt container for prediction
Make online predictions
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Ray on Vertex AI overview
Get started with PyTorch on Ray on Vertex AI
.
Learn how to efficiently distribute the training process of a PyTorch image classification model by leveraging Ray on Vertex AI.


 Learn more about
Ray on Vertex AI overview
.
Tutorial steps
Prepare the training script
Submit a Ray job using the Ray Jobs API
Download a trained image model from PyTorch
Create a custom model handler
Package model artifacts in a model archive file
Register model in Vertex AI Model Registry
Deploy model in Vertex AI Endpoint
Make online predictions
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Ray on Vertex AI overview
Ray on Vertex AI cluster management
.
Learn how to create a cluster, list existing clusters, get a cluster, update  a cluster, and delete a cluster.


 Learn more about
Ray on Vertex AI overview
.
Tutorial steps
Create a cluster.
List existing clusters.
Get a cluster.
Manually scale up the cluster, then scale down the cluster.
Autoscaling a cluster.
Delete existing clusters.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Ray on Vertex AI
Spark on Ray on Vertex AI
Spark on Ray on Vertex AI
.
Learn how to use RayDP to run Spark applications on a Ray cluster on Vertex AI.


 Learn more about
Ray on Vertex AI
.

 Learn more about
Spark on Ray on Vertex AI
.
Tutorial steps
Create custom Ray on Vertex AI container image
Create a Ray cluster on Vertex AI using custom container image
Run Spark interactively on the cluster using RayDP
Run Spark application on cluster via Ray Job API
Read files from Google Cloud Storage in Spark application
Pandas UDF in Spark application on Ray on Vertex AI
Delete the Ray cluster on Vertex AI
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Training
Vertex AI Reduction Server
PyTorch distributed training with Vertex AI Reduction Server
.
Learn how to create a PyTorch distributed training job that uses PyTorch distributed training framework and tools, and run the training job on the Vertex AI Training service with Reduction Server.


 Learn more about
Vertex AI Training
.

 Learn more about
Vertex AI Reduction Server
.
Tutorial steps
Create a PyTorch distributed training application
Package the training application with prebuilt containers
Create a custom job on Vertex AI with Reduction Server
Submit and monitor the job
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Custom training
Custom training using Python package, managed text dataset, and TF Serving container
.
Learn how to create a custom model using Custom Python Package Training and you learn how to serve the model using TensorFlow-Serving Container for online prediction.


 Learn more about
Custom training
.
Tutorial steps
Create utility functions to download data and prepare csv files for creating Vertex AI managed dataset
Download Data
Prepare CSV Files for creating managed dataset
Create custom training Python package
Create TensorFlow Serving container
Run custom Python package training with managed text dataset
Deploy a model and create an endpoint on Vertex AI
Predict on the endpoint
Create a Batch Prediction job on the model
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Tabular Workflow for TabNet
Vertex AI Explanations with TabNet models
.
Learn how to provide a sample plotting tool to visualize the output of TabNet, which is helpful in explaining the algorithm.


 Learn more about
Tabular Workflow for TabNet
.
Tutorial steps
Setup the project.
Download the prediction data of pretrain model on Syn2 data.
Visualize and understand the feature importance based on the masks output.
Clean up the resource created by this tutorial.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
BigQuery ML ARIMA+ forecasting for tabular data
Train a BigQuery ML ARIMA_PLUS Model using Vertex AI tabular workflows
.
Learn how to create the BigQuery ML ARIMA_PLUS model using a training Vertex AI Pipeline from Google Cloud Pipeline Components , and then do a batch prediction using the corresponding prediction pipeline.


 Learn more about
BigQuery ML ARIMA+ forecasting for tabular data
.
Tutorial steps
Train the BigQuery ML ARIMA_PLUS model.
View BigQuery ML model evaluation.
Make a batch prediction with the BigQuery ML model.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Google Cloud Pipeline Components
Prophet for tabular data
Train a Prophet Model using Vertex AI Tabular Workflows
.
Learn how to create several Prophet models using a training Vertex AI Pipeline from Google Cloud Pipeline Components , and then do a batch prediction using the corresponding prediction pipeline.


 Learn more about
Google Cloud Pipeline Components
.

 Learn more about
Prophet for tabular data
.
Tutorial steps
1. Train the Prophet models.
1. View the evaluation metrics.
1. Make a batch prediction with the Prophet models.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Tabular Workflow for TabNet
TabNet Pipeline
.
Learn how to create classification models on tabular data using two of the Vertex AI TabNet Tabular Workflows.


 Learn more about
Tabular Workflow for TabNet
.
Tutorial steps
Create a TabNet CustomJob. This is the best option if you know which hyperparameters to use for training.
Create a TabNet HyperparameterTuningJob. This allows you to get the best set of hyperparameters for your dataset.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Tabular Workflow for Wide & Deep
Wide & Deep Pipeline
.
Learn how to create two classification models using Vertex AI Wide & Deep Tabular Workflows.


 Learn more about
Tabular Workflow for Wide & Deep
.
Tutorial steps
Create a Wide & Deep CustomJob. This is the best option if you know which hyperparameters to use for training.
Create a Wide & Deep HyperparameterTuningJob. This allows you to get the best set of hyperparameters for your dataset.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI TensorBoard
Custom training
Vertex AI TensorBoard custom training with custom container
.
Learn how to create a custom training job using custom containers, and monitor your training process on Vertex AI TensorBoard in near real time.


 Learn more about
Vertex AI TensorBoard
.

 Learn more about
Custom training
.
Tutorial steps
Create docker repository & config.
Create a custom container image with your customized training code.
Setup service account and Google Cloud Storage buckets.
Create & launch your custom training job with your custom container.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI TensorBoard
Custom training
Vertex AI TensorBoard custom training with prebuilt container
.
Learn how to create a custom training job using prebuilt containers, and monitor your training process on Vertex AI TensorBoard in near real time.


 Learn more about
Vertex AI TensorBoard
.

 Learn more about
Custom training
.
Tutorial steps
Setup service account and Cloud Storage buckets.
Write your customized training code.
Package and upload your training code to Cloud Storage.
Create & launch your custom training job with Vertex AI TensorBoard enabled for near real time monitoring.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI TensorBoard
Vertex AI TensorBoard hyperparameter tuning with the HParams Dashboard
.
In this notebook, you train a model and perform hyperparameter tuning using tensorflow.
Tutorial steps
Adapt TensorFlow runs to log hyperparameters and metrics.
Start runs and log them all under one parent directory.
Visualize the results in Vertex AI TensorBoard's HParams dashboard.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Profiler
Vertex AI TensorBoard
Profile model training performance using Cloud Profiler
.
Learn how to enable Profiler for custom training jobs.


 Learn more about
Profiler
.

 Learn more about
Vertex AI TensorBoard
.
Tutorial steps
Setup a service account and a Cloud Storage bucket
Create a Vertex AI TensorBoard instance
Create and run a custom training job that enables Profiler
View the Profiler dashboard to debug your model training performance
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Profiler
Vertex AI TensorBoard
Profile model training performance using Cloud Profiler in custom training with prebuilt container
.
Learn how to enable Profiler in Vertex AI for custom training jobs with a prebuilt container.


 Learn more about
Profiler
.

 Learn more about
Vertex AI TensorBoard
.
Tutorial steps
Prepare your custom training code and load your training code as a Python package to a prebuilt container
Create and run a custom training job that enables Profiler
View the Profiler dashboard to debug your model training performance
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI TensorBoard
Vertex AI Pipelines
Vertex AI TensorBoard integration with Vertex AI Pipelines
.
Learn how to create a training pipeline using the KFP SDK, execute the pipeline in Vertex AI Pipelines, and monitor the training process on Vertex AI TensorBoard in near real time.


 Learn more about
Vertex AI TensorBoard
.

 Learn more about
Vertex AI Pipelines
.
Tutorial steps
Setup a service account and Google Cloud Storage buckets.
Construct a KFP pipeline with your custom training code.
Compile and execute the KFP pipeline in Vertex AI Pipelines with Vertex AI TensorBoard enabled for near real time monitoring.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Hyperparameter Tuning
Distributed Vertex AI Hyperparameter Tuning
.
In this notebook, you create a custom trained model from a Python script in a Docker container.


 Learn more about
Vertex AI Hyperparameter Tuning
.
Tutorial steps
Training using a Python package.
Report accuracy when hyperparameter tuning.
Save the model artifacts to Cloud Storage using Cloud StorageFuse.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Training
Get started with Vertex AI Training for LightGBM
.
Learn how to train a LightGBM custom model using the custom container method for Vertex AI Training.
Tutorial steps
Training using a Python package.
Save the model artifacts to Cloud Storage using Cloud StorageFuse.
Construct a FastAPI prediction server.
Construct a Dockerfile deployment image for the server.
Test the deployment image locally (optional and not for Colab users).
Create a Vertex AI model resource.
Run a batch prediction job.
Deploy the model to an endpoint and send online prediction requests.
Clean up the created resources.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI distributed training
Get started with Vertex AI distributed training
.
Learn how to use Vertex AI distributed training when training with
Vertex AI
.


 Learn more about
Vertex AI distributed training
.
Tutorial steps
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Hyperparameter Tuning
Run hyperparameter tuning for a TensorFlow model
.
Learn how to run a Vertex AI hyperparameter tuning job for a TensorFlow model.


 Learn more about
Vertex AI Hyperparameter Tuning
.
Tutorial steps
Modify training application code for automated hyperparameter tuning.
Containerize training application code.
Configure and launch a hyperparameter tuning job with the Vertex AI Python SDK.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI hyperparameter tuning
Vertex AI Hyperparameter Tuning for XGBoost
.
Learn how to use the Vertex AI hyperparameter tuning service for training an XGBoost model.


 Learn more about
Vertex AI hyperparameter tuning
.
Tutorial steps
Train using a Python training application package.
Report accuracy during hyperparameter tuning.
Save the model artifacts to Cloud Storage using Cloud StorageFuse.
List the best model.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Training
PyTorch image classification multi-node distributed data parallel training on cpu using Vertex AI training with custom container
.
Learn how to create a distributed PyTorch training job using Vertex AI SDK for Python and custom containers.


 Learn more about
Vertex AI Training
.
Tutorial steps
Setting up your Google Cloud project
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Training
PyTorch image classification using multi-node NCCL distributed data parallel training on CPU and Vertex AI
.
Learn how to create a distributed PyTorch training job using Vertex AI SDK for Python and custom containers.


 Learn more about
Vertex AI Training
.
Tutorial steps
Building a custom container using Artifact Registry and Docker.
Creating a Vertex AI tensorboard instance to store your Vertex AI experiment.
Run a Vertex AI training job using Vertex AI SDK for Python.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Custom training
Training, tuning and deploying a PyTorch text sentiment classification model on Vertex AI
.
Learn to build, train, tune and deploy a PyTorch model on Vertex AI.


 Learn more about
Custom training
.
Tutorial steps
Create training package for the text classification model.
Train the model with custom training on Vertex AI.
Check the created model artifacts.
Create a custom container for predictions.
Deploy the trained model to a Vertex AI Endpoint using the custom container for predictions.
Send online prediction requests to the deployed model and validate.
Clean up the resources created in this notebook.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
PyTorch integration in Vertex AI
Train PyTorch model on Vertex AI with data from Cloud Storage
.
Learn how to create a training job using PyTorch and a dataset stored on Cloud Storage.


 Learn more about
PyTorch integration in Vertex AI
.
Tutorial steps
Writing a custom training script that creates your train & test datasets and trains the model.
Running a
CustomTrainingJob
using Vertex AI SDK for Python.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Distributed training
Using PyTorch torchrun to simplify multi-node training with custom containers
.
Learn how to train an Imagenet model using PyTorch's Torchrun on multiple nodes.


 Learn more about
Distributed training
.
Tutorial steps
Create a shell script to start an ETCD cluster on the master node
Create a training script using code from PyTorch Elastic's GitHub repository
Create containers that download the data, and start an ETCD cluster on the host
Train the model using multiple nodes with GPUs
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Custom training
Distributed XGBoost training with Dask
.
Learn how to create a distributed training job using XGBoost with Dask.


 Learn more about
Custom training
.
Tutorial steps
Configure the
PROJECT_ID
and
LOCATION
variables for your Google Cloud project.
Create a Cloud Storage bucket to store your model artifacts.
Build a custom Docker container that hosts your training code and push the container image to Artifact Registry.
Run a Vertex AI SDK CustomContainerTrainingJob
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
vector_search
Using Vertex AI Multimodal Embeddings and Vector Search
.
Learn how to encode custom text embeddings, create an  Approximate Nearest Neighbor  index, and query against indexes.
Tutorial steps
Convert an image dataset to embeddings.
Create an index.
Upload embeddings to the index.
Create an index endpoint.
Deploy the index to the index endpoint.
Perform an online query.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Vector Search
Using Vertex AI Vector Search for StackOverflow Questions
.
Learn how to encode custom text embeddings, create an Approximate Nearest Neighbor  index, and query against indexes.


 Learn more about
Vertex AI Vector Search
.
Tutorial steps
Create ANN index.
Create an index endpoint with VPC Network.
Deploy ANN index.
Perform online query.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Vector Search
Vertex AI embeddings for text
Using Vertex AI Vector Search and Vertex AI embeddings for text for StackOverflow Questions
.
Learn how to encode text embeddings, create an Approximate Nearest Neighbor  index, and query against indexes.


 Learn more about
Vertex AI Vector Search
.

 Learn more about
Vertex AI embeddings for text
.
Tutorial steps
Convert a BigQuery dataset to embeddings.
Create an index.
Upload embeddings to the index.
Create an index endpoint.
Deploy the index to the index endpoint.
Perform an online query.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Vector Search
Create Vertex AI Vector Search index
.
Learn how to create Approximate Nearest Neighbor  Index, query against indexes, and validate the performance of the index.


 Learn more about
Vertex AI Vector Search
.
Tutorial steps
Create ANN Index and Brute Force Index.
Create an IndexEndpoint with VPC Network.
Deploy ANN Index and Brute Force Index.
Perform online query.
Compute recall.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Vizier
Optimizing multiple objectives with Vertex AI Vizier
.
Learn how to use Vertex AI Vizier to optimize a multi-objective study.


 Learn more about
Vertex AI Vizier
.
Tutorial steps
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Vizier
Get started with Vertex AI Vizier
.
Learn how to use Vertex AI Vizier when training with Vertex AI.


 Learn more about
Vertex AI Vizier
.
Tutorial steps
Hyperparameter tuning with Random algorithm.
Hyperparameter tuning with Vertex AI Vizier (Bayesian) algorithm.
Suggesting trials and updating results for Vertex AI Vizier study
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Workbench
Vertex AI Training
Train a multi-class classification model for ads-targeting
.
Learn how to collect data from BigQuery, preprocess it, and train a multi-class classification model on an e-commerce dataset.


 Learn more about
Vertex AI Workbench
.

 Learn more about
Vertex AI Training
.
Tutorial steps
Fetch the required data from BigQuery
Preprocess the data
Train a TensorFlow (>=2.4) classification model
Evaluate the loss for the trained model
Automate the notebook execution using the executor feature
Save the model to a Cloud Storage path
Clean up the created resources
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Workbench
Vertex Explainable AI
Taxi fare prediction using the Chicago Taxi Trips dataset
.
The goal of this notebook is to provide an overview on Vertex AI features like Vertex Explainable AI and BigQuery in Notebooks by trying to solve a taxi fare prediction problem.


 Learn more about
Vertex AI Workbench
.

 Learn more about
Vertex Explainable AI
.
Tutorial steps
Loading the dataset using "BigQuery in Notebooks".
Performing exploratory data analysis on the dataset.
Feature selection and preprocessing.
Building a linear regression model using scikitlearn.
Configuring the model for Vertex Explainable AI.
Deploying the model to Vertex AI.
Testing the deployed model.
Clean up.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Workbench
BigQuery ML
Forecasting retail demand with Vertex AI and BigQuery ML
.
Learn how to build ARIMA (Autoregressive integrated moving average) model from BigQuery ML on retail data


 Learn more about
Vertex AI Workbench
.

 Learn more about
BigQuery ML
.
Tutorial steps
Explore data
Model with BigQuery and the ARIMA model
Evaluate the model
Evaluate the model results using BigQuery ML (on training data)
Evaluate the model results  MAE, MAPE, MSE, RMSE (on test data)
Use the executor feature
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Workbench
BigQuery ML
Interactive exploratory analysis of BigQuery data in a notebook
.
Learn about various ways to explore and gain insights from BigQuery data in a Jupyter notebook environment.


 Learn more about
Vertex AI Workbench
.

 Learn more about
BigQuery ML
.
Tutorial steps
Using Python & SQL to query public data in BigQuery
Exploring the dataset using BigQuery INFORMATION_SCHEMA
Creating interactive elements to help explore interesting parts of the data
Doing some exploratory correlation and time series analysis
Creating static and interactive outputs (data tables and plots) in the notebook
Saving some outputs to Cloud Storage
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Workbench
Custom training
Build a fraud detection model on Vertex AI
.
This tutorial demonstrates data analysis and model-building using a synthetic financial dataset.


 Learn more about
Vertex AI Workbench
.

 Learn more about
Custom training
.
Tutorial steps
Installation of required libraries
Reading the dataset from a Cloud Storage bucket
Performing exploratory analysis on the dataset
Preprocessing the dataset
Training a random forest model using scikitlearn
Saving the model to a Cloud Storage bucket
Creating a Vertex AI model resource and deploying to an endpoint
Running the WhatIf Tool on test data
Undeploying the model and cleaning up the model resources
Colab
GitHub
Vertex AI Workbench
Vertex AI Workbench
BigQuery ML
Churn prediction for game developers using Google Analytics 4 and BigQuery ML
.
Learn how to train, evaluate a propensity model in BigQuery ML.


 Learn more about
Vertex AI Workbench
.

 Learn more about
BigQuery ML
.
Tutorial steps
Explore the data exported from Google Analytics 4 in BigQuery.
Prepare the training data using demographic, behavioral data, and labels (churn/notchurn).
Train an XGBoost model using BigQuery ML.
Evaluate the model using BigQuery ML.
Use BigQuery ML to predict which users are likely to churn.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Workbench
Vertex AI training
Predictive maintenance using Vertex AI
.
Learn how to use the executor feature of Vertex AI Workbench to automate a workflow to train and deploy a model.


 Learn more about
Vertex AI Workbench
.

 Learn more about
Vertex AI training
.
Tutorial steps
Loading the required dataset from a Cloud Storage bucket.
Analyzing the fields present in the dataset.
Selecting the required data for the predictive maintenance model.
Training an XGBoost regression model for predicting the remaining useful life.
Evaluating the model.
Running the notebook endtoend as a training job using Executor.
Deploying the model on Vertex AI.
Clean up.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Workbench
BigQuery ML
Analysis of pricing optimization on CDM Pricing Data
.
The objective of this notebook is to build a pricing optimization model using BigQuery ML.


 Learn more about
Vertex AI Workbench
.

 Learn more about
BigQuery ML
.
Tutorial steps
Load the required dataset from a Cloud Storage bucket.
Analyze the fields present in the dataset.
Process the data to build a model.
Build a BigQuery ML forecast model on the processed data.
Get forecasted values from the BigQuery ML model.
Interpret the forecasts to identify the best prices.
Clean up.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Workbench
Dataproc Serverless for Spark
Digest and analyze data from BigQuery with Dataproc
.
This notebook tutorial runs an Apache Spark job that fetches data from the BigQuery "GitHub Activity Data" dataset, queries the data, and then writes the results back to BigQuery.


 Learn more about
Vertex AI Workbench
.

 Learn more about
Dataproc Serverless for Spark
.
Tutorial steps
Setting up a Google Cloud project and Dataproc cluster.
Configuring the sparkbigqueryconnector.
Ingesting data from BigQuery into a Spark DataFrame.
Preprocessing ingested data.
Querying the most frequently used programming language in monoglot repos.
Querying the average size (MB) of code in each language stored in monoglot repos.
Querying the languages files most frequently found together in polyglot repos.
Writing the query results back into BigQuery.
Deleting the resources created for this notebook tutorial.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Vertex AI Workbench
Dataproc
SparkML with Dataproc and BigQuery
.
This tutorial runs an Apache SparkML job that fetches data from the BigQuery dataset, performs exploratory data analysis, cleans the data, executes feature engineering, trains the model, evaluates the model, outputs results, and saves the model to a Cloud Storage bucket.


 Learn more about
Vertex AI Workbench
.

 Learn more about
Dataproc
.
Tutorial steps
Sets up a Google Cloud project and Dataproc cluster.
Creates a Cloud Storage bucket and a BigQuery dataset.
Configures the sparkbigqueryconnector.
Ingests BigQuery data into a Spark DataFrame.
Performa Exploratory Data Analysis (EDA).
Visualizes the data with samples.
Cleans the data.
Selects features.
Trains the model.
Outputs results.
Saves the model to a Cloud Storage bucket.
Deletes the resources created for the tutorial.
Colab
Colab Enterprise
GitHub
Vertex AI Workbench
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/reference.txt
Notebooks API usage overview  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Vertex AI
Overview
Authenticate to Vertex AI
Vertex AI SDK for Python
Introduction to the Vertex AI SDK
Install the Vertex AI SDK
Vertex AI SDK reference
Vertex AI language model SDK reference
gcloud CLI reference
gcloud ai
gcloud beta ai
gcloud colab
gcloud beta colab
Client libraries
Install the client libraries
C# reference
Go reference
Java reference
Node.js reference
Long-running operations
REST reference
All methods
v1
REST Resources
media
Overview
upload
projects.locations
Overview
augmentPrompt
corroborateContent
deploy
evaluateInstances
getRagEngineConfig
retrieveContexts
updateRagEngineConfig
projects.locations.batchPredictionJobs
Overview
cancel
create
delete
get
list
projects.locations.cachedContents
Overview
create
delete
get
list
patch
projects.locations.customJobs
Overview
cancel
create
delete
get
list
projects.locations.datasets
Overview
create
delete
export
get
import
list
patch
searchDataItems
projects.locations.datasets.annotationSpecs
Overview
get
projects.locations.datasets.dataItems
Overview
list
projects.locations.datasets.dataItems.annotations
Overview
list
projects.locations.datasets.datasetVersions
Overview
create
delete
get
list
patch
restore
projects.locations.datasets.savedQueries
Overview
delete
list
projects.locations.deploymentResourcePools
Overview
create
delete
get
list
patch
queryDeployedModels
projects.locations.endpoints
Overview
create
delete
deployModel
directPredict
directRawPredict
explain
get
list
mutateDeployedModel
patch
predict
predictLongRunning
rawPredict
serverStreamingPredict
streamRawPredict
undeployModel
update
projects.locations.featureGroups
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureGroups.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.featureOnlineStores
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureOnlineStores.featureViews
Overview
create
delete
directWrite
fetchFeatureValues
get
getIamPolicy
list
patch
searchNearestEntities
setIamPolicy
sync
testIamPermissions
projects.locations.featureOnlineStores.featureViews.featureViewSyncs
Overview
get
list
projects.locations.featurestores
Overview
batchReadFeatureValues
create
delete
get
getIamPolicy
list
patch
searchFeatures
setIamPolicy
testIamPermissions
projects.locations.featurestores.entityTypes
Overview
create
delete
deleteFeatureValues
exportFeatureValues
get
getIamPolicy
importFeatureValues
list
patch
readFeatureValues
setIamPolicy
streamingReadFeatureValues
testIamPermissions
writeFeatureValues
projects.locations.featurestores.entityTypes.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.hyperparameterTuningJobs
Overview
cancel
create
delete
get
list
projects.locations.indexEndpoints
Overview
create
delete
deployIndex
get
list
mutateDeployedIndex
patch
undeployIndex
projects.locations.indexes
Overview
create
delete
get
list
patch
removeDatapoints
upsertDatapoints
projects.locations.metadataStores
Overview
create
delete
get
list
projects.locations.metadataStores.artifacts
Overview
create
delete
get
list
patch
purge
queryArtifactLineageSubgraph
projects.locations.metadataStores.contexts
Overview
addContextArtifactsAndExecutions
addContextChildren
create
delete
get
list
patch
purge
queryContextLineageSubgraph
removeContextChildren
projects.locations.metadataStores.executions
Overview
addExecutionEvents
create
delete
get
list
patch
purge
queryExecutionInputsAndOutputs
projects.locations.metadataStores.metadataSchemas
Overview
create
get
list
projects.locations.migratableResources
Overview
batchMigrate
search
projects.locations.modelDeploymentMonitoringJobs
Overview
create
delete
get
list
patch
pause
resume
searchModelDeploymentMonitoringStatsAnomalies
projects.locations.models
Overview
copy
delete
deleteVersion
export
get
getIamPolicy
list
listCheckpoints
listVersions
mergeVersionAliases
patch
setIamPolicy
testIamPermissions
updateExplanationDataset
upload
projects.locations.models.evaluations
Overview
get
import
list
projects.locations.models.evaluations.slices
Overview
batchImport
get
list
projects.locations.nasJobs
Overview
cancel
create
delete
get
list
projects.locations.nasJobs.nasTrialDetails
Overview
get
list
projects.locations.notebookExecutionJobs
Overview
create
delete
get
list
projects.locations.notebookRuntimeTemplates
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.notebookRuntimes
Overview
assign
delete
get
list
start
stop
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
wait
projects.locations.persistentResources
Overview
create
delete
get
list
patch
reboot
projects.locations.pipelineJobs
Overview
batchCancel
batchDelete
cancel
create
delete
get
list
projects.locations.publishers.models
Overview
predict
predictLongRunning
rawPredict
serverStreamingPredict
streamRawPredict
projects.locations.ragCorpora
Overview
create
delete
get
list
patch
projects.locations.ragCorpora.ragFiles
Overview
delete
get
import
list
projects.locations.reasoningEngines
Overview
create
delete
get
list
patch
query
streamQuery
projects.locations.schedules
Overview
create
delete
get
list
patch
pause
resume
projects.locations.specialistPools
Overview
create
delete
get
list
patch
projects.locations.studies
Overview
create
delete
get
list
lookup
projects.locations.studies.trials
Overview
addTrialMeasurement
checkTrialEarlyStoppingState
complete
create
delete
get
list
listOptimalTrials
stop
suggest
projects.locations.tensorboards
Overview
batchRead
create
delete
get
list
patch
readSize
readUsage
projects.locations.tensorboards.experiments
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs.timeSeries
Overview
create
delete
exportTensorboardTimeSeries
get
list
patch
read
readBlobData
projects.locations.trainingPipelines
Overview
cancel
create
delete
get
list
projects.locations.tuningJobs
Overview
cancel
create
get
list
rebaseTunedModel
publishers.models
Overview
get
Types
Annotation
ApiKeyConfig
AutomaticResources
BigQueryDestination
BigQuerySource
Content
CsvSource
CustomJobSpec
DataItem
DedicatedResources
DestinationFeatureSetting
DiskSpec
EncryptionSpec
EnvVar
Event
Explanation
ExplanationSpec
Fact
FeatureSelector
FeatureValue
FeatureValueDestination
FeatureViewDataKey
FetchFeatureValuesResponse
GcsDestination
GcsSource
JobState
LineageSubgraph
ListFeaturesResponse
MachineSpec
Measurement
ModelContainerSpec
ModelExplanation
NasTrial
NetworkSpec
NotebookEucConfig
NotebookExecutionJobView
NotebookIdleShutdownConfig
NotebookRuntimeType
NotebookSoftwareConfig
PSCAutomationConfig
PersistentDiskSpec
PipelineState
PredictResponse
PredictSchemata
PrivateServiceConnectConfig
PscInterfaceConfig
RagChunk
RagEngineConfig
RagFileTransformationConfig
ReadFeatureValuesResponse
ShieldedVmConfig
StreamingPredictResponse
StudySpec
Tensor
TensorboardBlob
TimeSeriesData
TimeSeriesDataPoint
v1beta1
REST Resources
media
Overview
upload
projects
Overview
fetchPublisherModelConfig
setPublisherModelConfig
projects.locations
Overview
augmentPrompt
corroborateContent
deploy
deployPublisherModel
evaluateDataset
evaluateInstances
getRagEngineConfig
recommendSpec
retrieveContexts
updateRagEngineConfig
projects.locations.batchPredictionJobs
Overview
cancel
create
delete
get
list
projects.locations.cachedContents
Overview
create
delete
get
list
patch
projects.locations.customJobs
Overview
cancel
create
delete
get
list
projects.locations.datasets
Overview
assemble
assess
create
delete
export
get
import
list
patch
searchDataItems
projects.locations.datasets.annotationSpecs
Overview
get
projects.locations.datasets.dataItems
Overview
list
projects.locations.datasets.dataItems.annotations
Overview
list
projects.locations.datasets.datasetVersions
Overview
create
delete
get
list
patch
restore
projects.locations.datasets.savedQueries
Overview
delete
list
projects.locations.deploymentResourcePools
Overview
create
delete
get
list
patch
queryDeployedModels
projects.locations.endpoints
Overview
countTokens
create
delete
deployModel
directPredict
directRawPredict
explain
get
getIamPolicy
list
mutateDeployedModel
patch
predict
rawPredict
serverStreamingPredict
setIamPolicy
streamRawPredict
testIamPermissions
undeployModel
update
projects.locations.endpoints.chat
Overview
completions
projects.locations.exampleStores
Overview
create
delete
fetchExamples
get
list
patch
removeExamples
searchExamples
upsertExamples
projects.locations.extensions
Overview
delete
execute
get
import
list
patch
query
projects.locations.featureGroups
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureGroups.featureMonitors
Overview
create
delete
get
list
patch
projects.locations.featureGroups.featureMonitors.featureMonitorJobs
Overview
create
get
list
projects.locations.featureGroups.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.featureOnlineStores
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.featureOnlineStores.featureViews
Overview
create
delete
directWrite
fetchFeatureValues
get
getIamPolicy
list
patch
searchNearestEntities
setIamPolicy
streamingFetchFeatureValues
sync
testIamPermissions
projects.locations.featureOnlineStores.featureViews.featureViewSyncs
Overview
get
list
projects.locations.featurestores
Overview
batchReadFeatureValues
create
delete
get
getIamPolicy
list
patch
searchFeatures
setIamPolicy
testIamPermissions
projects.locations.featurestores.entityTypes
Overview
create
delete
deleteFeatureValues
exportFeatureValues
get
getIamPolicy
importFeatureValues
list
patch
readFeatureValues
setIamPolicy
streamingReadFeatureValues
testIamPermissions
writeFeatureValues
projects.locations.featurestores.entityTypes.features
Overview
batchCreate
create
delete
get
list
patch
projects.locations.hyperparameterTuningJobs
Overview
cancel
create
delete
get
list
projects.locations.indexEndpoints
Overview
create
delete
deployIndex
get
list
mutateDeployedIndex
patch
undeployIndex
projects.locations.indexes
Overview
create
delete
get
import
list
patch
removeDatapoints
upsertDatapoints
projects.locations.metadataStores
Overview
create
delete
get
list
projects.locations.metadataStores.artifacts
Overview
create
delete
get
list
patch
purge
queryArtifactLineageSubgraph
projects.locations.metadataStores.contexts
Overview
addContextArtifactsAndExecutions
addContextChildren
create
delete
get
list
patch
purge
queryContextLineageSubgraph
removeContextChildren
projects.locations.metadataStores.executions
Overview
addExecutionEvents
create
delete
get
list
patch
purge
queryExecutionInputsAndOutputs
projects.locations.metadataStores.metadataSchemas
Overview
create
get
list
projects.locations.migratableResources
Overview
batchMigrate
search
projects.locations.modelDeploymentMonitoringJobs
Overview
create
delete
get
list
patch
pause
resume
searchModelDeploymentMonitoringStatsAnomalies
projects.locations.modelMonitors
Overview
create
delete
get
list
patch
searchModelMonitoringAlerts
searchModelMonitoringStats
projects.locations.modelMonitors.modelMonitoringJobs
Overview
create
delete
get
list
projects.locations.models
Overview
copy
delete
deleteVersion
export
get
getIamPolicy
list
listCheckpoints
listVersions
mergeVersionAliases
patch
setIamPolicy
testIamPermissions
updateExplanationDataset
upload
projects.locations.models.evaluations
Overview
get
import
list
projects.locations.models.evaluations.slices
Overview
batchImport
get
list
projects.locations.notebookExecutionJobs
Overview
create
delete
get
list
projects.locations.notebookRuntimeTemplates
Overview
create
delete
get
getIamPolicy
list
patch
setIamPolicy
testIamPermissions
projects.locations.notebookRuntimes
Overview
assign
delete
get
list
start
stop
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
wait
projects.locations.persistentResources
Overview
create
delete
get
list
patch
reboot
projects.locations.pipelineJobs
Overview
batchCancel
batchDelete
cancel
create
delete
get
list
projects.locations.publishers.models
Overview
countTokens
export
fetchPublisherModelConfig
getIamPolicy
predict
rawPredict
serverStreamingPredict
setPublisherModelConfig
streamRawPredict
projects.locations.ragCorpora
Overview
create
delete
get
list
patch
projects.locations.ragCorpora.ragFiles
Overview
delete
get
import
list
projects.locations.reasoningEngines
Overview
create
delete
get
list
patch
query
streamQuery
projects.locations.reasoningEngines.memories
Overview
create
delete
generate
get
list
patch
retrieve
projects.locations.reasoningEngines.sessions
Overview
appendEvent
create
delete
get
list
patch
projects.locations.reasoningEngines.sessions.events
Overview
list
projects.locations.schedules
Overview
create
delete
get
list
patch
pause
resume
projects.locations.specialistPools
Overview
create
delete
get
list
patch
projects.locations.studies
Overview
create
delete
get
list
lookup
projects.locations.studies.trials
Overview
addTrialMeasurement
checkTrialEarlyStoppingState
complete
create
delete
get
list
listOptimalTrials
stop
suggest
projects.locations.tensorboards
Overview
batchRead
create
delete
get
list
patch
readSize
readUsage
projects.locations.tensorboards.experiments
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs
Overview
batchCreate
create
delete
get
list
patch
write
projects.locations.tensorboards.experiments.runs.timeSeries
Overview
create
delete
exportTensorboardTimeSeries
get
list
patch
read
readBlobData
projects.locations.trainingPipelines
Overview
cancel
create
delete
get
list
projects.locations.tuningJobs
Overview
cancel
create
get
list
rebaseTunedModel
projects.modelGardenEula
Overview
accept
check
publishers.models
Overview
get
list
Types
Annotation
ApiKeyConfig
AutomaticResources
BatchDedicatedResources
BigQueryDestination
BigQuerySource
Content
CountTokensResponse
CsvSource
CustomJobSpec
DataItem
DedicatedResources
DestinationFeatureSetting
DiskSpec
EncryptionSpec
EnvVar
Event
Example
ExamplesArrayFilter
Explanation
ExplanationSpec
Fact
FeatureSelectionConfig
FeatureSelector
FeatureStatsAndAnomaly
FeatureStatsAndAnomalySpec
FeatureStatsAnomaly
FeatureValue
FeatureValueDestination
FeatureViewDataFormat
FeatureViewDataKey
FeaturestoreMonitoringConfig
FetchFeatureValuesResponse
FlexStart
FunctionDeclaration
GcsDestination
GcsSource
GeminiRequestReadConfig
JobState
LineageSubgraph
ListFeaturesResponse
MachineSpec
ModelContainerSpec
ModelExplanation
ModelMonitoringAlertConfig
ModelMonitoringInput
ModelMonitoringNotificationSpec
ModelMonitoringObjectiveConfig
ModelMonitoringOutputSpec
NetworkSpec
NfsMount
NotebookEucConfig
NotebookExecutionJobView
NotebookIdleShutdownConfig
NotebookRuntimeType
NotebookSoftwareConfig
PSCAutomationConfig
PersistentDiskSpec
PipelineState
PredictResponse
PredictSchemata
PrivateServiceConnectConfig
PscInterfaceConfig
PublisherModelConfig
PublisherModelEulaAcceptance
PublisherModelView
RagChunk
RagEngineConfig
RagFileChunkingConfig
RagFileMetadataConfig
RagFileParsingConfig
RagFileTransformationConfig
ReadFeatureValuesResponse
SamplingStrategy
Schema
SearchKeyGenerationMethod
SessionEvent
ShieldedVmConfig
SliceSpec
StoredContentsExampleFilter
StreamingPredictResponse
StudySpec
TabularObjective
Tensor
TensorboardBlob
ThresholdConfig
TimeSeriesData
TimeSeriesDataPoint
Shared types
Types
Binding
CancelOperationRequest
DeleteOperationRequest
GetIamPolicyRequest
GetOperationRequest
HttpBody
Interval
LatLng
ListOperationsRequest
ListOperationsResponse
Policy
SetIamPolicyRequest
TestIamPermissionsRequest
TestIamPermissionsResponse
WaitOperationRequest
RPC reference
Overview
cloud.ai.large_models.vision
google.api
google.cloud.aiplatform.v1
Overview
schema
Overview
modelevaluation.metrics
predict.instance
predict.params
predict.prediction
trainingjob.definition
google.cloud.aiplatform.v1beta1
Overview
schema
Overview
modelevaluation.metrics
predict.instance
predict.params
predict.prediction
trainingjob.definition
google.iam.v1
google.longrunning
google.rpc
google.type
Google Cloud Pipelines Components reference
GCPC SDK reference
ML metadata artifact types
Vertex AI AutoML components
Batch prediction components
BigQuery ML components
CustomJob components
Dataflow components
Dataproc Serverless components
Dataset components
Forecasting components
Hyperparameter tuning components
Model and endpoint components
Model evaluation components
Email notification component
Vertex AI Neural Architecture Search reference
Prebuilt search spaces
PyGlove reference
Client library
Vertex AI Workbench: Notebooks API
Overview
Authenticate to Vertex AI Workbench
gcloud CLI reference
gcloud workbench
gcloud notebooks
gcloud beta notebooks
Client libraries
Overview
C++ reference
C# reference
Go reference
Java reference
Node.js reference
PHP reference
Python reference
Ruby reference
REST reference
Overview
v2
REST Resources
projects.locations
Overview
get
list
projects.locations.instances
Overview
checkUpgradability
create
delete
diagnose
get
getConfig
getIamPolicy
list
patch
reset
resizeDisk
restore
rollback
setIamPolicy
start
stop
testIamPermissions
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
v1
REST Resources
projects.locations
Overview
get
list
projects.locations.environments
Overview
create
delete
get
list
projects.locations.executions
Overview
create
delete
get
list
projects.locations.instances
Overview
create
delete
diagnose
get
getIamPolicy
getInstanceHealth
isUpgradeable
list
migrate
register
report
reset
rollback
setAccelerator
setIamPolicy
setLabels
setMachineType
start
stop
testIamPermissions
updateConfig
updateMetadataItems
updateShieldedInstanceConfig
upgrade
projects.locations.operations
Overview
cancel
delete
get
list
projects.locations.runtimes
Overview
create
delete
get
getIamPolicy
list
migrate
patch
reportEvent
reset
setIamPolicy
start
stop
switch
testIamPermissions
projects.locations.schedules
Overview
create
delete
get
list
Types
ContainerImage
ExecutionTemplate
UpgradeType
VmImage
Shared types
Types
Binding
CancelOperationRequest
DeleteOperationRequest
GetIamPolicyRequest
GetLocationRequest
GetOperationRequest
ListLocationsRequest
ListLocationsResponse
ListOperationsRequest
ListOperationsResponse
Policy
SetIamPolicyRequest
TestIamPermissionsRequest
TestIamPermissionsResponse
RPC reference
Overview
google.cloud.common
google.cloud.location
google.cloud.notebooks.v1
google.cloud.notebooks.v2
google.iam.v1
google.longrunning
google.rpc
google.type
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Reference
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Notebooks API usage overview
This guide provides an overview of using the Notebooks API
  and its reference documentation.
REST, gRPC, and client libraries
You can access the API via
  
  REST, gRPC, or one of the provided client libraries (built on gRPC).
Client libraries
Google provides
client libraries
for many popular languages to access this API.
  If your desired programming language is supported by the client libraries,
  you should use this option.
Pros
Cons
Maintained by Google.
Built-in
authentication
.
Built-in retries.
Idiomatic for each language.
Efficient
protocol buffer
HTTP request body.
Not available for all programming languages.
REST
This API supports
REST
.
  See the
REST reference
for this API.
  Also see
How to call Google APIs: REST edition
.
Pros
Cons
Simple JSON interface.
Well supported by many Google and third-party tools and libraries.
You must build your own client.
You must
implement authentication
.
You must implement retries.
Less efficient JSON HTTP request body.
REST streaming is not supported by this API.
gRPC
This API supports
gRPC
.
  See the
RPC reference
for this API,
  which provides a generic description of the types, methods,
  and fields generated for a gRPC library.
  Also see
How to call Google APIs: RPC edition
.
Pros
Cons
Supports
many programming languages
.
Efficient
protocol buffer
HTTP request body.
You must generate your own client from Google-supplied protocol buffers.
You must
implement authentication
.
You must implement retries.
Type, method, and field names
Depending on whether you are using
  
  client libraries, REST, or gRPC,
  
  the type, method, and field names for the API vary somewhat:
REST is arranged by resource hierarchies and their methods.
Client libraries and gRPC are
    
    arranged by services and their methods.
REST field names use camel case,
    though the API service will accept either camel case or snake case.
gRPC field names use snake case.
Client library field names use either title case, camel case or snake case,
    depending on which name is idiomatic for the language.
Protocol buffers
Whether you are using
  
  client libraries, REST, or gRPC,
  
  the underlying service is defined using
protocol buffers
.
  In particular, the service uses
proto3
.
When calling the API,
  some request or response fields can require a basic understanding of
protocol buffer well-known types
.
In addition, when calling the REST API, the
default value
behavior for protocol buffers may result in missing fields in a JSON response.
  These fields are simply set to the default value,
  so they are not included in the response.
API versions
The following API versions are available:
v2
(
generally available
) is for
managing Vertex AI Workbench instances.
v1
(
generally available
) is for
managing user-managed notebooks and managed notebooks instances.
v1beta1
is scheduled for removal.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/release-notes.txt
Vertex AI Workbench release notes  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Vertex AI Workbench release notes
Stay organized with collections
Save and categorize content based on your preferences.
This page documents production updates to Vertex AI Workbench. Check this page for
announcements about new or updated features, bug fixes, known issues, and
deprecated functionality.

Vertex AI Workbench is a component of Vertex AI. For information
on all Vertex AI releases, see the
Vertex AI release notes
.
You can see the latest product updates for all of Google Cloud on the
Google Cloud
page, browse and filter all release notes in the
Google Cloud console
,
        or programmatically access release notes in
BigQuery
.
To get the latest product updates delivered to you, add the URL of this page to your
feed
          reader
, or add the
feed URL
directly.
October 09, 2025
v2
Feature
M134 release
The M134 release of Vertex AI Workbench instances includes the following:
Patched a regression with custom notebook metrics reporting (for example,
jupyterlab_kernels
and
docker_status metrics
).
Updated the Dataproc JupyterLab plugin (
dataproc-jupyter-plugin
) to
version 0.1.92.
When using Google Cloud CLI commands, the
project
and
region
properties
are preset.
October 01, 2025
v2
Feature
Generally available (GA)
:
You can use
Workforce Identity Federation
with Vertex AI Workbench instances. Workforce Identity Federation lets
you create and manage Vertex AI Workbench instances with credentials
provided by an external identity provider (IdP). For more information, see
Create an instance with third party credentials
.
September 17, 2025
v2
Feature
M133 release
The M133 release of Vertex AI Workbench instances includes the following:
Patched an incompatibility with the Dataproc JupyterLab plugin (
dataproc-jupyter-plugin
) and instances with end-user credentials enabled.
August 29, 2025
v2
Feature
M132 release
The M132 release of Vertex AI Workbench instances includes the following:
The new scheduler Jupyter plugin (
scheduler-jupyter-plugin
) is now preinstalled in the Jupyterlab 4 environment, with support for both the Cloud Composer and Vertex AI notebook schedulers.
Updated the Dataproc JupyterLab plugin (
dataproc-jupyter-plugin
) to version 0.1.90.
Patched bugs related to the managed end user credentials feature (Preview), resolving an incompatibility with listing Dataproc remote kernels.
Patched a bug that caused instances with disabled proxy access to get stuck in provisioning.
Removed the archived Debian 11 backports repository, resolving an issue with running
apt update
within the instance.
August 05, 2025
v2
Feature
Generally available
: You can consume reservations with Vertex AI Workbench instances. Reservations of Compute Engine zonal resources help you gain a high level of assurance that your jobs have the necessary resources to run. For more information, see
Use reservations with Vertex AI Workbench instances
.
July 10, 2025
v2
Feature
M131 release
The M131 release of Vertex AI Workbench instances includes the following:
Updated the Dataproc JupyterLab plugin to version 0.1.89.
June 26, 2025
v2
Feature
M130 release
The M130 release of Vertex AI Workbench instances includes the following:
Updated the Dataproc JupyterLab plugin to version 0.1.87.
Added the BigQuery JupyterLab plugin, version 0.0.1.
The
GOOGLE_CLOUD_REGION
environment variable is now set by default.
June 10, 2025
v2
Feature
Available in
Preview
: You can consume reservations with Vertex AI Workbench instances. Reservations of Compute Engine zonal resources help you gain a high level of assurance that your jobs have the necessary resources to run. For more information, see
Use reservations with Vertex AI Workbench instances
.
April 16, 2025
v2
Feature
M129 release
The M129 release of Vertex AI Workbench instances includes the following:
Updated the Dataproc JupyterLab plugin to version 0.1.85.
March 26, 2025
v2
Feature
The ability to back up and restore data on a Vertex AI Workbench instance is now
generally available
. For more information, see
Back up and restore data on an instance
.
March 20, 2025
v2
Feature
Encrypt your data-in-use by using
Confidential Computing
. This feature is now available in
Preview
. You can enable the Confidential VM service when you create a Vertex AI Workbench instance. To get started, see
Create an instance with Confidential Computing
.
March 12, 2025
v1
Feature
M128 release
The M128 release of Vertex AI Workbench user-managed notebooks includes the following:
Miscellaneous package updates.
v1
Feature
The M128 release of Vertex AI Workbench managed notebooks includes the following:
Miscellaneous package updates.
v2
Feature
M128 release
The M128 release of Vertex AI Workbench instances includes the following:
Miscellaneous package updates.
January 16, 2025
v1
Feature
M127 release
The M127 release of Vertex AI Workbench user-managed notebooks includes the following:
Fixed an issue related to ownership of the home directory when using authorized ssh keys.
v1
Feature
The M127 release of Vertex AI Workbench managed notebooks includes the following:
Fixed an issue related to ownership of the home directory when using authorized ssh keys.
v2
Feature
M127 release
The M127 release of Vertex AI Workbench instances includes the following:
Fixed an issue related to ownership of the home directory when using authorized ssh keys.
November 20, 2024
v1
Feature
M126 release
The M126 release of Vertex AI Workbench user-managed notebooks includes the following:
Upgraded JupyterLab to 3.6.8.
One or more framework versions have reached their end of patch and support dates. To view end of patch and support dates, see
Supported framework versions
.
v1
Feature
The M126 release of Vertex AI Workbench managed notebooks includes the following:
Upgraded JupyterLab to 3.6.8.
v2
Feature
M126 release
The M126 release of Vertex AI Workbench instances includes the following:
Preview
: JupyterLab 4+ is available on new Vertex AI Workbench instances. To try it, select JupyterLab 4 when you
create your instance
.
Upgraded JupyterLab to 3.6.8.
September 26, 2024
v1
Feature
M125 release
The M125 release of Vertex AI Workbench user-managed notebooks includes the following:
Patched a vulnerability with
adm
and
docker
permissions when the instance's root access isn't enabled.
v1
Feature
The M125 release of Vertex AI Workbench managed notebooks includes the following:
Patched a vulnerability with
adm
and
docker
permissions when the instance's root access isn't enabled.
v2
Feature
M125 release
The M125 release of Vertex AI Workbench instances includes the following:
bigframes
1.9.0 is now available in all environments except TensorFlow.
Fixed a regression introduced in M124 where Conda was getting downgraded to an older version.
Patched a vulnerability with
adm
and
docker
permissions when the instance's root access isn't enabled.
September 10, 2024
v2
Feature
The ability to back up and restore data on a Vertex AI Workbench instance is now available in
Preview
. For more information, see
Back up and restore an instance
.
August 20, 2024
v1
Feature
M124 release
The M124 release of Vertex AI Workbench user-managed notebooks includes the following:
Pytorch 2.3.0 with CUDA 12.1 and Python 3.10 user-managed notebooks instances are now available.
Fixed a bug that prevented kernels from appearing when the Cloud Resource Manager API is turned off and Dataproc is enabled.
August 19, 2024
v2
Announcement
The ability to create a Vertex AI Workbench instance based on a custom container is now
generally available
. Only custom containers derived from the Google-provided base container are supported. For more information, see
Create an instance using a custom container
.
August 08, 2024
v1
Feature
M124 release
The M124 release of Vertex AI Workbench managed notebooks includes the following:
Fixed a bug that prevented kernels from appearing when the Cloud Resource Manager API is turned off and Dataproc is enabled.
v2
Feature
M124 release
The M124 release of Vertex AI Workbench instances includes the following:
Fixed a bug that prevented kernels from appearing when the Cloud Resource Manager API is turned off and Dataproc is enabled.
Spark notebooks on Dataproc: The Serverless Spark runtime template creation screen now has an easy-to-use UI for configuring resource allocation, autoscaling, and GPU settings.
July 24, 2024
v1
Feature
M123 release
The M123 release of Vertex AI Workbench managed notebooks includes the following:
Fixed a bug that caused conflicting permissions with the Jupyter user and google-sudoers.
Updated Nvidia drivers to version 550.90.07 to fix vulnerabilities.
July 16, 2024
v2
Feature
M123 release
The M123 release of Vertex AI Workbench instances includes the following:
Fixed a bug that caused conflicting permissions with the Jupyter user and google-sudoers.
v1
Feature
M123 release
The M123 release of Vertex AI Workbench user-managed notebooks includes the following:
Fixed a bug that caused conflicting permissions with the Jupyter user and google-sudoers.
Fixed a bug for custom container instances using a disabled root.
June 21, 2024
v2
Feature
M122 release
The M122 release of Vertex AI Workbench instances includes the following:
Updated Nvidia drivers to version 550.90.07 to fix vulnerabilities.
v1
Feature
M122 release
The M122 release of Vertex AI Workbench user-managed notebooks includes the following:
Updated Nvidia drivers to version 550.90.07 to fix vulnerabilities.
June 07, 2024
v2
Feature
You can now create a Vertex AI Workbench instance based on a custom container. This feature is available in
Preview
. Only custom containers derived from the Google-provided base container are supported. For more information, see
Create an instance using a custom container
.
June 03, 2024
v2
Feature
You can now use
Workforce Identity Federation
with Vertex AI Workbench instances in
Preview
. Workforce Identity Federation lets you create and manage Vertex AI Workbench instances with credentials provided by an external identity provider (IdP).
For more information, see
Create an instance with third party credentials
.
May 17, 2024
v1
Feature
M121 release
The M121 release of Vertex AI Workbench user-managed notebooks includes the following:
Updated Nvidia drivers to 550.54.15 to fix an issue where Nvidia drivers failed to install on startup after Debian 11 images upgraded kernel to
linux-image-5.10.0-29-cloud-amd64
.
The
linux-headers-cloud-amd64
metapackage is now installed for faster driver recompiling on kernel upgrades.
TensorFlow 2.6 CPU and GPU images are deprecated. There will be no further updates to these images in future releases.
v1
Feature
The M121 release of Vertex AI Workbench managed notebooks includes the following:
Updated the R CPU kernel from R 4.3 to R 4.4.
v2
Feature
M121 release
The M121 release of Vertex AI Workbench instances includes the following:
Updated Nvidia drivers to 550.54.15 to fix an issue where Nvidia drivers failed to install on startup after Debian 11 images upgraded kernel to
linux-image-5.10.0-29-cloud-amd64
.
The
linux-headers-cloud-amd64
metapackage is now installed for faster driver recompiling on kernel upgrades.
April 29, 2024
v1
Feature
M120 release
The M120 release of Vertex AI Workbench managed notebooks includes the following:
Minor bug fixes for the
libcurl
package.
April 25, 2024
v1
Feature
M120 release
The M120 release of Vertex AI Workbench user-managed notebooks includes the following:
Upgraded TensorFlow 2.15 user-managed notebooks to TensorFlow 2.15.1.
Minor bug fixes for the
libcurl
package.
v2
Feature
M120 release
The M120 release of Vertex AI Workbench instances includes the following:
Minor bug fixes for the
libcurl
package.
March 29, 2024
v1
Fixed
M119 release
The M119 release of Vertex AI Workbench user-managed notebooks includes the following:
Fixed an issue wherein Dataproc extensions caused JupyterLab to crash when remote kernels weren't available.
March 18, 2024
v2
Feature
M118 release
The M118 release of Vertex AI Workbench instances includes the following:
Updated Nvidia drivers to R535.
v1
Feature
M118 release
The M118 release of Vertex AI Workbench user-managed notebooks includes the following:
PyTorch 2.1.0 with CUDA 12.1 and Python 3.10 user-managed notebooks instances are now available.
PyTorch 2.2.0 with CUDA 12.1 and Python 3.10 user-managed notebooks instances are now available.
Updated Nvidia drivers of older user-managed notebooks images to R535.
v1
Feature
The M118 release of Vertex AI Workbench managed notebooks includes the following:
Updated Nvidia drivers to R535, which fixed a bug where the latest PyTorch 2.0 kernel didn't work due to outdated drivers.
February 28, 2024
v2
Feature
M117 release
The M117 release of Vertex AI Workbench instances includes the following:
Removed the Cloud Storage browser in the left side pane in favor of the existing
Mount shared storage
button.
February 08, 2024
v1
Feature
M116 release
The M116 release of Vertex AI Workbench user-managed notebooks includes the following:
Updated custom container user-managed notebooks to use NVIDIA driver version 535.104.05.
Fixed bugs in custom container user-managed notebooks where GPUs either wouldn't attach to the container properly, or detached after some time.
v1
Feature
The M116 release of Vertex AI Workbench managed notebooks includes the following:
Fixed a bug (present in versions M113 through M115) that prevented new local kernels from being usable.
January 19, 2024
v2
Feature
M115 release
The M115 release of Vertex AI Workbench instances includes the following:
Added support for
venv
kernels.
v1
Feature
M115 release
The M115 release of Vertex AI Workbench user-managed notebooks includes the following:
Added support for TensorFlow 2.15 with Python 3.10 on Debian 11.
Added support for TensorFlow 2.14 with Python 3.10 on Debian 11.
v1
Feature
The M115 release of Vertex AI Workbench managed notebooks includes the following:
Fixed the BigQuery connector within PySpark containers.
January 16, 2024
v1
Deprecated
Vertex AI Workbench managed notebooks is
deprecated
. On January 30, 2025, support for managed notebooks will end and the ability to create managed notebooks instances will be removed. Existing instances will continue to function but patches, updates, and upgrades won't be available. To continue using Vertex AI Workbench, you can
migrate your managed notebooks instances to Vertex AI Workbench instances
.
v1
Deprecated
Vertex AI Workbench user-managed notebooks is
deprecated
. On January 30, 2025, support for user-managed notebooks will end and the ability to create user-managed notebooks instances will be removed. Existing instances will continue to function but patches, updates, and upgrades won't be available. To continue using Vertex AI Workbench, you can
migrate your user-managed notebooks instances to Vertex AI Workbench instances
.
December 14, 2023
v1
Feature
M114 release
The M114 release of Vertex AI Workbench user-managed notebooks includes the following:
Starting with this release, Python 3.7 is no longer available.
Upgraded R to 4.3 on Debian 11 Python 3.10 instances.
Upgraded JupyterLab to 3.6.6.
v1
Feature
The M114 release of Vertex AI Workbench managed notebooks includes the following:
Starting with this release, Python 3.7 is no longer available.
Added new Dataproc extension for remote kernels.
Upgraded JupyterLab to 3.6.6.
Fixed an issue that sometimes prevented users from running or scheduling notebooks using a default kernel.
November 16, 2023
v2
Feature
M113 release
The M113 release of Vertex AI Workbench instances includes the following:
Added the Dataproc JupyterLab plugin to Vertex AI Workbench instances. To get started, see
Create a Dataproc-enabled instance
.
When using an instance's Google Cloud CLI,
gcloud config
is preset with the following defaults:
project
is set to your instance's project.
Your compute region is set to your instance's region.
Your Dataproc region is set to your instance's region.
Fixed an issue that prevented Dataproc kernels from working.
Fixed a CORS (cross-origin resource sharing) error.
v1
Feature
M113 release
The M113 release of Vertex AI Workbench user-managed notebooks includes the following:
Miscellaneous bug fixes and improvements in Python 3.10 notebooks.
October 10, 2023
v1
Feature
M112 release
The M112 release of Vertex AI Workbench user-managed notebooks includes the following:
Miscellaneous bug fixes and improvements.
September 25, 2023
v2
Feature
Vertex AI Workbench instances are now generally available (
GA
). Vertex AI Workbench instances combine features from managed notebooks and user-managed notebooks to provide a robust data science solution. Supported features include:
Idle timeout
BigQuery and Cloud Storage integrations
End-user and service account authentication
VPC Service Controls
Customer managed encryption keys (CMEK) and Cloud External Key Manager (Cloud EKM)
Health status monitoring
Scheduled notebook runs
Dataproc integration
To get started, see
Introduction to Vertex AI Workbench instances
.
September 18, 2023
v1
Deprecated
Debian 10 and Python 3.7 images have reached their end of patch and support life for Vertex AI Workbench managed notebooks and user-managed notebooks. Debian 11 and Python 3.10 images are available.
September 14, 2023
v1 & v2
Feature
M111 release
The M111 release of Vertex AI Workbench instances includes the following:
Miscellaneous software updates.
v1 & v2
Feature
The M111 release of Vertex AI Workbench user-managed notebooks includes the following:
PyTorch 2.0 user-managed notebooks instances now include PyTorch XLA 2.0.
Miscellaneous software updates.
v1 & v2
Feature
The M111 release of Vertex AI Workbench managed notebooks includes the following:
Miscellaneous software updates.
August 10, 2023
v1 & v2
Feature
M110 release
The M110 release of Vertex AI Workbench user-managed notebooks includes the following:
Added support for TensorFlow 2.13 with Python 3.10 on Debian 11.
Added support for TensorFlow 2.8 with Python 3.10 on Debian 11.
Miscellaneous software updates.
v1 & v2
Deprecated
TensorFlow 2.9 user-managed instances are deprecated.
v1 & v2
Feature
The M110 release of Vertex AI Workbench managed notebooks includes the following:
Increased shared memory size to available memory capacity.
Added support for Python 3.10 on Debian 11.
Added support for PyTorch 2.0 with Python 3.10.
Note:
Python 3.7 on Debian 10 images are still available.
July 19, 2023
v2
Feature
Vertex AI Workbench instances are now available in
Preview
. Vertex AI Workbench instances combine features from managed notebooks and user-managed notebooks to provide a robust data science solution. Supported features include:
Idle timeout
BigQuery and Cloud Storage integrations
End-user and service account authentication
VPC Service Controls
Customer managed encryption keys (CMEK)
Health status monitoring
Run notebooks on a schedule
Dataproc integration
To get started, see
Introduction to Vertex AI Workbench instances
.
June 26, 2023
v1
Feature
M109 release
The M109 release of Vertex AI Workbench user-managed notebooks includes the following:
PyTorch 2.0 with Python 3.10 and CUDA 11.8 user-managed notebooks instances are now available.
Miscellaneous software updates.
v1
Feature
The M109 release of Vertex AI Workbench managed notebooks includes the following:
Fixed a bug that caused high cpu utilization due to excessive internal diagnostic tool processes.
Fixed a bug that was showing incorrect kernel image icons in the Jupyterlab launcher.
May 04, 2023
v1
Feature
M108 release
The M108 release of Vertex AI Workbench user-managed notebooks includes the following:
Miscellaneous software updates.
April 13, 2023
v1
Feature
M107 release
The M107 release of Vertex AI Workbench user-managed notebooks includes the following:
Fixed a bug that displayed the wrong version of the JupyterLab user interface.
Fixed a bug where a cron job for the diagnostic tool was added at every restart.
Miscellaneous software updates.
April 06, 2023
v1
Feature
M106 release
The M106 release of Vertex AI Workbench user-managed notebooks includes the following:
Rolled back a previous change in which Jupyter dependencies were located in a separate Conda environment.
Fixed a bug in which kernels used by notebooks did not contain the specified machine learning frameworks.
Miscellaneous software updates.
March 31, 2023
v1
Feature
M105 release
The M105 release of Vertex AI Workbench user-managed notebooks includes the following:
The following user-managed notebooks images are now available with Python 3.10 on Debian 11:
TensorFlow 2.11 CPU (
tf-2-11-cpu-debian-11-py310
)
TensorFlow 2.11 GPU with Cuda 11.3 (
tf-2-11-cu113-notebooks-debian-11-py310
)
PyTorch 1.13 with Cuda 11.3 (
pytorch-1-13-cu113-notebooks-debian-11-py310
)
Base CPU (
common-cpu-notebooks-debian-11-py310
)
Base GPU with Cuda 11.3 (
common-cu113-notebooks-debian11-py310
)
The following user-managed notebooks images are now available with Python 3.9 on Debian 11:
TensorFlow 2.6 CPU (
tf-2-6-cpu-notebooks-debian-11-py39
)
TensorFlow 2.6 GPU with Cuda 11.3 (
tf-2-6-cu113-notebooks-debian-11-py39
)
Jupyter-related libraries have been moved to a different Conda environment, separate from the one containing machine learning frameworks and base software libraries.
March 27, 2023
v1
Feature
M105 release
The M105 release of Vertex AI Workbench managed notebooks includes the following:
Fixed an issue wherein a runtime with idle shutdown enabled doesn't detect activity and shuts down.
Fixed an issue wherein the runtime data disk runs out of space and prevents access.
Fixed an issue wherein end user credentials are not preserved after shutdown.
Changed Health Agent logging levels from
DEBUG
to
INFO
.
March 16, 2023
v1
Feature
M104 release
The M104 release of Vertex AI Workbench user-managed notebooks includes the following:
Fixed a regression in which
jupyter-user
metadata was ignored.
Enabled access to the Jupyter Gateway Client configuration by using the
notebook-enable-gateway-client
and
gateway-client-url
metadata tags.
Added the following packages:
google-cloud-artifact-registry
google-cloud-bigquery-storage
google-cloud-language
keyring
keyrings.google-artifactregistry-auth
Fixed a bug in which curl could not find the right SSL certificate path by default.
v1
Change
TensorFlow Enterprise 2.1 has reached the end of its support period. See
Version details
.
February 21, 2023
v1
Feature
M104 update
This update of the M104 release of Vertex AI Workbench managed notebooks includes the following:
Fixed a bug where local and remote kernels are not displayed. This happens when remote kernels are not accessible.
Minor bug fixes and improvements.
February 09, 2023
v1
Feature
M104 release
The M104 release of Vertex AI Workbench managed notebooks includes the following:
Added a fix for a security vulnerability in single-user managed notebooks instances.
Made enhancements to the network selection user experience in the managed notebooks executor.
Minor bug fixes and improvements.
January 30, 2023
v1
Feature
M103 release
The M103 release of Vertex AI Workbench user-managed notebooks includes the following:
Fixed a bug in which a warning tells the user to run
jupyter lab build
when creating a new instance.
Upgraded PyTorch to 1.13.1.
Minor bug fixes and improvements.
December 15, 2022
v1
Feature
M102 release
The M102 release of Vertex AI Workbench user-managed notebooks includes the following:
TensorFlow 2.11 is now available.
PyTorch 1.13 is now available.
Regular security patches and package upgrades.
December 09, 2022
v1
Feature
M101 release
The M101 release of Vertex AI Workbench includes the following:
TensorFlow patch version upgrades:
From 2.8.3 to 2.8.4.
From 2.9.2 to 2.9.3.
From 2.10.0 to 2.10.1.
TensorFlow 1.15 on Vertex AI Workbench is now deprecated.
Added
*.notebooks.cloud.google.com
as part of the domains required for users to access Notebooks API. Removed
*.datalab.cloud.google.com
.
Regular security patches and package upgrades.
November 08, 2022
v1
Feature
M100 release
The M100 release of Vertex AI Workbench includes the following:
Fixed a bug that prevented an instance with a GPU from starting.
Regular package updates.
Miscellaneous bug and display fixes.
v1
Fixed
Fixed a server-side request forgery (SSRF) vulnerability. Previous versions of managed notebooks and user-managed notebooks instances still contain the vulnerability. It is recommended that you
migrate your data
to a new instance.
October 25, 2022
v1beta1
Breaking
The
v1beta1
version of the Notebooks API is scheduled for removal no earlier than January 16, 2023. After this date, you must use Notebooks API
v1
to manage Vertex AI Workbench resources.
October 18, 2022
v1
Feature
M98 release
The M98 release of Vertex AI Workbench managed notebooks includes the following:
Upgraded Go from 1.16.5 to 1.19.2.
Upgraded R from 4.1 to 4.2.
Upgraded JupyterLab from 3.2 to 3.4.
Miscellaneous bug and display fixes.
Added a fix for the BigQuery SQL editor to run queries correctly in non-US locations.
Regular package updates.
Learn more about managed notebooks versions
.
September 20, 2022
v1
Feature
M96 release
The M96 release of Vertex AI Workbench managed notebooks includes the following:
Fixed a problem where users were not able to save large Notebooks.
Fixed a display issue when using JupyterLab's simple interface.
Improved timeout behavior switch hardware operations.
Improved error messaging when a service account cannot access the Runtime.
Security fixes.
Regular package refreshment and bug fixes.
Learn more about managed notebooks versions
.
v1
Fixed
Fixed a server-side request forgery (SSRF) vulnerability. Previous versions of managed notebooks and user-managed notebooks instances still contain the vulnerability. It is recommended that you
migrate your data
to a new instance.
August 17, 2022
v1 & v1beta1
Feature
M95 release
The M95 release of Vertex AI Workbench managed notebooks includes the following:
Fixed a bug where users were regularly getting a 502 error when trying to access JupyterLab.
Fixed a bug where opening an instance in Single User mode slowed the start of an instance.
Fixed a bug where a managed notebooks instance was not starting after adding a GPU.
Fixed bugs on the Serverless Spark form input.
Improved the ActivityLog refresh after Serverless Spark creation.
Fixed a bug related to the display of materialized views in BigQuery.
Refreshed the JupyterLab interface with an improved Google-specific theme.
Fixed a bug related to viewing Cloud Storage buckets and folders with large numbers of objects.
Regular package refreshment and bug fixes.
Learn more about managed notebooks versions
.
May 27, 2022
v1 & v1beta1
Feature
M93 release
The M93 release of Vertex AI Workbench managed notebooks includes the following:
v1 & v1beta1
Fixed
Fixed a bug that prevented kernels from shutting down properly in Vertex AI Workbench managed notebooks.
Learn more about managed notebooks versions
.
May 12, 2022
v1 & v1beta1
Feature
M91 release
The M91 release of Vertex AI Workbench managed notebooks includes the following:
Log streaming to the consumer project via Logs Viewer is now supported.
Added the
net-tools
package.
Regular package refreshments and bug fixes.
v1 & v1beta1
Fixed
Fixed an issue that caused Spark server networking errors when using Dataproc Serverless Spark and VPC Peering.
Learn more about managed notebooks versions
.
April 06, 2022
v1 & v1beta1
Feature
Vertex AI Workbench
is generally available (
GA
). Vertex AI Workbench is a single notebook surface for all your data science needs that lets you access BigQuery data and Cloud Storage from within JupyterLab, execute notebook code in Vertex AI custom training and Spark, use custom containers, manage costs with idle timeout, and secure your instances with VPC Service Controls and customer managed encryption keys (CMEK).
Features supported include:
Google-managed instances and the latest GPU support
Idle shutdown
for managed notebooks instances
Custom containers
End-user and service account authentication
Native plug-ins for
BigQuery
and
Cloud Storage
In-notebook Spark connect to Dataproc clusters
Jobs support via the
managed notebooks executor
on Vertex AI custom training and Spark
One-click deploy for NGC containers
VPC Service Controls
Customer managed encryption keys (CMEK)
v1 & v1beta1
Feature
The Vertex AI Workbench managed notebooks
executor
is generally available (
GA
). Use the executor to run notebook files on a schedule or as a one-time execution. You can use parameters in your execution to make specific changes to each run. For example, you might specify a different dataset to use, change the learning rate on your model, or change the version of the model. For more information, see
Run notebook files with the executor
.
October 11, 2021
v1 & v1beta1
Feature
Vertex AI Workbench
is now available in
Preview
. Vertex AI Workbench is a notebook-based development environment for the entire data science workflow.
v1 & v1beta1
Announcement
The Notebooks product and all existing Notebooks instances are now part of Vertex AI Workbench as
user-managed notebooks
.
September 10, 2021
v1 & v1beta1
Change
Due to a recent change, the
iam.serviceAccounts.actAs
permission on the specified service account for the notebook instance is required for users to continue to have access to their notebook instances. The Google internal Inverting Proxy server that provides access to notebook instances now verifies that this permission is present before allowing users access to the JupyterLab URL. The JupyterLab URL this update covers is:
*.notebooks.googleusercontent.com
This update only applies to notebook instances in
Single User
mode and verifies that the assigned single user is authorized to execute code inside the notebook instance. Notebook instances running in Service Account or Project Editor mode already perform this verification via the Inverting Proxy server.
July 26, 2021
v1 & v1beta1
Change
If using proxy single-user mode, Notebooks API now verifies if the specified user (
proxy-user-mail
) has Service Account permissions on the Service Account. This check is performed during instance creation and registration.
June 18, 2021
v1
Feature
Support for Compute Reservations. Notebooks API allows the use of Compute Reservations during instance creation.
March 26, 2021
v1
Feature
Cross Project Service Account is supported for user-managed notebooks.
March 04, 2021
v1
Change
New Notebooks instances add labels for VM image (
goog-caip-notebook
) and volume (
goog-caip-notebook-volume
).
February 01, 2021
v1
Feature
Notebooks
Terraform Module
supports Notebooks API v1
January 23, 2021
v1
Announcement
VPC-SC for Notebooks (now known as user-managed notebooks) is now
Generally Available
.
v1
Feature
Notebooks API supports
Shielded VM configuration
.
September 21, 2020
v1
Feature
AI Platform Notebooks (now known as user-managed notebooks) API is now
Generally Available
. The API now includes an
isUpgradable
endpoint and adds manual and auto-upgrade functionality to notebooks instances created using the API.
v1
Feature
Cloud Audit Logging for AI Platform Notebooks
(now known as user-managed notebooks) is now
Generally Available
.
Granular IAM permissions for AI Platform Notebooks
(now known as user-managed notebooks) is now
Generally Available
.
AI Platform Notebooks now supports
E2 machine types
.
The following new regions have been added:
europe-west2
(London, UK)
europe-west3
(Frankfurt, Germany)
europe-west6
(Zürich, Switzerland)
March 31, 2020
v1beta1
Feature
AI Platform Notebooks (now known as user-managed notebooks) is now
Generally Available
. Some integrations with and specific features of AI Platform Notebooks are still in beta, such as
Virtual Private Cloud Service Controls
,
Identity and Access Management (IAM)
roles, and
AI Platform Notebooks API
.
February 04, 2020
v1beta1
Feature
VPC Service Controls
now supports AI Platform Notebooks. Learn
how to use a notebook instance within a service perimeter
. This functionality is in
beta
.
February 03, 2020
v1beta1
Feature
AI Platform Notebooks now supports Access Transparency. Access Transparency provides you with logs of actions that Google staff have taken when accessing your data. To learn more about Access Transparency, see the
Overview of Access Transparency
.
September 12, 2019
v1
Feature
You can now use customer-managed encryption keys (CMEK) to protect data on the boot disks of your AI Platform Notebooks (now known as user-managed notebooks) VM instances. CMEK in AI Platform Notebooks is generally available. For more information, see
Using customer-managed encryption keys (CMEK)
.
September 09, 2019
v1beta1
Feature
AI Platform Notebooks now provides more ways for you to customize your network settings, encrypt your notebook content, and grant access to your notebook instance. These options are available when you
create a notebook
.
v1beta1
Feature
Now you can implement AI Platform Notebooks using custom containers. Use a
Deep Learning Containers image
or
create a derivative container
of your own, then
create a new notebook instance using your custom container
.
July 12, 2019
v1beta1
Change
R upgraded to version 3.6.
R Notebooks are no longer dependent on a Conda environment.
June 03, 2019
v1beta1
Feature
You can now create AI Platform Notebooks instances with R and core R packages installed. Learn
how to install R dependencies
, and read guides for
using R with BigQuery in AI Platform Notebooks
and
using R and Python in the same notebook
.
March 01, 2019
v1beta1
Feature
AI Platform Notebooks
is now available in beta. AI Platform Notebooks enables you to create and manage virtual machine (VM) instances that are pre-packaged with
JupyterLab
and a suite of deep learning software.
Visit the
AI Platform Notebooks overview
and the
guide to creating a new notebook instance
to learn more.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/audit-logging.txt
User-managed notebooks audit logging  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
User-managed notebooks audit logging
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
If you're looking for information on audit logs created by
Vertex AI, see the
Vertex AI page on
audit logging
.
This document describes the audit logs created by Vertex AI Workbench as part of
Cloud Audit Logs
.
Overview
Google Cloud services write audit logs to help you answer the questions, "Who
did what, where, and when?" within your Google Cloud resources.
Your Google Cloud projects contain only the audit logs for resources that are
directly within the Google Cloud project. Other Google Cloud resources,
such as folders, organizations, and billing accounts, contain the audit logs for
the entity itself.
For a general overview of Cloud Audit Logs, see
Cloud Audit Logs overview
. For a deeper understanding
of the audit log format, see
Understand audit logs
.
Available audit logs
The following types of audit logs are available for Vertex AI Workbench:
Admin Activity audit logs
Includes "admin write" operations that write metadata or configuration
        information.
You can't disable Admin Activity audit logs.
Data Access audit logs
Includes "admin read" operations that read metadata or configuration
        information.
        
        Also includes "data read" and "data write" operations that
        read or write user-provided data.
To receive Data Access audit logs, you must
explicitly enable
them.
For fuller descriptions of the audit log types, see
Types of audit logs
.
Audited operations
The following table summarizes which API operations correspond to each audit log
type in Vertex AI Workbench:
Audit logs category
Vertex AI Workbench operations
Admin Activity audit logs
CreateInstance
RegisterInstance
SetInstanceAccelerator
SetInstanceMachineType
SetInstanceLabels
DeleteInstance
StartInstance
StopInstance
ResetInstance
UpgradeInstance
DiagnoseInstance
CreateEnvironment
DeleteEnvironment
ReportInstanceInfo
UpgradeInstanceInternal
UpdateShieldInstanceConfig
CreateRuntime
DeleteRuntime
StartRuntime
StopRuntime
ResetRuntime
UpgradeRuntime
RollbackRuntime
UpdateRuntime
DiagnoseRuntime
Data Access (ADMIN_READ) audit logs
ListInstances
GetInstance
ListEnvironments
GetEnvironment
GetHealth
ListRun
ListSchedules
GetRun
GetSchedules
RuntimeList
GetRuntime
Audit log format
Audit log entries include the following objects:
The log entry itself, which is an object of type
LogEntry
.
Useful fields include the following:
The
logName
contains the resource ID and audit log type.
The
resource
contains the target of the audited operation.
The
timeStamp
contains the time of the audited operation.
The
protoPayload
contains the audited information.
The audit logging data, which is an
AuditLog
object held in
the
protoPayload
field of the log entry.
Optional service-specific audit information, which is a service-specific
object. For earlier integrations, this object is held in the
serviceData
field of the
AuditLog
object; later integrations use the
metadata
field.
For other fields in these objects, and how to interpret them, review
Understand audit logs
.
Log name
Cloud Audit Logs log names include resource identifiers indicating the
Google Cloud project or other Google Cloud entity that owns the audit
logs, and whether the log contains Admin Activity, Data Access, Policy Denied,
or System Event audit logging data.
The following are the audit log names, including variables for the resource
identifiers:
projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Factivity
   projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fdata_access
   projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fsystem_event
   projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fpolicy

   folders/
FOLDER_ID
/logs/cloudaudit.googleapis.com%2Factivity
   folders/
FOLDER_ID
/logs/cloudaudit.googleapis.com%2Fdata_access
   folders/
FOLDER_ID
/logs/cloudaudit.googleapis.com%2Fsystem_event
   folders/
FOLDER_ID
/logs/cloudaudit.googleapis.com%2Fpolicy

   billingAccounts/
BILLING_ACCOUNT_ID
/logs/cloudaudit.googleapis.com%2Factivity
   billingAccounts/
BILLING_ACCOUNT_ID
/logs/cloudaudit.googleapis.com%2Fdata_access
   billingAccounts/
BILLING_ACCOUNT_ID
/logs/cloudaudit.googleapis.com%2Fsystem_event
   billingAccounts/
BILLING_ACCOUNT_ID
/logs/cloudaudit.googleapis.com%2Fpolicy

   organizations/
ORGANIZATION_ID
/logs/cloudaudit.googleapis.com%2Factivity
   organizations/
ORGANIZATION_ID
/logs/cloudaudit.googleapis.com%2Fdata_access
   organizations/
ORGANIZATION_ID
/logs/cloudaudit.googleapis.com%2Fsystem_event
   organizations/
ORGANIZATION_ID
/logs/cloudaudit.googleapis.com%2Fpolicy
Note:
The part of the log name following
/logs/
must be URL-encoded. The
forward-slash character,
/
, must be written as
%2F
.
Service name
Vertex AI Workbench audit logs use the service name
notebooks.googleapis.com
.
For a list of all the Cloud Logging API service names and their corresponding
monitored resource type, see
Map services to resources
.
Resource types
Vertex AI Workbench audit logs use the resource type
audited_resource
for all audit logs.
For a list of all the Cloud Logging monitored resource types and descriptive
information, see
Monitored resource types
.
Caller identities
The IP address of the caller is held in the
RequestMetadata.caller_ip
field of
the
AuditLog
object. Logging might redact certain
caller identities and IP addresses.
For information about what information is redacted in audit logs, see
Caller identities in audit logs
.
Enable audit logging
Admin Activity audit logs are always enabled; you can't disable them.
Data Access audit logs are disabled by default and aren't written unless
explicitly enabled (the exception is Data Access audit logs for
BigQuery, which can't be disabled).
For information about enabling some or all of your Data Access audit logs, see
Enable Data Access audit logs
.
Permissions and roles
IAM
permissions and roles determine your ability to
access audit logs data in Google Cloud resources.
When deciding which
Logging-specific permissions and roles
apply to your use case, consider the following:
The Logs Viewer role (
roles/logging.viewer
) gives you read-only access to
Admin Activity, Policy Denied, and System Event audit logs. If you have just
this role, you cannot view Data Access audit logs that are in the
_Default
bucket.
The Private Logs Viewer role
(roles/logging.privateLogViewer
) includes the
permissions contained in
roles/logging.viewer
, plus the ability to read
Data Access audit logs in the
_Default
bucket.
Note that if these private logs are stored in user-defined buckets, then any
user who has permissions to read logs in those buckets can read the private
logs. For more information about log buckets, see
Routing and storage overview
.
For more information about the IAM permissions and roles that
apply to audit logs data, see
Access control with IAM
.
View logs
You can query for all audit logs or you can query for logs by their
audit log name. The audit log name includes the
resource identifier
of the Google Cloud project, folder, billing account, or
organization for which you want to view audit logging information.
Your queries can specify indexed
LogEntry
fields.
For more information about querying your logs, see
Build queries in the Logs Explorer
The Logs Explorer lets you view filter individual log entries. If you want
to use SQL to analyze groups of log entries, then use the
Log Analytics
page. For more information, see:
Query and view logs in Log Analytics
.
Sample queries for security insights
.
Chart query results
.
Most audit logs can be viewed in Cloud Logging by using the
Google Cloud console, the Google Cloud CLI, or the Logging API.
However, for audit logs related to billing, you can only use the
Google Cloud CLI or the Logging API.
Console
In the Google Cloud console, you can use the Logs Explorer
to retrieve your audit log entries for your Google Cloud project, folder,
or organization:
Note:
You can't view audit logs for Cloud Billing accounts in the
Google Cloud console. You must use the API or the gcloud CLI.
In the Google Cloud console, go to the
Logs Explorer
page:
Go to
Logs Explorer
If you use the search bar to find this page, then select the result whose subheading is
Logging
.
Select an existing Google Cloud project, folder, or organization.
To display all audit logs, enter either of the following queries
into the query-editor field, and then click
Run query
:
logName:"cloudaudit.googleapis.com"
protoPayload."@type"="type.googleapis.com/google.cloud.audit.AuditLog"
To display the audit logs for a specific resource and audit log type,
in the
Query builder
pane, do the following:
In
Resource type
, select the Google Cloud resource whose
audit logs you want to see.
In
Log name
, select the audit log type that you want to see:
For Admin Activity audit logs, select
activity
.
For Data Access audit logs, select
data_access
.
For System Event audit logs, select
system_event
.
For Policy Denied audit logs, select
policy
.
Click
Run query
.
If you don't see these options, then there aren't any audit logs of
that type available in the Google Cloud project, folder, or
organization.
If you're experiencing issues when trying to view logs in the
Logs Explorer, see the
troubleshooting
information.
For more information about querying by using the Logs Explorer, see
Build queries in the Logs Explorer
.
gcloud
The Google Cloud CLI provides a command-line interface to the
Logging API. Supply a valid resource identifier in each of the log
names. For example, if your query includes a
PROJECT_ID
, then the
project identifier you supply must refer to the currently selected
Google Cloud project.
To read your Google Cloud project-level audit log entries, run
the following command:
gcloud logging read "logName : projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com" \
    --project=
PROJECT_ID
To read your folder-level audit log entries, run the following command:
gcloud logging read "logName : folders/
FOLDER_ID
/logs/cloudaudit.googleapis.com" \
    --folder=
FOLDER_ID
To read your organization-level audit log entries, run the following
command:
gcloud logging read "logName : organizations/
ORGANIZATION_ID
/logs/cloudaudit.googleapis.com" \
    --organization=
ORGANIZATION_ID
To read your Cloud Billing account-level audit log entries, run the following command:
gcloud logging read "logName : billingAccounts/
BILLING_ACCOUNT_ID
/logs/cloudaudit.googleapis.com" \
    --billing-account=
BILLING_ACCOUNT_ID
Add the
--freshness
flag
to your command to read logs that are more than 1 day old.
For more information about using the gcloud CLI, see
gcloud logging read
.
REST
When building your queries, supply a valid resource identifier in each of
the log names. For example, if your query includes a
PROJECT_ID
,
then the project identifier you supply must refer to the currently selected
Google Cloud project.
For example, to use the Logging API to view your project-level
audit log entries, do the following:
Go to the
Try this API
section in the documentation for the
entries.list
method.
Put the following into the
Request body
part of the
Try this
API
form. Clicking this
prepopulated form
automatically fills the request body, but you need to supply a valid
PROJECT_ID
in each of the log names.
{
  "resourceNames": [
    "projects/
PROJECT_ID
"
  ],
  "pageSize": 5,
  "filter": "logName : projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com"
}
Click
Execute
.
For example, to view all the project-level audit logs for
Vertex AI Workbench, use the following query, supplying a valid resource
identifier in each of the log names:
logName
=
(
"projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Factivity"
OR
"projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fdata_access"
OR
"projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fsystem_event"
OR
"projects/
PROJECT_ID
/logs/cloudaudit.googleapis.com%2Fpolicy"
)
protoPayload
.
serviceName
=
"notebooks.googleapis.com"
Route audit logs
You can
route audit logs
to supported
destinations in the same way that you can route other kinds of logs. Here are
some reasons you might want to route your audit logs:
To keep audit logs for a longer period of time or to use more powerful
search capabilities, you can route copies of your audit logs to
Cloud Storage, BigQuery, or Pub/Sub. Using
Pub/Sub, you can route to other applications, other
repositories, and to third parties.
To manage your audit logs across an entire organization, you can create
aggregated sinks
that can
route logs from any or all Google Cloud projects in the organization.
If your enabled Data Access audit logs are pushing your
Google Cloud projects over your log allotments, you can create sinks that
exclude the Data Access audit logs from Logging.
For instructions about routing logs, see
Route logs to supported destinations
.
Pricing
For more information about pricing, see the Cloud Logging sections in the
Google Cloud Observability pricing
page.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/backup-snapshot.txt
Back up your data by using a snapshot  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Back up your data by using a snapshot
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
This page shows you how to back up data stored on your
Vertex AI Workbench user-managed notebooks instance
by creating a snapshot.
The data on your instance is stored on
a zonal
persistent disk
.
You can create and use snapshots of this disk to back up your data,
create a recurring backup schedule, and restore data to a new instance.
Create a snapshot
You can create snapshots from disks even while they are attached to running
instances. Snapshots are
global resources
,
so you can use them to
restore data
to
a new disk or instance within the same project. You can also
share snapshots
across projects.
Permissions required for this task
To perform this task, you must have the following
permissions
:
compute.snapshots.create
on the project
compute.disks.createSnapshot
on the disk
Note:
Your Jupyter notebook files are stored on a disk
      named
INSTANCE_NAME
-data
.
Console
In the Google Cloud console, go to the
VM instances
page.
Go to VM instances
school
The remaining steps will appear automatically in
      the Google Cloud console.
Select the project that contains your VM instances.
In the
Name
column, click the name of the VM that has the disk to back up.
In
Storage
:
To back up the boot disk, in the
Boot disk
section, click the
Name
of the
        boot disk.
To back up an attached data disk, in
Additional disks
, click the
Name
of the disk.
Click
Create snapshot
.
In
Name
, enter a unique name to help identify the purpose of the snapshot, for example:
boot-disk-snapshot
attached-data-disk-snapshot
In
Type
, the default is a standard snapshot. Standard snapshots are
    best for long-term back up and disaster recovery.
Choose
Archive snapshot
to create a more cost-efficient backup than standard
      snapshots, but with a longer data recovery time.
For more information, see
Snapshot type comparison
.
In the
Location
section,
choose your snapshot
        storage location
.
        The predefined or customized default location defined in your snapshot settings is
        automatically selected. Optionally, you can override the snapshot settings and store your
        snapshots in a custom storage location by doing the following:
Choose the type of storage location that you want for your snapshot.
Choose
Multi-regional
for higher availability at a higher cost.
Choose
Regional snapshots
for more control over the physical location of your data at a lower cost.
In the
Select location
field, select the specific region or multi-region that
            you want to use. To use the region or multi-region that is closest to your
            source disk, choose a location from the section titled
Based on disk's location
.
To create a snapshot, click
Create
.
gcloud
In the Google Cloud console, activate Cloud Shell.
Activate Cloud Shell
At the bottom of the Google Cloud console, a
Cloud Shell
session starts and displays a command-line prompt. Cloud Shell is a shell environment
      with the Google Cloud CLI
      already installed and with values already set for
      your current project. It can take a few seconds for the session to initialize.
Create your snapshot using the storage location policy defined by your
snapshot settings
or using an alternative storage location of your choice. For more
information, see
Choose your snapshot storage location
.
You must specify a snapshot name. The name must be 1-63
characters long, and comply with
RFC 1035
.
To create a snapshot of a Persistent Disk volume in the predefined or
customized default location configured in your snapshot settings, use
the
gcloud compute snapshots create
command
.
gcloud compute snapshots create
SNAPSHOT_NAME
\
    --source-disk
SOURCE_DISK
\
    --snapshot-type
SNAPSHOT_TYPE
\
    --source-disk-zone
SOURCE_DISK_ZONE
Alternatively, to override the snapshot settings and create a snapshot
in a custom storage location, include the
--storage-location
flag to
indicate where to store your snapshot:
gcloud compute snapshots create
SNAPSHOT_NAME
\
  --source-disk
SOURCE_DISK
\
  --source-disk-zone
SOURCE_DISK_ZONE
\
  --storage-location
STORAGE_LOCATION
\
  --snapshot-type
SNAPSHOT_TYPE
Replace the following:
SNAPSHOT_NAME
: A name for the snapshot.
SOURCE_DISK
: The name of the zonal Persistent Disk volume from which you want to
    create a snapshot.
SNAPSHOT_TYPE
: The snapshot type, either
STANDARD
or
ARCHIVE
. If a snapshot type is not specified, a
STANDARD
snapshot is created. Choose Archive for more cost-efficient data retention.
SOURCE_DISK_ZONE
: The zone of the zonal Persistent Disk volume from which you want
    to create a snapshot.
STORAGE_LOCATION
: For custom storage locations,
this is the
Cloud Storage multi-region
or the
Cloud Storage region
where you want to store your snapshot. You can specify only one
storage location.
Use the
--storage-location
flag only when you want to override the
predefined or customized default storage location configured in your
snapshot settings.
The gcloud CLI waits until the operation returns a status of
READY
or
FAILED
, or reaches the maximum timeout and returns the last
known details of the snapshot.
Note:
Google recommends using the
gcloud compute snapshots create
command
instead of the
gcloud compute disks snapshot
command
because it supports more features, such as creating snapshots in a
project different from the source disk project.
Terraform
To create a snapshot of the zonal persistent disk, use the
google_compute_snapshot
resource.
resource "google_compute_snapshot" "snapdisk" {
  name        = "snapshot-name"
  source_disk = google_compute_disk.default.name
  zone        = "us-central1-a"
}
To learn how to apply or remove a Terraform configuration, see
Basic Terraform commands
.
API
Create your snapshot in the storage location policy defined by your
snapshot settings
or using an alternative storage location of your choice. For more
information, see
Choose your snapshot storage location
.
To create your snapshot in the predefined or customized default location
configured in your snapshot settings, make a
POST
request to the
snapshots.insert
method
:
POST https://compute.googleapis.com/compute/v1/projects/
DESTINATION_PROJECT_ID
/global/snapshots

{
  "name":
SNAPSHOT_NAME
"sourceDisk": "projects/
SOURCE_PROJECT_ID
/zones/
SOURCE_ZONE
/disks/
SOURCE_DISK_NAME
"snapshotType":
SNAPSHOT_TYPE
}
Replace the following:
DESTINATION_PROJECT_ID
: The ID of project in which you want
to create the snapshot.
SNAPSHOT_NAME
: A name for the snapshot.
SOURCE_PROJECT_ID
: The ID of the source disk project.
SOURCE_ZONE
: The zone of the source disk.
SOURCE_DISK_NAME
: The name of the persistent disk from
which you want to create a snapshot.
SNAPSHOT_TYPE
: The snapshot type, either
STANDARD
or
ARCHIVE
. If a snapshot type is not specified, a
STANDARD
snapshot is created.
Alternatively, to override the snapshot settings and create a snapshot in
a custom storage location, make a
POST
request to the
snapshots.insert
method
and include the
storageLocations
property in your request:
POST https://compute.googleapis.com/compute/v1/projects/
DESTINATION_PROJECT_ID
/global/snapshots

{
  "name":
SNAPSHOT_NAME
"sourceDisk": "projects/
SOURCE_PROJECT_ID
/zones/
SOURCE_ZONE
/disks/
SOURCE_DISK_NAME
"snapshotType":
SNAPSHOT_TYPE
"storageLocations":
STORAGE_LOCATION
}
Replace the following:
DESTINATION_PROJECT_ID
: The ID of project in which you want
to create the snapshot.
SNAPSHOT_NAME
: A name for the snapshot.
SOURCE_PROJECT_ID
: The ID of the source disk project.
SOURCE_ZONE
: The zone of the source disk.
SOURCE_DISK_NAME
: The name of the persistent disk from
which you want to create a snapshot.
SNAPSHOT_TYPE
: The snapshot type, either
STANDARD
or
ARCHIVE
. If a snapshot type is not specified, a
STANDARD
snapshot is created.
STORAGE_LOCATION
: The
Cloud Storage multi-region
or the
Cloud Storage region
where you want to store your snapshot. You can specify only one
storage location.
Use the
storageLocations
parameter only when you want to override
the predefined or customized default storage location configured in
your snapshot settings.
Note:
Google recommends using the
snapshots.insert
method
instead of the
disks.createSnapshot
method
because it supports more features, such as creating snapshots in a
project different from the source disk project.
Go
Go
Before trying this sample, follow the setup instructions in the
Compute Engine quickstart using client libraries
.
To authenticate to Compute Engine, set up Application Default Credentials. For
more information, see
Set up authentication for a local development environment
.
import
(
"context"
"fmt"
"io"
compute
"cloud.google.com/go/compute/apiv1"
computepb
"cloud.google.com/go/compute/apiv1/computepb"
"google.golang.org/protobuf/proto"
)
// createSnapshot creates a snapshot of a disk.
func
createSnapshot
(
w
io
.
Writer
,
projectID
,
diskName
,
snapshotName
,
zone
,
region
,
location
,
diskProjectID
string
,
)
error
{
// projectID := "your_project_id"
// diskName := "your_disk_name"
// snapshotName := "your_snapshot_name"
// zone := "europe-central2-b"
// region := "eupore-central2"
// location = "eupore-central2"
// diskProjectID = "YOUR_DISK_PROJECT_ID"
ctx
:=
context
.
Background
()
snapshotsClient
,
err
:=
compute
.
NewSnapshotsRESTClient
(
ctx
)
if
err
!=
nil
{
return
fmt
.
Errorf
(
"NewSnapshotsRESTClient: %w"
,
err
)
}
defer
snapshotsClient
.
Close
()
if
zone
==
""
&&
region
==
""
{
return
fmt
.
Errorf
(
"you need to specify `zone` or `region` for this function to work"
)
}
if
zone
!=
""
&&
region
!=
""
{
return
fmt
.
Errorf
(
"you can't set both `zone` and `region` parameters"
)
}
if
diskProjectID
==
""
{
diskProjectID
=
projectID
}
disk
:=
&
computepb
.
Disk
{}
locations
:=
[]
string
{}
if
location
!=
""
{
locations
=
append
(
locations
,
location
)
}
if
zone
!=
""
{
disksClient
,
err
:=
compute
.
NewDisksRESTClient
(
ctx
)
if
err
!=
nil
{
return
fmt
.
Errorf
(
"NewDisksRESTClient: %w"
,
err
)
}
defer
disksClient
.
Close
()
getDiskReq
:=
&
computepb
.
GetDiskRequest
{
Project
:
projectID
,
Zone
:
zone
,
Disk
:
diskName
,
}
disk
,
err
=
disksClient
.
Get
(
ctx
,
getDiskReq
)
if
err
!=
nil
{
return
fmt
.
Errorf
(
"unable to get disk: %w"
,
err
)
}
}
else
{
regionDisksClient
,
err
:=
compute
.
NewRegionDisksRESTClient
(
ctx
)
if
err
!=
nil
{
return
fmt
.
Errorf
(
"NewRegionDisksRESTClient: %w"
,
err
)
}
defer
regionDisksClient
.
Close
()
getDiskReq
:=
&
computepb
.
GetRegionDiskRequest
{
Project
:
projectID
,
Region
:
region
,
Disk
:
diskName
,
}
disk
,
err
=
regionDisksClient
.
Get
(
ctx
,
getDiskReq
)
if
err
!=
nil
{
return
fmt
.
Errorf
(
"unable to get disk: %w"
,
err
)
}
}
req
:=
&
computepb
.
InsertSnapshotRequest
{
Project
:
projectID
,
SnapshotResource
:
&
computepb
.
Snapshot
{
Name
:
proto
.
String
(
snapshotName
),
SourceDisk
:
proto
.
String
(
disk
.
GetSelfLink
()),
StorageLocations
:
locations
,
},
}
op
,
err
:=
snapshotsClient
.
Insert
(
ctx
,
req
)
if
err
!=
nil
{
return
fmt
.
Errorf
(
"unable to create snapshot: %w"
,
err
)
}
if
err
=
op
.
Wait
(
ctx
);
err
!=
nil
{
return
fmt
.
Errorf
(
"unable to wait for the operation: %w"
,
err
)
}
fmt
.
Fprintf
(
w
,
"Snapshot created\n"
)
return
nil
}
Java
Java
Before trying this sample, follow the setup instructions in the
Compute Engine quickstart using client libraries
.
To authenticate to Compute Engine, set up Application Default Credentials. For
more information, see
Set up authentication for a local development environment
.
import
com.google.cloud.compute.v1.
Disk
;
import
com.google.cloud.compute.v1.
DisksClient
;
import
com.google.cloud.compute.v1.
Operation
;
import
com.google.cloud.compute.v1.
RegionDisksClient
;
import
com.google.cloud.compute.v1.
Snapshot
;
import
com.google.cloud.compute.v1.
SnapshotsClient
;
import
java.io.IOException
;
import
java.util.concurrent.ExecutionException
;
import
java.util.concurrent.TimeUnit
;
import
java.util.concurrent.TimeoutException
;
public
class
CreateSnapshot
{
public
static
void
main
(
String
[]
args
)
throws
IOException
,
ExecutionException
,
InterruptedException
,
TimeoutException
{
// TODO(developer): Replace these variables before running the sample.
// You need to pass `zone` or `region` parameter relevant to the disk you want to
// snapshot, but not both. Pass `zone` parameter for zonal disks and `region` for
// regional disks.
// Project ID or project number of the Cloud project you want to use.
String
projectId
=
"YOUR_PROJECT_ID"
;
// Name of the disk you want to create.
String
diskName
=
"YOUR_DISK_NAME"
;
// Name of the snapshot that you want to create.
String
snapshotName
=
"YOUR_SNAPSHOT_NAME"
;
// The zone of the source disk from which you create the snapshot (for zonal disks).
String
zone
=
"europe-central2-b"
;
// The region of the source disk from which you create the snapshot (for regional disks).
String
region
=
"your-disk-region"
;
// The Cloud Storage multi-region or the Cloud Storage region where you
// want to store your snapshot.
// You can specify only one storage location. Available locations:
// https://cloud.google.com/storage/docs/locations#available-locations
String
location
=
"europe-central2"
;
// Project ID or project number of the Cloud project that
// hosts the disk you want to snapshot. If not provided, the value will be defaulted
// to 'projectId' value.
String
diskProjectId
=
"YOUR_DISK_PROJECT_ID"
;
createSnapshot
(
projectId
,
diskName
,
snapshotName
,
zone
,
region
,
location
,
diskProjectId
);
}
// Creates a snapshot of a disk.
public
static
void
createSnapshot
(
String
projectId
,
String
diskName
,
String
snapshotName
,
String
zone
,
String
region
,
String
location
,
String
diskProjectId
)
throws
IOException
,
ExecutionException
,
InterruptedException
,
TimeoutException
{
// Initialize client that will be used to send requests. This client only needs to be created
// once, and can be reused for multiple requests. After completing all of your requests, call
// the `snapshotsClient.close()` method on the client to safely
// clean up any remaining background resources.
try
(
SnapshotsClient
snapshotsClient
=
SnapshotsClient
.
create
())
{
if
(
zone
.
isEmpty
()
&&
region
.
isEmpty
())
{
throw
new
Error
(
"You need to specify 'zone' or 'region' for this function to work"
);
}
if
(
!
zone
.
isEmpty
()
&&
!
region
.
isEmpty
())
{
throw
new
Error
(
"You can't set both 'zone' and 'region' parameters"
);
}
// If Disk's project id is not specified, then the projectId parameter will be used.
if
(
diskProjectId
.
isEmpty
())
{
diskProjectId
=
projectId
;
}
// If zone is not empty, use the DisksClient to create a disk.
// Else, use the RegionDisksClient.
Disk
disk
;
if
(
!
zone
.
isEmpty
())
{
DisksClient
disksClient
=
DisksClient
.
create
();
disk
=
disksClient
.
get
(
projectId
,
zone
,
diskName
);
}
else
{
RegionDisksClient
regionDisksClient
=
RegionDisksClient
.
create
();
disk
=
regionDisksClient
.
get
(
diskProjectId
,
region
,
diskName
);
}
// Set the snapshot properties.
Snapshot
snapshotResource
;
if
(
!
location
.
isEmpty
())
{
snapshotResource
=
Snapshot
.
newBuilder
()
.
setName
(
snapshotName
)
.
setSourceDisk
(
disk
.
getSelfLink
())
.
addStorageLocations
(
location
)
.
build
();
}
else
{
snapshotResource
=
Snapshot
.
newBuilder
()
.
setName
(
snapshotName
)
.
setSourceDisk
(
disk
.
getSelfLink
())
.
build
();
}
// Wait for the operation to complete.
Operation
operation
=
snapshotsClient
.
insertAsync
(
projectId
,
snapshotResource
)
.
get
(
3
,
TimeUnit
.
MINUTES
);
if
(
operation
.
hasError
())
{
System
.
out
.
println
(
"Snapshot creation failed!"
+
operation
);
return
;
}
// Retrieve the created snapshot.
Snapshot
snapshot
=
snapshotsClient
.
get
(
projectId
,
snapshotName
);
System
.
out
.
printf
(
"Snapshot created: %s"
,
snapshot
.
getName
());
}
}
}
Node.js
Node.js
Before trying this sample, follow the setup instructions in the
Compute Engine quickstart using client libraries
.
To authenticate to Compute Engine, set up Application Default Credentials. For
more information, see
Set up authentication for a local development environment
.
/**
* TODO(developer): Uncomment and replace these variables before running the sample.
*/
// const projectId = 'YOUR_PROJECT_ID';
// const diskName = 'YOUR_DISK_NAME';
// const snapshotName = 'YOUR_SNAPSHOT_NAME';
// const zone = 'europe-central2-b';
// const region = '';
// const location = 'europe-central2';
// let diskProjectId = 'YOUR_DISK_PROJECT_ID';
const
compute
=
require
(
'
@google-cloud/compute
'
);
async
function
createSnapshot
()
{
const
snapshotsClient
=
new
compute
.
SnapshotsClient
();
let
disk
;
if
(
!
zone
&&
!
region
)
{
throw
new
Error
(
'You need to specify `zone` or `region` for this function to work.'
);
}
if
(
zone
&&
region
)
{
throw
new
Error
(
"You can't set both `zone` and `region` parameters"
);
}
if
(
!
diskProjectId
)
{
diskProjectId
=
projectId
;
}
if
(
zone
)
{
const
disksClient
=
new
compute
.
DisksClient
();
[
disk
]
=
await
disksClient
.
get
({
project
:
diskProjectId
,
zone
,
disk
:
diskName
,
});
}
else
{
const
regionDisksClient
=
new
compute
.
RegionDisksClient
();
[
disk
]
=
await
regionDisksClient
.
get
({
project
:
diskProjectId
,
region
,
disk
:
diskName
,
});
}
const
snapshotResource
=
{
name
:
snapshotName
,
sourceDisk
:
disk
.
selfLink
,
};
if
(
location
)
{
snapshotResource
.
storageLocations
=
[
location
];
}
const
[
response
]
=
await
snapshotsClient
.
insert
({
project
:
projectId
,
snapshotResource
,
});
let
operation
=
response
.
latestResponse
;
const
operationsClient
=
new
compute
.
GlobalOperationsClient
();
// Wait for the create snapshot operation to complete.
while
(
operation
.
status
!==
'DONE'
)
{
[
operation
]
=
await
operationsClient
.
wait
({
operation
:
operation
.
name
,
project
:
projectId
,
});
}
console
.
log
(
'Snapshot created.'
);
}
createSnapshot
();
Python
Python
Before trying this sample, follow the setup instructions in the
Compute Engine quickstart using client libraries
.
To authenticate to Compute Engine, set up Application Default Credentials. For
more information, see
Set up authentication for a local development environment
.
from
__future__
import
annotations
import
sys
from
typing
import
Any
from
google.api_core.extended_operation
import
ExtendedOperation
from
google.cloud
import
compute_v1
def
wait_for_extended_operation
(
operation
:
ExtendedOperation
,
verbose_name
:
str
=
"operation"
,
timeout
:
int
=
300
)
-
>
Any
:
"""
Waits for the extended (long-running) operation to complete.
If the operation is successful, it will return its result.
If the operation ends with an error, an exception will be raised.
If there were any warnings during the execution of the operation
they will be printed to sys.stderr.
Args:
operation: a long-running operation you want to wait on.
verbose_name: (optional) a more verbose name of the operation,
used only during error and warning reporting.
timeout: how long (in seconds) to wait for operation to finish.
If None, wait indefinitely.
Returns:
Whatever the operation.result() returns.
Raises:
This method will raise the exception received from `operation.exception()`
or RuntimeError if there is no exception set, but there is an `error_code`
set for the `operation`.
In case of an operation taking longer than `timeout` seconds to complete,
a `concurrent.futures.TimeoutError` will be raised.
"""
result
=
operation
.
result
(
timeout
=
timeout
)
if
operation
.
error_code
:
print
(
f
"Error during
{
verbose_name
}
: [Code:
{
operation
.
error_code
}
]:
{
operation
.
error_message
}
"
,
file
=
sys
.
stderr
,
flush
=
True
,
)
print
(
f
"Operation ID:
{
operation
.
name
}
"
,
file
=
sys
.
stderr
,
flush
=
True
)
raise
operation
.
exception
()
or
RuntimeError
(
operation
.
error_message
)
if
operation
.
warnings
:
print
(
f
"Warnings during
{
verbose_name
}
:
\n
"
,
file
=
sys
.
stderr
,
flush
=
True
)
for
warning
in
operation
.
warnings
:
print
(
f
" -
{
warning
.
code
}
:
{
warning
.
message
}
"
,
file
=
sys
.
stderr
,
flush
=
True
)
return
result
def
create_snapshot
(
project_id
:
str
,
disk_name
:
str
,
snapshot_name
:
str
,
*
,
zone
:
str
|
None
=
None
,
region
:
str
|
None
=
None
,
location
:
str
|
None
=
None
,
disk_project_id
:
str
|
None
=
None
,
)
-
>
compute_v1
.
Snapshot
:
"""
Create a snapshot of a disk.
You need to pass `zone` or `region` parameter relevant to the disk you want to
snapshot, but not both. Pass `zone` parameter for zonal disks and `region` for
regional disks.
Args:
project_id: project ID or project number of the Cloud project you want
to use to store the snapshot.
disk_name: name of the disk you want to snapshot.
snapshot_name: name of the snapshot to be created.
zone: name of the zone in which is the disk you want to snapshot (for zonal disks).
region: name of the region in which is the disk you want to snapshot (for regional disks).
location: The Cloud Storage multi-region or the Cloud Storage region where you
want to store your snapshot.
You can specify only one storage location. Available locations:
https://cloud.google.com/storage/docs/locations#available-locations
disk_project_id: project ID or project number of the Cloud project that
hosts the disk you want to snapshot. If not provided, will look for
the disk in the `project_id` project.
Returns:
The new snapshot instance.
"""
if
zone
is
None
and
region
is
None
:
raise
RuntimeError
(
"You need to specify `zone` or `region` for this function to work."
)
if
zone
is
not
None
and
region
is
not
None
:
raise
RuntimeError
(
"You can't set both `zone` and `region` parameters."
)
if
disk_project_id
is
None
:
disk_project_id
=
project_id
if
zone
is
not
None
:
disk_client
=
compute_v1
.
DisksClient
()
disk
=
disk_client
.
get
(
project
=
disk_project_id
,
zone
=
zone
,
disk
=
disk_name
)
else
:
regio_disk_client
=
compute_v1
.
RegionDisksClient
()
disk
=
regio_disk_client
.
get
(
project
=
disk_project_id
,
region
=
region
,
disk
=
disk_name
)
snapshot
=
compute_v1
.
Snapshot
()
snapshot
.
source_disk
=
disk
.
self_link
snapshot
.
name
=
snapshot_name
if
location
:
snapshot
.
storage_locations
=
[
location
]
snapshot_client
=
compute_v1
.
SnapshotsClient
()
operation
=
snapshot_client
.
insert
(
project
=
project_id
,
snapshot_resource
=
snapshot
)
wait_for_extended_operation
(
operation
,
"snapshot creation"
)
return
snapshot_client
.
get
(
project
=
project_id
,
snapshot
=
snapshot_name
)
Caution:
If you attempt to create a snapshot and the snapshotting process fails,
you won't be able to delete the original persistent disk until you have cleaned
up the failed snapshot. This failsafe helps to prevent the accidental deletion
of source data in the event of an unsuccessful backup.
Schedule a recurring backup
When you create a snapshot schedule, you create a
resource policy
that you can apply to one or more persistent disks. You can create snapshot
schedules in the following ways:
Create a snapshot schedule
and then
attach it to an existing persistent
disk
.
Create a new persistent disk with a snapshot schedule
.
A snapshot schedule includes the following properties:
Schedule name
Schedule description
Snapshot frequency (hourly, daily, weekly)
Snapshot start time
Region where the snapshot schedule is available
Source disk deletion policy for handling auto-generated snapshots
if the source disk is deleted
Retention policy to define how long to keep snapshots that are generated
from the snapshot schedule
Restrictions
A persistent disk can have at most 10 snapshot schedules attached to it
at a time.
You cannot create archive snapshots using a snapshot schedule.
You can create a maximum of 1,000 in-use snapshot schedules per region.
Snapshot schedules apply only in the project that they were created in.
Snapshot schedules cannot be used in other projects or organizations.
You might need to request an increase in
resource quota
through the console if you require additional resources in your region.
You cannot delete a snapshot schedule if it is attached to a disk. You must
detach the schedule
from all disks, then delete the schedule.
You can update an existing snapshot schedule to change the description,
schedule, and labels. To update other values for a snapshot schedule, you
must delete the snapshot schedule and create a new one.
For persistent disks that use a
customer-supplied encryption key
(CSEK)
, you can't
create snapshot schedules.
For persistent disks that use a
customer-managed encryption key
(CMEK)
, all snapshots created with a snapshot schedule are
automatically encrypted with the same key.
Create a schedule
Permissions required for this task
To perform this task, you must have the following
permissions
:
compute.resourcePolicies.create
on the project or organization
Create a snapshot schedule for your persistent disks using the Google Cloud console,
Google Cloud CLI, or the Compute Engine API. You must create your snapshot
schedule in the same region where your persistent disk resides. For example, if
your persistent disk resides in zone
us-west1-a
, your snapshot schedule must
reside in the
us-west1
region. For more information, see
Choose a storage location
.
Note:
If you use the gcloud CLI or the Google Cloud console, you
must always set a retention policy when creating a snapshot schedule. If you
make a request to the API directly, you can omit this field and your snapshots
will be retained indefinitely.
Console
In the Google Cloud console, go to the
VM instances
page.
Go to VM instances
school
The remaining steps will appear automatically in
        the Google Cloud console.
Select the project that contains your VM instances.
In the
Name
column, click the name of the VM that has the persistent disk to create a
      snapshot schedule for.
In
Storage
,
      click the name of the
Boot disk
or the
Additional disk
to create a snapshot
      schedule for.
Click
edit
Edit
. You might need to
       click the
more_vert
More actions
menu and then
edit
Edit
.
In
Snapshot schedule
, choose
Create a schedule
.
In
Name
, enter one of the following names for the snapshot schedule:
boot-disk-snapshot-schedule
attached-persistent-disk-snapshot-schedule
In the
Location
section,
choose your snapshot
          storage location
.
          The predefined or customized default location defined in your snapshot settings is
          automatically selected. Optionally, you can override the snapshot settings and store your
          snapshots in a custom storage location by doing the following:
Choose the type of storage location that you want for your snapshot.
Choose
Multi-regional
for higher availability at a higher cost.
Choose
Regional snapshots
for more control over the physical location of your data at a lower cost.
In the
Select location
field, select the specific region or multi-region that
              you want to use. To use the region or multi-region that is closest to your
              source disk, select
Based on disk's location
.
To finish creating the snapshot schedule, click
Create
.
To attach this snapshot schedule to the persistent disk, click
Save
.
gcloud
To create a snapshot schedule for persistent disks, use the
compute resource-policies create snapshot-schedule
gcloud
command. Set your schedule frequency to hourly, daily, or weekly.
gcloud
compute
resource
-
policies
create
snapshot
-
schedule
[
SCHEDULE_NAME
]
\
--description "[SCHEDULE_DESCRIPTION]" \
--max-retention-days [MAX_RETENTION_DAYS] \
--start-time [START_TIME] \
--hourly-schedule [SNAPSHOT_INTERVAL] \
--daily-schedule \
--weekly-schedule [SNAPSHOT_INTERVAL] \
--weekly-schedule-from-file [FILE_NAME] \
--on-source-disk-delete [DELETION_OPTION]
where:
[SCHEDULE_NAME]
is the name of the new snapshot schedule.
"[SCHEDULE_DESCRIPTION]"
is a description of the snapshot schedule.
 Use quotes around your description.
[MAX_RETENTION_DAYS]
is the number of days to retain the snapshot.
 For example, setting
3
would mean that snapshots are retained for 3
 days before they are deleted. You must set a retention policy of at
 least 1 day.
[START_TIME]
is the UTC start time. The time must start on the hour.
 For example:
2:00 PM PST is
22:00
.
If you set a start time of
22:13
, you will receive an error.
[SNAPSHOT_INTERVAL]
defines the interval at which you want
 snapshotting to occur. Set the hourly schedule using an integer
 between 1 and 23. Choose an hourly number that is evenly divided into 24.
 For example, setting
--hourly-schedule
to 12, means the snapshot
 is generated every 12 hours. For a weekly schedule define
 the days you want the snapshotting to occur. You must spell out the
 week days, they are not case-sensitive. The snapshot frequency flags
hourly-schedule
,
daily-schedule
, and
weekly-schedule
are
 mutually-exclusive. You must pick one for your snapshot schedule.
Note:
If you want to specify a weekly schedule with different days
 of the week and with different start times, use
--weekly-schedule-from-file
instead.
[FILE_NAME]
is the file name that contains the weekly snapshot
 schedule, if you choose to provide the schedule in this format.
 Note that you can specify weekly schedules on different days of the
 week and at different times using a file (but you cannot specify
 multiple weekly schedules directly on the command-line). For example,
 your file might specify a snapshot schedule on Monday and Wednesday:
[{"day": "MONDAY", "startTime": "04:00"}, {"day": "WEDNESDAY", "startTime": "02:00"}]
If you include a start time in your file, you do not need to set the
--start-time
flag. The schedule uses the UTC time standard.
[DELETION_OPTION]
determines what happens to your snapshots if the
 source disk is deleted. Choose either the default
keep-auto-snapshots
by omitting this flag, or use
apply-retention-policy
to apply a
 retention policy.
These are additional examples for setting up a snapshot schedule. In all
the following examples:
The disk deletion rule is included; the
--on-source-disk-delete
flag
is set to the default of
keep-auto-snapshots
to permanently keep all
auto-generated snapshots. The alternative is to set this flag to
apply-retention-policy
to use your snapshot retention policy.
The storage location is set the
US
so all generated snapshots will be
stored in the US multi-region.
The labels
env=dev
and
media=images
are applied to all generated
snapshots.
The retention policy is set to 10 days.
Hourly schedule:
In this example, the snapshot schedule starts at 22:00
UTC and occurs every 4 hours.
gcloud
compute
resource
-
policies
create
snapshot
-
schedule
SCHEDULE_NAME
\
--
description
"MY HOURLY SNAPSHOT SCHEDULE"
\
--
max
-
retention
-
days
10
\
--
start
-
time
22
:
00
\
--
hourly
-
schedule
4
\
--
region
us
-
west1
\
--
on
-
source
-
disk
-
delete
keep
-
auto
-
snapshots
\
--
snapshot
-
labels
env
=
dev
,
media
=
images
\
--
storage
-
location
US
Daily schedule:
In this example, the snapshot schedule starts at 22:00
UTC and occurs every day at the same time. The
--daily-schedule
flag must
be present, but not set to anything.
gcloud
compute
resource
-
policies
create
snapshot
-
schedule
SCHEDULE_NAME
\
--
description
"MY DAILY SNAPSHOT SCHEDULE"
\
--
max
-
retention
-
days
10
\
--
start
-
time
22
:
00
\
--
daily
-
schedule
\
--
region
us
-
west1
\
--
on
-
source
-
disk
-
delete
keep
-
auto
-
snapshots
\
--
snapshot
-
labels
env
=
dev
,
media
=
images
\
--
storage
-
location
US
Weekly schedule:
In this example, the snapshot schedule starts at 22:00
UTC and occurs every week on Tuesday and Thursday.
gcloud
compute
resource
-
policies
create
snapshot
-
schedule
SCHEDULE_NAME
\
--
description
"MY WEEKLY SNAPSHOT SCHEDULE"
\
--
max
-
retention
-
days
10
\
--
start
-
time
22
:
00
\
--
weekly
-
schedule
tuesday
,
thursday
\
--
region
us
-
west1
\
--
on
-
source
-
disk
-
delete
keep
-
auto
-
snapshots
\
--
snapshot
-
labels
env
=
dev
,
media
=
images
\
--
storage
-
location
US
API
In the API, construct a
POST
request to
resourcePolicies.insert
to create a snapshot schedule. At the minimum, you must include the snapshot
schedule name, snapshot storage regional location, and snapshot frequency.
By default, the
onSourceDiskDelete
parameter is set to
keepAutoSnapshots
.
This means that if the source disk is deleted, the auto-generated snapshot
for that disk is retained indefinitely. Alternatively, you can set the flag
to
applyRetentionPolicy
to apply your retention policy.
The following example sets a daily snapshot schedule that starts at 12:00
UTC and repeats every day. The example also sets a retention policy of 5
days; after 5 days, snapshots are automatically removed.
You can also include
snapshot locality options
and
snapshot labels
in your request to ensure your snapshots are stored in the location of your
choice.
POST
https
:
//
compute
.
googleapis
.
com
/
compute
/
v1
/
projects
/[
PROJECT_ID
]/
regions
/[
REGION
]/
resourcePolicies
{
"name"
:
"[SCHEDULE_NAME]"
,
"description"
:
"[SCHEDULE_DESCRIPTION]"
,
"snapshotSchedulePolicy"
:
{
"schedule"
:
{
"dailySchedule"
:
{
"startTime"
:
"12:00"
,
"daysInCycle"
:
"1"
}
}
,
"retentionPolicy"
:
{
"maxRetentionDays"
:
"5"
}
,
"snapshotProperties"
:
{
"guestFlush"
:
"False"
,
"labels"
:
{
"env"
:
"dev"
,
"media"
:
"images"
}
,
"storageLocations"
:
[
"US"
]
}
}
}
where:
[PROJECT_ID]
is the project name.
[REGION]
is the location of the snapshot schedule resource policy.
[SCHEDULE_DESCRIPTION]
is the description of the snapshot schedule.
[SCHEDULE_NAME]
is the name of the snapshot schedule.
Similarly, you can create a weekly or monthly schedule. Review the
API reference
for details specific to setting a weekly or monthly schedule.
For example, the following request creates a weekly schedule that runs
on Tuesday and Thursday, at 9:00 and 2:00 respectively.
POST
https
:
//
compute
.
googleapis
.
com
/
compute
/
v1
/
projects
/[
PROJECT_ID
]/
regions
/[
REGION
]/
resourcePolicies
{
"name"
:
"[SCHEDULE_NAME]"
,
"description"
:
"[SCHEDULE_DESCRIPTION]"
,
"snapshotSchedulePolicy"
:
{
"schedule"
:
{
"weeklySchedule"
:
{
"dayOfWeeks"
:
[
{
"day": "Monday",
"startTime": "9:00"
},
{
"day": "Thursday",
"startTime": "2:00"
}
]
}
}
,
"retentionPolicy"
:
{
"maxRetentionDays"
:
"5"
}
,
"snapshotProperties"
:
{
"guestFlush"
:
"False"
,
"labels"
:
{
"production"
:
"webserver"
}
,
"storageLocations"
:
[
"US"
]
}
}
}
Attach a snapshot schedule to a disk
Permissions required for this task
To perform this task, you must have the following
permissions
:
compute.disks.addResourcePolicies
on the disk
compute.resourcePolicies.use
on the resource policy to use
Once you have a schedule, attach it to an existing disk. Use the console,
gcloud
command, or the Compute Engine API method.
Console
Attach a snapshot schedule to an existing disk.
In the Google Cloud console, go to the
Disks
page.
Go to the Disks page
Select the name of the disk to which you want to attach a snapshot
schedule. This opens the
Manage disk
page.
On the
Manage disk
page, hover and click the
more_vert
More actions
menu and select
edit
Edit
.
Use the
Snapshot schedule
drop-down menu to add the schedule
to the disk. Or create a new schedule.
If you created a new schedule, click
Create
.
Click
Save
to complete the task.
gcloud
To attach a snapshot schedule to a disk, use the
disks add-resource-policies
gcloud
command.
gcloud
compute
disks
add
-
resource
-
policies
[
DISK_NAME
]
\
--resource-policies [SCHEDULE_NAME] \
--zone [ZONE]
where:
[DISK_NAME]
is the name of the existing disk.
[SCHEDULE_NAME]
is the name of the snapshot schedule.
[ZONE]
is the location of your disk.
API
In the API, construct a
POST
request to
disks.addResourcePolicies
to attach a snapshot schedule to an existing disk.
POST
https
:
//
compute
.
googleapis
.
com
/
compute
/
v1
/
projects
/[
PROJECT_ID
]/
zones
/[
ZONE
]/
disks
/[
DISK_NAME
]/
addResourcePolicies
{
"resourcePolicies"
:
[
"regions/[REGION
]/
resourcePolicies
/[
SCHEDULE_NAME
]
"
]
}
where:
[PROJECT_ID]
is the project name.
[ZONE]
is the location of the disk.
[REGION]
is the location of the snapshot schedule.
[DISK_NAME]
is the name of the disk.
[SCHEDULE_NAME]
is the name of the snapshot schedule in that
  region you are applying to this disk.
Restore data from a snapshot
If you backed up a boot or non-boot disk with a snapshot, you can create a new
disk based on the snapshot.
Restrictions
The new disk must be at least the same size as the original source
disk for the snapshot. If you create a disk that is larger
than the original source disk for the snapshot, you must
resize the file
system on that persistent disk
to include the additional disk space. Depending on your operating system and
file system type, you might need to use a different file system resizing tool.
For more information, see your operating system documentation.
Create a disk from a snapshot and attach it to a VM
Note:
You must create the disk in the same zone as your instance.
Permissions required for this task
To perform this task, you must have the following
permissions
:
compute.disks.create
on the project to create a new disk
compute.instances.attachDisk
on the VM instance
compute.disks.use
permission on the disk to attach
Console
In the Google Cloud console, go to the
Snapshots
page.
Go to Snapshots
Find the name of the snapshot that you want to restore.
Go to the
Disks
page.
Go to the Disks page
Click
Create new disk
.
Specify the following configuration parameters:
A name for the disk.
A type for the disk.
Optionally, you can override the default region and zone selection.
You can select any region and zone, regardless of the storage
location of the source snapshot.
Under
Source type
, click
Snapshot
.
Select the name of the snapshot to restore.
Select the size of the new disk, in gigabytes. This number must be equal
to or larger than the original source disk for the snapshot.
Click
Create
to create the disk.
You can then attach the new disk to an existing instance.
Go to the
VM instances
page.
Go to the VM instances page
Click the name of the instance where you want to restore your non-boot disk.
At the top of the instance details page, click
Edit
.
Under
Additional disks
, click
Attach existing disk
.
Select the name of the new disk made from your snapshot.
Click
Done
to attach the disk.
At the bottom of the instance details page, click
Save
to apply
your changes to the instance.
gcloud
Use the
gcloud compute snapshots list
command
command to find the name of the snapshot you want to restore:
gcloud compute snapshots list
Use the
gcloud compute snapshots describe
command
command to find the size of the snapshot you want to restore:
gcloud compute snapshots describe
SNAPSHOT_NAME
Replace
SNAPSHOT_NAME
with the name of the snapshot being
restored.
Use the
gcloud compute disks create
command
command to create a new
regional
or
zonal
disk from your
snapshot. If you need an SSD persistent disk for additional throughput
or IOPS, include the
--type
flag and specify
pd-ssd
.
gcloud
compute
disks
create
DISK_NAME
\
--
size
=
DISK_SIZE
\
--
source
-
snapshot
=
SNAPSHOT_NAME
\
--
type
=
DISK_TYPE
Replace the following:
DISK_NAME
: the name of the new disk.
DISK_SIZE
: The size of the new disk, in gigabytes. This
number must be equal to or larger than the original source disk for
the snapshot.
SNAPSHOT_NAME
: the name of the snapshot being restored.
DISK_TYPE
: full or partial URL for the
type
of the disk. For example,
https://www.googleapis.com/compute/v1/projects/
PROJECT_ID
/zones/
ZONE
/diskTypes/pd-ssd
.
Attach the new disk to an existing instance by
using the
gcloud compute instances attach-disk
command
:
gcloud compute instances attach-disk
INSTANCE_NAME
\
    --disk
DISK_NAME
Replace the following:
INSTANCE_NAME
is the name of the instance.
DISK_NAME
is the name of the disk made from your snapshot.
API
Construct a
GET
request to
snapshots.list
to display the list of snapshots in your project.
GET https://compute.googleapis.com/compute/v1/projects/
PROJECT_ID
/global/snapshots
Replace
PROJECT_ID
with your project ID.
Construct a
POST
request to create a zonal disk using
the
disks.insert
method. Include the
name
,
sizeGb
, and
type
properties. To restore a disk using a snapshot, you must include
the
sourceSnapshot
property.
POST
https
:
//compute.googleapis.com/compute/v1/projects/
PROJECT_ID
/zones/
ZONE
/disks
{
"name"
:
"
DISK_NAME
"
,
"sizeGb"
:
"
DISK_SIZE
"
,
"type"
:
"zones/
ZONE
/diskTypes/
DISK_TYPE
"
"sourceSnapshot"
:
"
SNAPSHOT_NAME
"
}
Replace the following:
PROJECT_ID
: your project ID.
ZONE
the zone where your instance and new
disk are located.
DISK_NAME
: the name of the new disk.
DISK_SIZE
: the size of the new disk, in gigabytes. This
number must be equal to or larger than the original source disk for
the snapshot.
DISK_TYPE
: full or partial URL for the
type
of the disk. For example
https://www.googleapis.com/compute/v1/projects/
PROJECT_ID
/zones/
ZONE
/diskTypes/pd-ssd
.
SNAPSHOT_NAME
: the source snapshot for the disk you are restoring.
You can then attach the new disk to an existing instance by
constructing a
POST
request to the
instances.attachDisk
method
,
and including the URL to the zonal disk that you just created from your
snapshot.
POST https://compute.googleapis.com/compute/v1/projects/
PROJECT_ID
/zones/
ZONE
/instances/
INSTANCE_NAME
/attachDisk

{
 "source": "/compute/v1/projects/
PROJECT_ID
/zones/
ZONE
/disks/
DISK_NAME
"
}
Replace the following:
PROJECT_ID
is your project ID.
ZONE
is the zone where your instance and new disk are located.
INSTANCE_NAME
is the name of the instance where you are adding the
new disk.
DISK_NAME
is the name of the new disk.
Go
Go
Before trying this sample, follow the
Go
setup instructions in the
Vertex AI quickstart using
            client libraries
.
        
      
      
  For more information, see the
Vertex AI
Go
API
    reference documentation
.
To authenticate to Vertex AI, set up Application Default Credentials.
      For more information, see
Set up authentication for a local development environment
.
import
(
"context"
"fmt"
"io"
compute
"cloud.google.com/go/compute/apiv1"
computepb
"cloud.google.com/go/compute/apiv1/computepb"
"google.golang.org/protobuf/proto"
)
// createDiskFromSnapshot creates a new disk in a project in given zone.
func
createDiskFromSnapshot
(
w
io
.
Writer
,
projectID
,
zone
,
diskName
,
diskType
,
snapshotLink
string
,
diskSizeGb
int64
,
)
error
{
// projectID := "your_project_id"
// zone := "us-west3-b" // should match diskType below
// diskName := "your_disk_name"
// diskType := "zones/us-west3-b/diskTypes/pd-ssd"
// snapshotLink := "projects/your_project_id/global/snapshots/snapshot_name"
// diskSizeGb := 120
ctx
:=
context
.
Background
()
disksClient
,
err
:=
compute
.
NewDisksRESTClient
(
ctx
)
if
err
!=
nil
{
return
fmt
.
Errorf
(
"NewDisksRESTClient: %w"
,
err
)
}
defer
disksClient
.
Close
()
req
:=
&
computepb
.
InsertDiskRequest
{
Project
:
projectID
,
Zone
:
zone
,
DiskResource
:
&
computepb
.
Disk
{
Name
:
proto
.
String
(
diskName
),
Zone
:
proto
.
String
(
zone
),
Type
:
proto
.
String
(
diskType
),
SourceSnapshot
:
proto
.
String
(
snapshotLink
),
SizeGb
:
proto
.
Int64
(
diskSizeGb
),
},
}
op
,
err
:=
disksClient
.
Insert
(
ctx
,
req
)
if
err
!=
nil
{
return
fmt
.
Errorf
(
"unable to create disk: %w"
,
err
)
}
if
err
=
op
.
Wait
(
ctx
);
err
!=
nil
{
return
fmt
.
Errorf
(
"unable to wait for the operation: %w"
,
err
)
}
fmt
.
Fprintf
(
w
,
"Disk created\n"
)
return
nil
}
Java
Java
Before trying this sample, follow the
Java
setup instructions in the
Vertex AI quickstart using
            client libraries
.
        
      
      
  For more information, see the
Vertex AI
Java
API
    reference documentation
.
To authenticate to Vertex AI, set up Application Default Credentials.
      For more information, see
Set up authentication for a local development environment
.
import
com.google.cloud.compute.v1.
Disk
;
import
com.google.cloud.compute.v1.
DisksClient
;
import
com.google.cloud.compute.v1.
InsertDiskRequest
;
import
com.google.cloud.compute.v1.
Operation
;
import
java.io.IOException
;
import
java.util.concurrent.ExecutionException
;
import
java.util.concurrent.TimeUnit
;
import
java.util.concurrent.TimeoutException
;
public
class
CreateDiskFromSnapshot
{
public
static
void
main
(
String
[]
args
)
throws
IOException
,
ExecutionException
,
InterruptedException
,
TimeoutException
{
// TODO(developer): Replace these variables before running the sample.
// Project ID or project number of the Cloud project you want to use.
String
projectId
=
"YOUR_PROJECT_ID"
;
// Name of the zone in which you want to create the disk.
String
zone
=
"europe-central2-b"
;
// Name of the disk you want to create.
String
diskName
=
"YOUR_DISK_NAME"
;
// The type of disk you want to create. This value uses the following format:
// "zones/{zone}/diskTypes/(pd-standard|pd-ssd|pd-balanced|pd-extreme)".
// For example: "zones/us-west3-b/diskTypes/pd-ssd"
String
diskType
=
String
.
format
(
"zones/%s/diskTypes/pd-ssd"
,
zone
);
// Size of the new disk in gigabytes.
long
diskSizeGb
=
10
;
// The full path and name of the snapshot that you want to use as the source for the new disk.
// This value uses the following format:
// "projects/{projectName}/global/snapshots/{snapshotName}"
String
snapshotLink
=
String
.
format
(
"projects/%s/global/snapshots/%s"
,
projectId
,
"SNAPSHOT_NAME"
);
createDiskFromSnapshot
(
projectId
,
zone
,
diskName
,
diskType
,
diskSizeGb
,
snapshotLink
);
}
// Creates a new disk in a project in given zone, using a snapshot.
public
static
void
createDiskFromSnapshot
(
String
projectId
,
String
zone
,
String
diskName
,
String
diskType
,
long
diskSizeGb
,
String
snapshotLink
)
throws
IOException
,
ExecutionException
,
InterruptedException
,
TimeoutException
{
// Initialize client that will be used to send requests. This client only needs to be created
// once, and can be reused for multiple requests. After completing all of your requests, call
// the `disksClient.close()` method on the client to safely
// clean up any remaining background resources.
try
(
DisksClient
disksClient
=
DisksClient
.
create
())
{
// Set the disk properties and the source snapshot.
Disk
disk
=
Disk
.
newBuilder
()
.
setName
(
diskName
)
.
setZone
(
zone
)
.
setSizeGb
(
diskSizeGb
)
.
setType
(
diskType
)
.
setSourceSnapshot
(
snapshotLink
)
.
build
();
// Create the insert disk request.
InsertDiskRequest
insertDiskRequest
=
InsertDiskRequest
.
newBuilder
()
.
setProject
(
projectId
)
.
setZone
(
zone
)
.
setDiskResource
(
disk
)
.
build
();
// Wait for the create disk operation to complete.
Operation
response
=
disksClient
.
insertAsync
(
insertDiskRequest
)
.
get
(
3
,
TimeUnit
.
MINUTES
);
if
(
response
.
hasError
())
{
System
.
out
.
println
(
"Disk creation failed!"
+
response
);
return
;
}
System
.
out
.
println
(
"Disk created. Operation Status: "
+
response
.
getStatus
());
}
}
}
Node.js
Node.js
Before trying this sample, follow the
Node.js
setup instructions in the
Vertex AI quickstart using
            client libraries
.
        
      
      
  For more information, see the
Vertex AI
Node.js
API
    reference documentation
.
To authenticate to Vertex AI, set up Application Default Credentials.
      For more information, see
Set up authentication for a local development environment
.
/**
* TODO(developer): Uncomment and replace these variables before running the sample.
*/
// const projectId = 'YOUR_PROJECT_ID';
// const zone = 'europe-central2-b';
// const diskName = 'YOUR_DISK_NAME';
// const diskType = 'zones/us-west3-b/diskTypes/pd-ssd';
// const diskSizeGb = 10;
// const snapshotLink = 'projects/project_name/global/snapshots/snapshot_name';
const
compute
=
require
(
'
@google-cloud/compute
'
);
async
function
createDiskFromSnapshot
()
{
const
disksClient
=
new
compute
.
DisksClient
();
const
[
response
]
=
await
disksClient
.
insert
({
project
:
projectId
,
zone
,
diskResource
:
{
sizeGb
:
diskSizeGb
,
name
:
diskName
,
zone
,
type
:
diskType
,
sourceSnapshot
:
snapshotLink
,
},
});
let
operation
=
response
.
latestResponse
;
const
operationsClient
=
new
compute
.
ZoneOperationsClient
();
// Wait for the create disk operation to complete.
while
(
operation
.
status
!==
'DONE'
)
{
[
operation
]
=
await
operationsClient
.
wait
({
operation
:
operation
.
name
,
project
:
projectId
,
zone
:
operation
.
zone
.
split
(
'/'
).
pop
(),
});
}
console
.
log
(
'Disk created.'
);
}
createDiskFromSnapshot
();
Python
Python
To learn how to install or update the Vertex AI SDK for Python, see
Install the Vertex AI SDK for Python
.
      
        For more information, see the
Python
API reference documentation
.
from
__future__
import
annotations
import
sys
from
typing
import
Any
from
google.api_core.extended_operation
import
ExtendedOperation
from
google.cloud
import
compute_v1
def
wait_for_extended_operation
(
operation
:
ExtendedOperation
,
verbose_name
:
str
=
"operation"
,
timeout
:
int
=
300
)
-
>
Any
:
"""
Waits for the extended (long-running) operation to complete.
If the operation is successful, it will return its result.
If the operation ends with an error, an exception will be raised.
If there were any warnings during the execution of the operation
they will be printed to sys.stderr.
Args:
operation: a long-running operation you want to wait on.
verbose_name: (optional) a more verbose name of the operation,
used only during error and warning reporting.
timeout: how long (in seconds) to wait for operation to finish.
If None, wait indefinitely.
Returns:
Whatever the operation.result() returns.
Raises:
This method will raise the exception received from `operation.exception()`
or RuntimeError if there is no exception set, but there is an `error_code`
set for the `operation`.
In case of an operation taking longer than `timeout` seconds to complete,
a `concurrent.futures.TimeoutError` will be raised.
"""
result
=
operation
.
result
(
timeout
=
timeout
)
if
operation
.
error_code
:
print
(
f
"Error during
{
verbose_name
}
: [Code:
{
operation
.
error_code
}
]:
{
operation
.
error_message
}
"
,
file
=
sys
.
stderr
,
flush
=
True
,
)
print
(
f
"Operation ID:
{
operation
.
name
}
"
,
file
=
sys
.
stderr
,
flush
=
True
)
raise
operation
.
exception
()
or
RuntimeError
(
operation
.
error_message
)
if
operation
.
warnings
:
print
(
f
"Warnings during
{
verbose_name
}
:
\n
"
,
file
=
sys
.
stderr
,
flush
=
True
)
for
warning
in
operation
.
warnings
:
print
(
f
" -
{
warning
.
code
}
:
{
warning
.
message
}
"
,
file
=
sys
.
stderr
,
flush
=
True
)
return
result
def
create_disk_from_snapshot
(
project_id
:
str
,
zone
:
str
,
disk_name
:
str
,
disk_type
:
str
,
disk_size_gb
:
int
,
snapshot_link
:
str
,
)
-
>
compute_v1
.
Disk
:
"""
Creates a new disk in a project in given zone.
Args:
project_id: project ID or project number of the Cloud project you want to use.
zone: name of the zone in which you want to create the disk.
disk_name: name of the disk you want to create.
disk_type: the type of disk you want to create. This value uses the following format:
"zones/{zone}/diskTypes/(pd-standard|pd-ssd|pd-balanced|pd-extreme)".
For example: "zones/us-west3-b/diskTypes/pd-ssd"
disk_size_gb: size of the new disk in gigabytes
snapshot_link: a link to the snapshot you want to use as a source for the new disk.
This value uses the following format: "projects/{project_name}/global/snapshots/{snapshot_name}"
Returns:
An unattached Disk instance.
"""
disk_client
=
compute_v1
.
DisksClient
()
disk
=
compute_v1
.
Disk
()
disk
.
zone
=
zone
disk
.
size_gb
=
disk_size_gb
disk
.
source_snapshot
=
snapshot_link
disk
.
type_
=
disk_type
disk
.
name
=
disk_name
operation
=
disk_client
.
insert
(
project
=
project_id
,
zone
=
zone
,
disk_resource
=
disk
)
wait_for_extended_operation
(
operation
,
"disk creation"
)
return
disk_client
.
get
(
project
=
project_id
,
zone
=
zone
,
disk
=
disk_name
)
Mount the disk
In the terminal, use the
lsblk
command to list the disks that are
attached to your instance and find the disk that you want to mount.
$
sudo
lsblk
NAME
MAJ:MIN
RM
SIZE
RO
TYPE
MOUNTPOINT
sda
8
:0
0
10G
0
disk
└─sda1
8
:1
0
10G
0
part
/
sdb
8
:16
0
250G
0
disk
In this example,
sdb
is the device name for the new blank persistent
disk.
Use the
mount tool
to mount the disk to the instance, and enable the
discard
option:
$
sudo
mount
-o
discard,defaults
/dev/
DEVICE_NAME
/home/jupyter
Replace the following:
DEVICE_NAME
: the device name of the disk to
mount.
Configure read and write permissions on the disk. For this example,
grant write access to the disk for all users.
$
sudo
chmod
a+w
/home/jupyter
What's next
Learn how to
save a notebook to GitHub
Learn more about
creating snapshots
.
Learn more about
scheduling snapshots
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/cmek.txt
Customer-managed encryption keys  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Customer-managed encryption keys
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
By default, Vertex AI Workbench encrypts customer content at
    rest. Vertex AI Workbench handles encryption for you without any
  additional actions on your part. This option is called
Google default encryption
.
If you want to control your encryption keys, then you can use customer-managed encryption keys
  (CMEKs) in
Cloud KMS
with CMEK-integrated services including
  Vertex AI Workbench. Using Cloud KMS keys gives you control over their protection
  level, location, rotation schedule, usage and access permissions, and cryptographic boundaries.
  Using Cloud KMS also lets
you view audit logs and control key lifecycles.
  
  Instead of Google owning and managing the symmetric
key encryption keys (KEKs)
that protect your data, you control and
  manage these keys in Cloud KMS.
After you set up your resources with CMEKs, the experience of accessing your
  Vertex AI Workbench resources is similar to using Google default encryption.
  For more information about your encryption
  options, see
Customer-managed encryption keys (CMEK)
.
This page describes some specific benefits and limitations of using CMEK with
user-managed notebooks and shows
how to configure a new user-managed notebooks instance
to use CMEK.
Benefits of CMEK
In general, CMEK is most useful if you need full control over the keys used to
encrypt your data. With CMEK, you can manage your keys within
Cloud Key Management Service. For example, you can rotate or disable a key or you can set
up a rotation schedule by using the Cloud KMS API.
When you run a user-managed notebooks instance,
your instance runs
on a virtual machine (VM) managed by Vertex AI Workbench.
When you enable
CMEK for a user-managed notebooks instance,
Vertex AI Workbench uses the key that you designate,
rather than a key managed by Google, to encrypt
data on the boot disks of the VM.
The CMEK key
doesn't
encrypt metadata, like the instance's name and region,
associated with your user-managed notebooks instance.
Metadata associated with
user-managed notebooks instances is always
encrypted using Google's default encryption mechanism.
Limitations of CMEK
To decrease latency and to prevent cases where resources depend on
services that are spread across multiple failure domains, Google recommends
that you protect regional
user-managed notebooks instances with keys in the same location.
You can encrypt regional user-managed notebooks instances
by using keys in the same location or in the global location. For example,
you can encrypt data in a disk in zone
us-west1-a
by using
a key in
us-west1
or
global
.
You can encrypt global instances by using keys in any location.
Configuring CMEK for
user-managed notebooks
doesn't
automatically configure CMEK
for other Google Cloud products that you use. To use CMEK to encrypt
data in other Google Cloud products, you must complete additional
configuration.
Configure CMEK for your user-managed notebooks instance
The following sections describe how to create a
key ring and key in Cloud Key Management Service,
grant the service account encrypter and decrypter permissions for your key,
and create a user-managed notebooks instance that uses CMEK.
Before you begin
We recommend using a setup that supports a
separation of
duties
. To configure CMEK for user-managed notebooks, you can use
two separate Google Cloud projects:
A Cloud KMS project: a project for managing your encryption key
A user-managed notebooks project: a project for accessing
user-managed notebooks instances and interacting with any
other Google Cloud products that you need for your use case
Alternatively, you can use a single Google Cloud project. To do so,
use the same project for all of the following tasks.
Set up the Cloud KMS project
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Cloud KMS API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Cloud KMS API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Set up the user-managed notebooks project
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Set up the Google Cloud CLI
The gcloud CLI is required for some steps on this page and optional
for others.
Install
the Google Cloud CLI.
        
          After installation,
initialize
the Google Cloud CLI by running the following command:
gcloud
init
If you're using an external identity provider (IdP), you must first
sign in to the gcloud CLI with your federated identity
.
Note:
You can run the gcloud CLI in
      the Google Cloud console without installing the Google Cloud CLI. To run the
      gcloud CLI in the Google Cloud console,
use
        Cloud Shell
.
Create a key ring and key
When you create a key ring and key, keep the following requirements in mind:
When you choose your key ring's location, use either
global
or the location where your user-managed notebooks instance
will be.
Make sure to create your key ring and key in your Cloud KMS project.
To create a key ring and a key, see
Create symmetric encryption keys
.
Grant user-managed notebooks permissions
To use CMEK for your user-managed notebooks instance,
you must grant your user-managed notebooks instance permission to
encrypt and decrypt data using your key.
Determine the service account to use
User-managed notebooks use a
service account
to run your
user-managed notebooks instance. This service account
is either the default Compute Engine service account or
a service account that you specify when you create the instance.
If the service account is the default Compute Engine service account,
complete the following to find the service account's email address:
In the Google Cloud console, go to the
IAM
page.
Go to IAM
Find the principal with the name
Compute Engine default service account
.
Make note of the email address for this service account, and use it in the
following steps to grant it permission to encrypt and decrypt data using
your key.
Grant the service account permission
Grant permission by using the Google Cloud console or
by using the Google Cloud CLI.
Console
In the Google Cloud console, go to the
Key management
page.
Go to Key management
Select your Cloud KMS project.
Click the name of the  key ring that you created in
Create a key ring and key
. The
Key ring details
page opens.
Select the checkbox for the key that you created in
Create a key ring and key
.
If an info panel labeled with the name of your key isn't already
open, click
Show info panel
.
In the info panel, click
person_add
Add member
.
The
Add members to "
KEY_NAME
"
dialog opens. In this
dialog, do the following:
In the
New members
field, enter the service account email
address for your instance.
In the
Select a role
list, click
Cloud KMS
and then select the
Cloud KMS CryptoKey Encrypter/Decrypter
role.
Click
Save
.
gcloud
Run the following command:
gcloud kms keys add-iam-policy-binding
KEY_NAME
\
    --keyring=
KEY_RING_NAME
\
    --location=
REGION
\
    --project=
KMS_PROJECT_ID
\
    --member=serviceAccount:
SERVICE_ACCOUNT
\
    --role=roles/cloudkms.cryptoKeyEncrypterDecrypter
Replace the following:
KEY_NAME
: the name of the key that you created
in
Create a key ring and key
KEY_RING_NAME
: the key ring that you created
in
Create a key ring and key
REGION
: the region where you created your key
ring
KMS_PROJECT_ID
: the ID of your
Cloud KMS project
SERVICE_ACCOUNT
: the service account email address
for your instance
Create a user-managed notebooks instance with CMEK
After you have granted your user-managed notebooks instance
permission
to encrypt and decrypt data
using your key, you can create a user-managed notebooks instance
that encrypts data using this key.
The following examples show how to encrypt and decrypt data
using your key by using the Google Cloud console
or gcloud CLI.
Console
To create
a user-managed notebooks instance with a
customer-managed encryption key, use the following steps:
In the Google Cloud console, go to the
User-managed notebooks
page.
Or go to
notebook.new
(https://notebook.new) and skip the next step.
Go to User-managed notebooks
Click
add_box
New notebook
,
and then select
Customize
.
On the
Create a user-managed notebook
page,
in the
Details
section,
provide the following information for your new instance:
Name
: a name for your new instance
Region
: the region that your key and key ring are in
Zone
: a zone within the region that you selected
Select the
Disks
section.
To use customer-managed encryption keys, under
Encryption
, select
Customer-managed encryption key (CMEK)
.
Click
Select a customer-managed key
.
If the customer-managed key that you want to use is in the list,
select it.
If the customer-managed key that you want to use isn't in the list,
enter the resource ID for your customer-managed key. The resource
ID for your customer-managed key looks like this:
projects/
NOTEBOOKS_PROJECT_NUMBER
/locations/global/keyRings/
KEY_RING_NAME
/cryptoKeys/
KEY_NAME
Replace the following:
NOTEBOOKS_PROJECT_NUMBER
: the ID of your
user-managed notebooks project
KEY_RING_NAME
: the key ring that you created
in
Create a key ring and key
KEY_NAME
: the name of the key that you
created in
Create a key ring and key
Complete the rest of the instance creation dialog,
and then click
Create
.
Vertex AI Workbench creates
a user-managed notebooks instance based on your
specified properties and automatically starts the instance. When the
instance is ready to use, Vertex AI Workbench activates an
Open JupyterLab
link.
gcloud
To use the gcloud CLI to create
a user-managed notebooks instance with a
customer-managed encryption key, run the following command.
This example assumes that you want to create
a user-managed notebooks instance
with an
n1-standard-1
machine type and a
100 GB standard persistent boot disk.
gcloud
notebooks
instances
create
notebook-vm-cmek
\
--location
=
REGION
\
--vm-image-family
=
IMAGE_FAMILY
\
--vm-image-project
=
deeplearning-platform-release
\
--machine-type
=
"n1-standard-1"
\
--boot-disk-type
=
"PD_STANDARD"
\
--boot-disk-size
=
100
\
--kms-key
=
KEY_NAME
\
--kms-project
=
KMS_PROJECT_ID
\
--kms-location
=
REGION
\
--kms-keyring
=
KEY_RING_NAME
\
--disk-encryption
=
CMEK
\
--metadata
=
'proxy-mode=project_editors'
Replace the following:
REGION
: the region where you created your key ring
and where you plan to create your user-managed notebooks
instance
IMAGE_FAMILY
: the
image family
that you want to use to create your user-managed notebooks
instance
KEY_NAME
: the name of the key that you created in
Create a key ring and key
KMS_PROJECT_ID
: the ID of your
Cloud KMS project
KEY_RING_NAME
: the key ring that you created in
Create a key ring and key
What's next
Learn more about
CMEK on Google Cloud
Learn
how to use CMEK with other Google Cloud
products
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/create-new.txt
Vertex AI deprecations  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Vertex AI
Release notes
Glossary
Pricing
Quotas and limits
Locations
Support policy
Shared responsibility
Vertex AI framework support policy
Supported frameworks list
Security bulletins
Service Level Agreement
Security controls
Vertex AI deprecations
AI Agent Ecosystem partners
Overview
AI Agent Ecosystem partners
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI
Resources
Send feedback
Vertex AI deprecations
Stay organized with collections
Save and categorize content based on your preferences.
The
Google Cloud Platform Terms of Service (section "Discontinuation of Services")
defines the deprecation policy that applies to Vertex AI.
  The
deprecation policy
only applies to the services,
  features, or products listed therein.
After a service, feature, or product is officially
  deprecated, it continues to be available for at least the period of time defined in the
  Terms of Service. After this period of time, the service is scheduled for shutdown.
Feature
Deprecated date
Shutdown date
Details
AutoML Video
July 31, 2024
July 31, 2025
AutoML Video is no longer available. See
Video
        tuning
for detailed instructions for fine-tuning Gemini on
        video data using supervised learning.
AutoML Text
September 15, 2024
June 15, 2025
Starting on September 15, 2024, you can only customize Text
        classification, entity extraction, and sentiment analysis objectives by
        moving to Vertex AI Gemini prompts and tuning. Training or
        updating models for Vertex AI AutoML for
        Text classification, entity extraction, and sentiment analysis
        objectives will no longer be available. You can continue using
        existing Vertex AI AutoML Text models until
        June 15, 2025. For more information about how Gemini offers enhanced
        user experience through improved prompting capabilities, see
Introduction
        to tuning
.
Legacy AI Platform Training
January 23, 2023
April 7, 2025
Migrate to
        Vertex AI custom training
, which includes all
        functionality of legacy AI Platform Training as well as new features.
Legacy AI Platform Prediction
January 23, 2023
April 7, 2025
All models, associated metadata, and deployments were deleted after
        the shutdown date.
Migrate your resources
        to Vertex AI prediction
to get the latest machine learning
        features, simplify end-to-end journeys, and productionize models
        with MLOps.
Vertex AI Workbench managed notebooks
January 16, 2024
April 14, 2025
On April 14, 2025, support for
          managed notebooks ended, and the ability to create
          managed notebooks instances was removed. Existing instances will
          continue to function until
          March 30, 2026, but patches,
          updates, and upgrades won't be available.
On March 30, 2026, all
          managed notebooks instances and their associated data will be
          deleted. There is no recovery of managed notebooks instances
          after deletion. There isn't a way to convert these instances
          to Compute Engine virtual machines (VMs). To continue using
          Vertex AI Workbench,
migrate
          your managed notebooks instances to
          Vertex AI Workbench instances
before
          March 30, 2026.
Vertex AI Workbench user-managed notebooks
January 16, 2024
April 14, 2025
On April 14, 2025, support for
          user-managed notebooks ended, and the ability to create
          user-managed notebooks instances was removed. Existing instances
          will continue to function until
          March 30, 2026, but patches,
          updates, and upgrades won't be available.
On March 30, 2026, your
          user-managed notebooks instances will be converted to standard
          Compute Engine virtual machines (VMs). You will have full control
          over these VMs, but they won't be supported or available
          through Vertex AI Workbench. You will incur costs for
          these Compute Engine VMs, so we recommend that you delete
          any VMs that you don't plan to use. To continue using
          Vertex AI Workbench,
migrate
          your user-managed notebooks instances to
          Vertex AI Workbench instances
before
          March 30, 2026.
Legacy AI Platform Pipelines
July 31, 2023
January 31, 2025
Migrate to
        Vertex AI Pipelines
, which includes all functionality of
        legacy AI Platform Pipelines as well as new features.
Legacy AI Platform Data Labeling Service
January 23, 2023
January 31, 2025
For new labeling tasks, you can
add labels
        using the Google Cloud console
or access data labeling solutions
        from our partners in the
Google Cloud
        Console Marketplace
.
Vertex AI Data Labeling Service
June 30, 2023
October 3, 2024
For new labeling tasks, you can
add labels
        using the Google Cloud console
or access data labeling solutions
        from our partners in the
Google Cloud
        Console Marketplace
.
Legacy AutoML Natural Language
July 31, 2023
August 7, 2024
New models can no longer be trained nor deployed on the
        legacy platform. Already deployed models stopped working on
        May 30, 2024. All the functionality of legacy Vertex AI and
        new features are available on the Vertex AI platform. See
Migrate to
        Vertex AI
to learn how to migrate your resources.
Legacy AutoML Video Intelligence
January 23, 2023
July 31, 2024
Migrate to
        Vertex AI
, which includes all functionality of
        legacy AutoML Video Intelligence as well as new features.
Legacy AutoML Vision
January 23, 2023
July 31, 2024
Migrate to
        Vertex AI
, which includes all functionality of
        legacy AutoML Vision as well as new features.
Legacy AutoML Tables
January 23, 2023
July 24, 2024
Migrate to
        Vertex AI
, which includes all functionality of
        legacy AutoML Tables as well as new features.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/create-specific-version.txt
Create a specific version of a Vertex AI Workbench user-managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create a specific version of a user-managed notebooks instance
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
This page describes how you can create
a user-managed notebooks instance based on a specific
Deep Learning VM Images
version.
Why you might want to create a specific version
To ensure that your user-managed notebooks instance has software
that is compatible with your code or application, you might want to create
a specific version.
User-managed notebooks instances are created by using Deep Learning VM images. Deep Learning VM
images are updated frequently, and specific versions of preinstalled software
and packages vary from version to version.
To learn more about specific Deep Learning VM versions,
see the
Deep Learning VM
release notes
.
After you create a specific version of a user-managed notebooks instance, you can upgrade it. Upgrading the instance updates the preinstalled software and packages. For more information, see
Upgrade a user-managed
notebooks instance's environment
.
Before you begin
Before you can create a user-managed notebooks instance,
you must have a
Google Cloud project and enable the Notebooks API
for that project.
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
If you plan to use GPUs with your
  user-managed notebooks instance,
check the quotas page in the
  Google Cloud console
to ensure that you have enough GPUs available in your project. If GPUs
  are not listed on the quotas page, or you require additional GPU quota,
  you can request a quota increase. See
Requesting an increase in
  quota
on the
  Compute Engine
Resource quotas
page.
Required roles
If you created the project, you have the
  Owner (
roles/owner
) IAM role on the project,
  which includes all required permissions. Skip this section and
  start creating your user-managed notebooks instance. If you didn't
  create the project yourself, continue in this section.
To get the permissions that
      you need to create a Vertex AI Workbench user-managed notebooks instance,
    
      ask your administrator to grant you the
    following IAM roles on the project:
Notebooks Admin (
roles/notebooks.admin
)
Service Account User (
roles/iam.serviceAccountUser
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Find the specific version that you want
To create a user-managed notebooks instance based on a specific
Deep Learning VM version, you must know
the image name of the specific Deep Learning VM
version that you want to use.
Each release of Deep Learning VM includes updates to
many different images, and each image in the release has its own
image name.
To find the specific image name that you want:
Find the Deep Learning VM release number
that you want to get image names for.
Release numbers are included in the
Deep Learning VM
release notes
.
Release numbers are in the form of an
M
followed by
the number of the release, for example,
M79
.
To list the image names for a specific Deep Learning VM
release, run the following command.
gcloud
compute
images
list
--project
=
"deeplearning-platform-release"
\
--format
=
"value(name)"
\
--filter
=
"labels.release=
RELEASE_NUMBER
"
\
--show-deprecated
Replace
RELEASE_NUMBER
with
a Deep Learning VM release number, such as
M79
.
Find the image name that you want to use.
Create a specific version from the command line
To create a specific version of
a user-managed notebooks instance from
the command line, complete the following steps:
Run the following
gcloud
notebooks
command:
gcloud
notebooks
instances
create
INSTANCE_NAME
\
--vm-image-project
=
"deeplearning-platform-release"
\
--vm-image-name
=
VM_IMAGE_NAME
\
--machine-type
=
MACHINE_TYPE
\
--location
=
LOCATION
Replace the following:
INSTANCE_NAME
: the name of your new
instance
VM_IMAGE_NAME
: the image name
that you want to use to create your instance
MACHINE_TYPE
: the
machine
type
of your instance's VM
LOCATION
: the Google Cloud
location
where
you want your new instance to be
Access your instance from the
Google Cloud console
.
What's next
Learn more about
upgrading
user-managed notebooks instances
to ensure that your instance upgrades only when you are ready.
Install dependencies
on
your new user-managed notebooks instance.
Learn more about Deep Learning VM instances in the
Deep Learning VM
documentation
.
Learn about
monitoring the health status
of
your user-managed notebooks instance.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/create-user-managed-notebooks-instance-console-quickstart.txt
Quickstart: Create a user-managed notebooks instance by using the Google Cloud console  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create a user-managed notebooks instance
by using the Google Cloud console
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
Learn how to create
a Vertex AI Workbench user-managed notebooks instance
and open JupyterLab by using the Google Cloud console.
This page also describes how to stop, start, reset, or delete
a user-managed notebooks instance.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
          select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Create an instance
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Click
add_box
Create new
.
For
Name
, enter
my-instance
.
Click
Create
.
When you finish the tasks that are described in this document, you can avoid
   continued billing by deleting the resources that you created. For more information, see
Clean up
.
Open JupyterLab
After you create your instance, Vertex AI Workbench automatically starts
the instance. When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link.
Next to your user-managed notebooks instance's name,
click
Open JupyterLab
.
Your user-managed notebooks instance opens JupyterLab.
Open a new notebook file
Select
File
>
New
>
Notebook
.
In the
Select Kernel
dialog, select
Python 3
,
and then click
Select
.
Your new notebook file opens.
Change the kernel
You can change the kernel of your JupyterLab notebook file from the menu
or in the file.
Menu
In JupyterLab, on the
Kernel
menu, click
Change kernel
.
In the
Select Kernel
dialog, select another kernel to use
and then click
Select
.
In the file
In your JupyterLab notebook file, click the kernel name.
In the
Select Kernel
dialog, select another kernel to use
and then click
Select
.
Stop your instance
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Select the instance that you want to stop.
Click
stop
Stop
.
Start your instance
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Select the instance that you want to start.
Click
play_arrow
Start
.
Reset your instance
Resetting a compute instance forcibly wipes the memory contents of your instance
and resets the instance to its initial state. To learn more about how resetting
an instance works, see
Resetting an instance
.
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Select the instance that you want to reset.
Click
Reset
, and then click
Reset
to confirm.
Clean up
To avoid incurring charges to your Google Cloud account for
          the resources used on this page, follow these steps.
If you created a new project to learn about
Vertex AI Workbench user-managed notebooks
and you no longer need the project, then
delete the project
.
If you used an existing Google Cloud project, then delete the resources
you created to avoid incurring charges to your account:
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Select the row containing the instance that you want to delete.
Click
delete
Delete
.
(Depending on the size of your window,
the
Delete
button might be in
the
more_vert
options menu.)
To confirm, click
Delete
.
What's next
Try one of the tutorials that is included
in your new user-managed notebooks instance.
In the JupyterLab
folder
File Browser
,
open the
tutorials
folder,
and open one of the notebook files.
Read the Introduction to user-managed notebooks
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/custom-container.txt
Create a Vertex AI Workbench user-managed notebooks instance with a custom container  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Create a user-managed notebooks instance with a custom container
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
You can create a user-managed notebooks instance based on a custom
container. Using a custom container lets you customize a
user-managed notebooks environment for your specific needs.
The container must be accessible to your
Google Cloud service account and expose a service on port 8080.
We recommend creating a container derived from a
Deep Learning Containers
image
,
because those images are already configured to be compatible
with user-managed notebooks.
How custom container kernels are updated
Vertex AI Workbench pulls the latest container image for your kernel:
When you create your instance.
When you upgrade your instance.
When you start your instance.
The custom container kernel doesn't persist when your instance is stopped,
so each time your instance is started, Vertex AI Workbench pulls
the latest version of the container image.
If your instance is running when a new version of a container is released,
your instance's kernel isn't updated until you stop and start your instance.
Before you begin
Before you can create a user-managed notebooks instance,
you must have a
Google Cloud project and enable the Notebooks API
for that project.
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
If you plan to use GPUs with your
  user-managed notebooks instance,
check the quotas page in the
  Google Cloud console
to ensure that you have enough GPUs available in your project. If GPUs
  are not listed on the quotas page, or you require additional GPU quota,
  you can request a quota increase. See
Requesting an increase in
  quota
on the
  Compute Engine
Resource quotas
page.
Required roles
If you created the project, you have the
  Owner (
roles/owner
) IAM role on the project,
  which includes all required permissions. Skip this section and
  start creating your user-managed notebooks instance. If you didn't
  create the project yourself, continue in this section.
To get the permissions that
      you need to create a Vertex AI Workbench user-managed notebooks instance,
    
      ask your administrator to grant you the
    following IAM roles on the project:
Notebooks Admin (
roles/notebooks.admin
)
Service Account User (
roles/iam.serviceAccountUser
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Make sure your custom container is ready
Make sure you have a custom container that is accessible to your
Google Cloud service account. For information about how to create a
custom container from a
Deep Learning Containers image
, see
Creating a derivative container
.
Create an instance with a custom container
To create a user-managed notebooks instance
with a custom container, complete the following steps:
In the Google Cloud console, go to the
User-managed notebooks
page.
Or go to
notebook.new
(https://notebook.new) and skip the next step.
Go to User-managed notebooks
Click
add_box
Create new
.
Click
Advanced options
.
On the
Create instance
page,
in the
Details
section,
provide the following information for your new instance:
Name
: a name for your new instance
Region
and
Zone
: Select a region and zone for
the new instance. For best network performance,
select the region that is geographically closest to you.
See the available
user-managed notebooks
locations
.
In the
Environment
section, in the
Environment
field,
select
Custom container
.
In the
Docker container image
field, add a Docker container image
in one of the following ways:
Enter a Docker container image path. For example,
to use a TensorFlow 2.12 container image with accelerators from
Deep Learning Containers
,
enter
us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf-cpu.2-12.py310
.
Click
Select
to add a Docker container image from
Artifact Registry. Then on the
Artifact Registry
tab
where your container image is stored, change the project
to the project that includes your container image, and select
your container image.
Make the rest of your selections, or leave them on their default
setting.
Click
Create
. Vertex AI Workbench creates
a user-managed notebooks instance for you, based
on your custom container.
What's next
Read about how to
push container images to
Artifact Registry
.
If the container images you push to Artifact Registry are derived from
a
Deep Learning Containers
image
,
you can use these container images when creating
user-managed notebooks instances.
Learn more about modifying your custom containers by reading
Best practices for writing
Dockerfiles
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/dependencies.txt
Install dependencies on a Vertex AI Workbench user-managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Install dependencies
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
After you create a user-managed notebooks instance, you might need to
install software that
your notebook depends on. You can install dependencies by adding install
commands to a file in your notebook or by using a terminal
window.
An advantage of adding install commands to a file is that, when you share
a notebook, the commands to install the dependencies are saved with the
notebook and are available to users that you share the notebook with.
Install dependencies from a user-managed notebooks instance
To install Python packages from a user-managed notebooks
instance:
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Select the instance where you want to install dependencies.
Click
Open JupyterLab
.
To add a notebook file, you can use the menu or the Launcher.
Menu
To add a new notebook file from the menu, select
File
>
New
>
Notebook
.
In the
Select kernel
dialog, select the kernel for your new
notebook, for example,
Python 3
, and then click
Select
.
Your new notebook file opens.
Launcher
To add a new Python 3 notebook file from the Launcher, select
File
>
New
>
Launcher
.
Click the
Python 3
tile.
Your new notebook file opens.
Rename your new notebook file.
Menu
Select
File
>
Rename notebook
. The
Rename file
dialog opens.
In the
New name
field, change
Untitled.ipynb
to something
meaningful, such as
install.ipynb
.
Click
Rename
.
Launcher
Right-click the
Untitled.ipynb
tab and then click
Rename notebook
. The
Rename file
dialog opens.
In the
New name
field, change
Untitled.ipynb
to something
meaningful, such as
install.ipynb
.
Click
Rename
.
Install dependencies as follows.
When you open your new notebook, there is a default code cell where you
can enter code, in this case Python 3.
To install the
mxnet
deep learning library in a Python 3 notebook, enter the following
command in the code cell:
%pip install mxnet
Click the run button to run the install command.
When installation is complete, select
Kernel
>
Restart kernel
to restart the kernel and ensure the library is available for import.
Select
File
>
Save notebook
to save the notebook.
Install dependencies from a terminal
To connect to a terminal, you can use your JupyterLab notebook or
SSH
. To install Python
packages from a terminal:
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Select the instance where you want to install dependencies.
Click
Open JupyterLab
.
To open a terminal window, you can use the menu or the Launcher.
Menu
To open a terminal window from the menu, select
File
>
New
>
Terminal
.
The terminal window opens.
Launcher
To open a terminal window from the Launcher, select
File
>
New
>
Launcher
.
In
Other
, click the
Terminal
tile.
The terminal window opens.
In the terminal window, enter the command to install the software
dependency for your user-managed notebooks instance.
To install the
mxnet
deep learning library for Python 3 notebooks, enter the following
command:
pip3 install mxnet
When installation is complete, restart the kernel to make sure the
library is available for import. In every open notebook file in the
same user-managed notebooks instance, select
Kernel
>
Restart kernel
.
Select
File
>
Save notebook
to save the notebook.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/iam.txt
Vertex AI Workbench user-managed notebooks access control  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
User-managed notebooks access control
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
This page describes how to use
Identity and Access Management (IAM)
and an access mode to manage access to
Vertex AI Workbench user-managed notebooks resources.
To manage access to
Vertex AI resources, see the
Vertex AI page on
access control
.
Vertex AI Workbench uses IAM to manage
access to user-managed notebooks instances
and an access mode to manage access to
each instance's JupyterLab interface.
Control access to an instance with IAM
You can manage access to a user-managed notebooks instance at the
project level or per instance.
To grant access to resources at the project level, assign one or more
roles
to a principal (user, group, or
service account
).
To grant access to a specific instance, set an IAM policy
on that resource. The policy defines which roles are assigned
to which principals. To learn more, see
Manage access to
a user-managed notebooks instance
.
Access to an instance can include a broad range of abilities. For example,
you might grant a principal the ability to start, stop, and upgrade
an instance. However, even granting a principal full access to
a user-managed notebooks instance doesn't grant
the ability to use the instance's JupyterLab interface. See the following
section.
Control access to an instance's JupyterLab interface with the access mode
You control access to a user-managed notebooks instance's
JupyterLab interface through the instance's access mode.
You set a JupyterLab access mode when you create
a user-managed notebooks instance.
The access mode can't be changed after the notebook is created.
The JupyterLab access mode determines who can use
the instance's JupyterLab interface.
The access mode also determines which credentials are used when
your instance interacts with other Google Cloud services.
To learn more, see
Manage access to a
user-managed notebooks instance's
JupyterLab interface
.
Types of IAM roles
There are different types of IAM roles that can be used in
Vertex AI Workbench:
Predefined roles
let you grant a set of related
permissions to your Vertex AI Workbench resources at the project level.
Basic roles
(Owner,
Editor, and Viewer) provide access control to your Vertex AI Workbench
resources at the project level, and are common to all Google Cloud
services.
Custom roles
enable you to choose a
specific set of permissions, create your own role with those permissions,
and grant the role to users in your organization.
To add, update, or remove these roles in your Vertex AI Workbench project,
see the documentation on
granting, changing, and
revoking access
.
Predefined user-managed notebooks IAM roles
Vertex AI Workbench resources are managed through the Notebooks API.
Therefore, Notebooks roles define permissions and access
to the use of Vertex AI Workbench.
Role
Permissions
Notebooks Admin
(
roles/
notebooks.admin
)
Full access to Notebooks, all resources.
Lowest-level resources where you can grant this role:
Instance
aiplatform.
notebookExecutionJobs.*
aiplatform.
notebookExecutionJobs.
create
aiplatform.
notebookExecutionJobs.
delete
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.operations.list
aiplatform.pipelineJobs.create
aiplatform.schedules.*
aiplatform.schedules.create
aiplatform.schedules.delete
aiplatform.schedules.get
aiplatform.schedules.list
aiplatform.schedules.update
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.*
notebooks.environments.create
notebooks.environments.delete
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.
environments.
setIamPolicy
notebooks.executions.create
notebooks.executions.delete
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
executions.
setIamPolicy
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.delete
notebooks.instances.diagnose
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.instances.reset
notebooks.
instances.
setAccelerator
notebooks.
instances.
setIamPolicy
notebooks.instances.setLabels
notebooks.
instances.
setMachineType
notebooks.instances.start
notebooks.instances.stop
notebooks.instances.update
notebooks.
instances.
updateConfig
notebooks.
instances.
updateShieldInstanceConfig
notebooks.instances.upgrade
notebooks.instances.use
notebooks.locations.get
notebooks.locations.list
notebooks.operations.cancel
notebooks.operations.delete
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.delete
notebooks.runtimes.diagnose
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.runtimes.reset
notebooks.
runtimes.
setIamPolicy
notebooks.runtimes.start
notebooks.runtimes.stop
notebooks.runtimes.switch
notebooks.runtimes.update
notebooks.runtimes.upgrade
notebooks.schedules.create
notebooks.schedules.delete
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
notebooks.
schedules.
setIamPolicy
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Legacy Admin
(
roles/
notebooks.legacyAdmin
)
Full access to Notebooks all resources through compute API.
backupdr.
backupPlanAssociations.
createForComputeDisk
backupdr.
backupPlanAssociations.
createForComputeInstance
backupdr.
backupPlanAssociations.
deleteForComputeDisk
backupdr.
backupPlanAssociations.
deleteForComputeInstance
backupdr.
backupPlanAssociations.
fetchForComputeDisk
backupdr.
backupPlanAssociations.
getForComputeDisk
backupdr.
backupPlanAssociations.
list
backupdr.
backupPlanAssociations.
triggerBackupForComputeDisk
backupdr.
backupPlanAssociations.
triggerBackupForComputeInstance
backupdr.
backupPlanAssociations.
updateForComputeDisk
backupdr.
backupPlanAssociations.
updateForComputeInstance
backupdr.backupPlans.get
backupdr.backupPlans.list
backupdr.
backupPlans.
useForComputeDisk
backupdr.
backupPlans.
useForComputeInstance
backupdr.backupVaults.get
backupdr.backupVaults.list
backupdr.locations.list
backupdr.operations.get
backupdr.operations.list
backupdr.
serviceConfig.
initialize
cloudkms.keyHandles.*
cloudkms.keyHandles.create
cloudkms.keyHandles.get
cloudkms.keyHandles.list
cloudkms.operations.get
cloudkms.
projects.
showEffectiveAutokeyConfig
compute.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.create
compute.
addresses.
createInternal
compute.
addresses.
createTagBinding
compute.addresses.delete
compute.
addresses.
deleteInternal
compute.
addresses.
deleteTagBinding
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.addresses.setLabels
compute.addresses.use
compute.addresses.useInternal
compute.advice.calendarMode
compute.autoscalers.create
compute.autoscalers.delete
compute.autoscalers.get
compute.autoscalers.list
compute.autoscalers.update
compute.
backendBuckets.
addSignedUrlKey
compute.backendBuckets.create
compute.
backendBuckets.
createTagBinding
compute.backendBuckets.delete
compute.
backendBuckets.
deleteSignedUrlKey
compute.
backendBuckets.
deleteTagBinding
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.
backendBuckets.
setIamPolicy
compute.
backendBuckets.
setSecurityPolicy
compute.backendBuckets.update
compute.backendBuckets.use
compute.
backendServices.
addSignedUrlKey
compute.backendServices.create
compute.
backendServices.
createTagBinding
compute.backendServices.delete
compute.
backendServices.
deleteSignedUrlKey
compute.
backendServices.
deleteTagBinding
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.
backendServices.
setIamPolicy
compute.
backendServices.
setSecurityPolicy
compute.backendServices.update
compute.backendServices.use
compute.commitments.create
compute.commitments.get
compute.commitments.list
compute.commitments.update
compute.
commitments.
updateReservations
compute.
crossSiteNetworks.
create
compute.
crossSiteNetworks.
delete
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.
crossSiteNetworks.
update
compute.diskSettings.get
compute.diskSettings.update
compute.diskTypes.get
compute.diskTypes.list
compute.
disks.
addResourcePolicies
compute.disks.create
compute.disks.createSnapshot
compute.disks.createTagBinding
compute.disks.delete
compute.disks.deleteTagBinding
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
disks.
removeResourcePolicies
compute.disks.resize
compute.disks.setIamPolicy
compute.disks.setLabels
compute.
disks.
startAsyncReplication
compute.
disks.
stopAsyncReplication
compute.
disks.
stopGroupAsyncReplication
compute.disks.update
compute.disks.updateKmsKey
compute.disks.use
compute.disks.useReadOnly
compute.
externalVpnGateways.
create
compute.
externalVpnGateways.
createTagBinding
compute.
externalVpnGateways.
delete
compute.
externalVpnGateways.
deleteTagBinding
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.
externalVpnGateways.
setLabels
compute.
externalVpnGateways.
use
compute.
firewallPolicies.
cloneRules
compute.
firewallPolicies.
copyRules
compute.
firewallPolicies.
create
compute.
firewallPolicies.
createTagBinding
compute.
firewallPolicies.
delete
compute.
firewallPolicies.
deleteTagBinding
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewallPolicies.move
compute.
firewallPolicies.
setIamPolicy
compute.
firewallPolicies.
update
compute.firewallPolicies.use
compute.firewalls.create
compute.
firewalls.
createTagBinding
compute.firewalls.delete
compute.
firewalls.
deleteTagBinding
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.firewalls.update
compute.forwardingRules.create
compute.
forwardingRules.
createTagBinding
compute.forwardingRules.delete
compute.
forwardingRules.
deleteTagBinding
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.
forwardingRules.
pscCreate
compute.
forwardingRules.
pscDelete
compute.
forwardingRules.
pscSetLabels
compute.
forwardingRules.
pscUpdate
compute.
forwardingRules.
setLabels
compute.
forwardingRules.
setTarget
compute.forwardingRules.update
compute.forwardingRules.use
compute.
futureReservations.
cancel
compute.
futureReservations.
create
compute.
futureReservations.
delete
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.
futureReservations.
setIamPolicy
compute.
futureReservations.
update
compute.globalAddresses.create
compute.
globalAddresses.
createInternal
compute.
globalAddresses.
createTagBinding
compute.globalAddresses.delete
compute.
globalAddresses.
deleteInternal
compute.
globalAddresses.
deleteTagBinding
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalAddresses.
setLabels
compute.globalAddresses.use
compute.
globalForwardingRules.
create
compute.
globalForwardingRules.
createTagBinding
compute.
globalForwardingRules.
delete
compute.
globalForwardingRules.
deleteTagBinding
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalForwardingRules.
pscCreate
compute.
globalForwardingRules.
pscDelete
compute.
globalForwardingRules.
pscSetLabels
compute.
globalForwardingRules.
pscUpdate
compute.
globalForwardingRules.
setLabels
compute.
globalForwardingRules.
setTarget
compute.
globalForwardingRules.
update
compute.
globalNetworkEndpointGroups.
attachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
create
compute.
globalNetworkEndpointGroups.
createTagBinding
compute.
globalNetworkEndpointGroups.
delete
compute.
globalNetworkEndpointGroups.
deleteTagBinding
compute.
globalNetworkEndpointGroups.
detachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.
globalNetworkEndpointGroups.
use
compute.
globalOperations.
delete
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalOperations.
setIamPolicy
compute.
globalPublicDelegatedPrefixes.
create
compute.
globalPublicDelegatedPrefixes.
delete
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.
globalPublicDelegatedPrefixes.
updatePolicy
compute.healthChecks.create
compute.
healthChecks.
createTagBinding
compute.healthChecks.delete
compute.
healthChecks.
deleteTagBinding
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.healthChecks.update
compute.healthChecks.use
compute.
healthChecks.
useReadOnly
compute.
httpHealthChecks.
create
compute.
httpHealthChecks.
createTagBinding
compute.
httpHealthChecks.
delete
compute.
httpHealthChecks.
deleteTagBinding
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.
httpHealthChecks.
update
compute.httpHealthChecks.use
compute.
httpHealthChecks.
useReadOnly
compute.
httpsHealthChecks.
create
compute.
httpsHealthChecks.
createTagBinding
compute.
httpsHealthChecks.
delete
compute.
httpsHealthChecks.
deleteTagBinding
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.
httpsHealthChecks.
update
compute.httpsHealthChecks.use
compute.
httpsHealthChecks.
useReadOnly
compute.images.create
compute.
images.
createTagBinding
compute.images.delete
compute.
images.
deleteTagBinding
compute.images.deprecate
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.images.setIamPolicy
compute.images.setLabels
compute.images.update
compute.images.useReadOnly
compute.
instanceGroupManagers.
create
compute.
instanceGroupManagers.
createTagBinding
compute.
instanceGroupManagers.
delete
compute.
instanceGroupManagers.
deleteTagBinding
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.
instanceGroupManagers.
update
compute.
instanceGroupManagers.
use
compute.instanceGroups.create
compute.
instanceGroups.
createTagBinding
compute.instanceGroups.delete
compute.
instanceGroups.
deleteTagBinding
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceGroups.update
compute.instanceGroups.use
compute.instanceSettings.get
compute.
instanceSettings.
update
compute.
instanceTemplates.
create
compute.
instanceTemplates.
delete
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.
instanceTemplates.
setIamPolicy
compute.
instanceTemplates.
useReadOnly
compute.
instances.
addAccessConfig
compute.
instances.
addNetworkInterface
compute.
instances.
addResourcePolicies
compute.instances.attachDisk
compute.instances.create
compute.
instances.
createTagBinding
compute.instances.delete
compute.
instances.
deleteAccessConfig
compute.
instances.
deleteNetworkInterface
compute.
instances.
deleteTagBinding
compute.instances.detachDisk
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instances.osAdminLogin
compute.instances.osLogin
compute.
instances.
pscInterfaceCreate
compute.
instances.
removeResourcePolicies
compute.instances.reset
compute.instances.resume
compute.
instances.
sendDiagnosticInterrupt
compute.
instances.
setDeletionProtection
compute.
instances.
setDiskAutoDelete
compute.instances.setIamPolicy
compute.instances.setLabels
compute.
instances.
setMachineResources
compute.
instances.
setMachineType
compute.instances.setMetadata
compute.
instances.
setMinCpuPlatform
compute.instances.setName
compute.
instances.
setScheduling
compute.
instances.
setSecurityPolicy
compute.
instances.
setServiceAccount
compute.
instances.
setShieldedInstanceIntegrityPolicy
compute.
instances.
setShieldedVmIntegrityPolicy
compute.instances.setTags
compute.
instances.
simulateMaintenanceEvent
compute.instances.start
compute.
instances.
startWithEncryptionKey
compute.instances.stop
compute.instances.suspend
compute.instances.update
compute.
instances.
updateAccessConfig
compute.
instances.
updateDisplayDevice
compute.
instances.
updateNetworkInterface
compute.
instances.
updateSecurity
compute.
instances.
updateShieldedInstanceConfig
compute.
instances.
updateShieldedVmConfig
compute.instances.use
compute.instances.useReadOnly
compute.
instantSnapshots.
create
compute.
instantSnapshots.
delete
compute.
instantSnapshots.
export
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
instantSnapshots.
setIamPolicy
compute.
instantSnapshots.
setLabels
compute.
instantSnapshots.
useReadOnly
compute.
interconnectAttachmentGroups.
create
compute.
interconnectAttachmentGroups.
delete
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachmentGroups.
patch
compute.
interconnectAttachments.
create
compute.
interconnectAttachments.
createTagBinding
compute.
interconnectAttachments.
delete
compute.
interconnectAttachments.
deleteTagBinding
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.
interconnectAttachments.
setLabels
compute.
interconnectAttachments.
update
compute.
interconnectAttachments.
use
compute.
interconnectGroups.
create
compute.
interconnectGroups.
delete
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectGroups.
patch
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.create
compute.
interconnects.
createTagBinding
compute.interconnects.delete
compute.
interconnects.
deleteTagBinding
compute.interconnects.get
compute.
interconnects.
getMacsecConfig
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.
interconnects.
setLabels
compute.interconnects.update
compute.interconnects.use
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.
licenseCodes.
setIamPolicy
compute.licenses.create
compute.licenses.delete
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.licenses.setIamPolicy
compute.licenses.update
compute.machineImages.create
compute.machineImages.delete
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.
machineImages.
setIamPolicy
compute.
machineImages.
setLabels
compute.
machineImages.
useReadOnly
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.create
compute.multiMig.delete
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.
networkAttachments.
create
compute.
networkAttachments.
createTagBinding
compute.
networkAttachments.
delete
compute.
networkAttachments.
deleteTagBinding
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkAttachments.
setIamPolicy
compute.
networkAttachments.
update
compute.networkAttachments.use
compute.
networkEdgeSecurityServices.
create
compute.
networkEdgeSecurityServices.
createTagBinding
compute.
networkEdgeSecurityServices.
delete
compute.
networkEdgeSecurityServices.
deleteTagBinding
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEdgeSecurityServices.
update
compute.
networkEndpointGroups.
attachNetworkEndpoints
compute.
networkEndpointGroups.
create
compute.
networkEndpointGroups.
createTagBinding
compute.
networkEndpointGroups.
delete
compute.
networkEndpointGroups.
deleteTagBinding
compute.
networkEndpointGroups.
detachNetworkEndpoints
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.
networkEndpointGroups.
use
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.access
compute.networks.addPeering
compute.networks.create
compute.
networks.
createTagBinding
compute.networks.delete
compute.
networks.
deleteTagBinding
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.networks.mirror
compute.networks.removePeering
compute.
networks.
setFirewallPolicy
compute.
networks.
switchToCustomMode
compute.networks.update
compute.networks.updatePeering
compute.networks.updatePolicy
compute.networks.use
compute.networks.useExternalIp
compute.nodeGroups.addNodes
compute.nodeGroups.create
compute.nodeGroups.delete
compute.nodeGroups.deleteNodes
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.
nodeGroups.
performMaintenance
compute.
nodeGroups.
setIamPolicy
compute.
nodeGroups.
setNodeTemplate
compute.
nodeGroups.
simulateMaintenanceEvent
compute.nodeGroups.update
compute.nodeTemplates.create
compute.nodeTemplates.delete
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.
nodeTemplates.
setIamPolicy
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
disableXpnHost
compute.
organizations.
disableXpnResource
compute.
organizations.
enableXpnHost
compute.
organizations.
enableXpnResource
compute.
organizations.
listAssociations
compute.
organizations.
setFirewallPolicy
compute.
organizations.
setSecurityPolicy
compute.
oslogin.
updateExternalUser
compute.
packetMirrorings.
create
compute.
packetMirrorings.
createTagBinding
compute.
packetMirrorings.
delete
compute.
packetMirrorings.
deleteTagBinding
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.
packetMirrorings.
update
compute.previewFeatures.get
compute.previewFeatures.list
compute.previewFeatures.update
compute.projects.get
compute.
projects.
setCloudArmorTier
compute.
projects.
setCommonInstanceMetadata
compute.
projects.
setDefaultNetworkTier
compute.
projects.
setDefaultServiceAccount
compute.
projects.
setManagedProtectionTier
compute.
projects.
setUsageExportBucket
compute.
publicAdvertisedPrefixes.
create
compute.
publicAdvertisedPrefixes.
delete
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicAdvertisedPrefixes.
update
compute.
publicAdvertisedPrefixes.
updatePolicy
compute.
publicDelegatedPrefixes.
announce
compute.
publicDelegatedPrefixes.
create
compute.
publicDelegatedPrefixes.
createTagBinding
compute.
publicDelegatedPrefixes.
delete
compute.
publicDelegatedPrefixes.
deleteTagBinding
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
publicDelegatedPrefixes.
update
compute.
publicDelegatedPrefixes.
updatePolicy
compute.
publicDelegatedPrefixes.
use
compute.
publicDelegatedPrefixes.
withdraw
compute.
regionBackendBuckets.
create
compute.
regionBackendBuckets.
createTagBinding
compute.
regionBackendBuckets.
delete
compute.
regionBackendBuckets.
deleteTagBinding
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendBuckets.
setIamPolicy
compute.
regionBackendBuckets.
update
compute.
regionBackendBuckets.
use
compute.
regionBackendServices.
create
compute.
regionBackendServices.
createTagBinding
compute.
regionBackendServices.
delete
compute.
regionBackendServices.
deleteTagBinding
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionBackendServices.
setIamPolicy
compute.
regionBackendServices.
setSecurityPolicy
compute.
regionBackendServices.
update
compute.
regionBackendServices.
use
compute.
regionCompositeHealthChecks.
create
compute.
regionCompositeHealthChecks.
delete
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionCompositeHealthChecks.
update
compute.
regionFirewallPolicies.
cloneRules
compute.
regionFirewallPolicies.
create
compute.
regionFirewallPolicies.
createTagBinding
compute.
regionFirewallPolicies.
delete
compute.
regionFirewallPolicies.
deleteTagBinding
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionFirewallPolicies.
setIamPolicy
compute.
regionFirewallPolicies.
update
compute.
regionFirewallPolicies.
use
compute.
regionHealthAggregationPolicies.
create
compute.
regionHealthAggregationPolicies.
delete
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthAggregationPolicies.
update
compute.
regionHealthCheckServices.
create
compute.
regionHealthCheckServices.
delete
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.
regionHealthCheckServices.
update
compute.
regionHealthCheckServices.
use
compute.
regionHealthChecks.
create
compute.
regionHealthChecks.
createTagBinding
compute.
regionHealthChecks.
delete
compute.
regionHealthChecks.
deleteTagBinding
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthChecks.
update
compute.regionHealthChecks.use
compute.
regionHealthChecks.
useReadOnly
compute.
regionHealthSources.
create
compute.
regionHealthSources.
delete
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionHealthSources.
update
compute.
regionNetworkEndpointGroups.
attachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
create
compute.
regionNetworkEndpointGroups.
createTagBinding
compute.
regionNetworkEndpointGroups.
delete
compute.
regionNetworkEndpointGroups.
deleteTagBinding
compute.
regionNetworkEndpointGroups.
detachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNetworkEndpointGroups.
use
compute.
regionNotificationEndpoints.
create
compute.
regionNotificationEndpoints.
delete
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.
regionNotificationEndpoints.
update
compute.
regionNotificationEndpoints.
use
compute.
regionOperations.
delete
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionOperations.
setIamPolicy
compute.
regionSecurityPolicies.
create
compute.
regionSecurityPolicies.
createTagBinding
compute.
regionSecurityPolicies.
delete
compute.
regionSecurityPolicies.
deleteTagBinding
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSecurityPolicies.
update
compute.
regionSecurityPolicies.
use
compute.
regionSslCertificates.
create
compute.
regionSslCertificates.
createTagBinding
compute.
regionSslCertificates.
delete
compute.
regionSslCertificates.
deleteTagBinding
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.
regionSslPolicies.
create
compute.
regionSslPolicies.
createTagBinding
compute.
regionSslPolicies.
delete
compute.
regionSslPolicies.
deleteTagBinding
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionSslPolicies.
update
compute.regionSslPolicies.use
compute.
regionTargetHttpProxies.
create
compute.
regionTargetHttpProxies.
createTagBinding
compute.
regionTargetHttpProxies.
delete
compute.
regionTargetHttpProxies.
deleteTagBinding
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpProxies.
setUrlMap
compute.
regionTargetHttpProxies.
use
compute.
regionTargetHttpsProxies.
create
compute.
regionTargetHttpsProxies.
createTagBinding
compute.
regionTargetHttpsProxies.
delete
compute.
regionTargetHttpsProxies.
deleteTagBinding
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
setSslCertificates
compute.
regionTargetHttpsProxies.
setUrlMap
compute.
regionTargetHttpsProxies.
update
compute.
regionTargetHttpsProxies.
use
compute.
regionTargetTcpProxies.
create
compute.
regionTargetTcpProxies.
createTagBinding
compute.
regionTargetTcpProxies.
delete
compute.
regionTargetTcpProxies.
deleteTagBinding
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.
regionTargetTcpProxies.
use
compute.regionUrlMaps.create
compute.
regionUrlMaps.
createTagBinding
compute.regionUrlMaps.delete
compute.
regionUrlMaps.
deleteTagBinding
compute.regionUrlMaps.get
compute.
regionUrlMaps.
invalidateCache
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.update
compute.regionUrlMaps.use
compute.regionUrlMaps.validate
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationBlocks.
performMaintenance
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.
reservationSubBlocks.
performMaintenance
compute.
reservationSubBlocks.
reportFaulty
compute.reservations.create
compute.reservations.delete
compute.reservations.get
compute.reservations.list
compute.
reservations.
performMaintenance
compute.reservations.resize
compute.reservations.update
compute.
resourcePolicies.
create
compute.
resourcePolicies.
delete
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.
resourcePolicies.
setIamPolicy
compute.
resourcePolicies.
update
compute.resourcePolicies.use
compute.
resourcePolicies.
useReadOnly
compute.rolloutPlans.create
compute.rolloutPlans.delete
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.cancel
compute.rollouts.delete
compute.rollouts.get
compute.rollouts.list
compute.routers.create
compute.
routers.
createTagBinding
compute.routers.delete
compute.
routers.
deleteRoutePolicy
compute.
routers.
deleteTagBinding
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routers.update
compute.
routers.
updateRoutePolicy
compute.routers.use
compute.routes.create
compute.
routes.
createTagBinding
compute.routes.delete
compute.
routes.
deleteTagBinding
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.
securityPolicies.
addAssociation
compute.
securityPolicies.
copyRules
compute.
securityPolicies.
create
compute.
securityPolicies.
createTagBinding
compute.
securityPolicies.
delete
compute.
securityPolicies.
deleteTagBinding
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.securityPolicies.move
compute.
securityPolicies.
removeAssociation
compute.
securityPolicies.
setLabels
compute.
securityPolicies.
update
compute.securityPolicies.use
compute.
serviceAttachments.
create
compute.
serviceAttachments.
createTagBinding
compute.
serviceAttachments.
delete
compute.
serviceAttachments.
deleteTagBinding
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.
serviceAttachments.
setIamPolicy
compute.
serviceAttachments.
update
compute.serviceAttachments.use
compute.snapshotSettings.get
compute.
snapshotSettings.
update
compute.snapshots.create
compute.
snapshots.
createTagBinding
compute.snapshots.delete
compute.
snapshots.
deleteTagBinding
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.snapshots.setIamPolicy
compute.snapshots.setLabels
compute.snapshots.updateKmsKey
compute.snapshots.useReadOnly
compute.spotAssistants.get
compute.sslCertificates.create
compute.
sslCertificates.
createTagBinding
compute.sslCertificates.delete
compute.
sslCertificates.
deleteTagBinding
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.create
compute.
sslPolicies.
createTagBinding
compute.sslPolicies.delete
compute.
sslPolicies.
deleteTagBinding
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.sslPolicies.update
compute.sslPolicies.use
compute.storagePools.create
compute.storagePools.delete
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.
storagePools.
setIamPolicy
compute.storagePools.update
compute.storagePools.use
compute.subnetworks.create
compute.
subnetworks.
createTagBinding
compute.subnetworks.delete
compute.
subnetworks.
deleteTagBinding
compute.
subnetworks.
expandIpCidrRange
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.subnetworks.mirror
compute.
subnetworks.
setIamPolicy
compute.
subnetworks.
setPrivateIpGoogleAccess
compute.subnetworks.update
compute.subnetworks.use
compute.
subnetworks.
useExternalIp
compute.
subnetworks.
usePeerMigration
compute.
targetGrpcProxies.
create
compute.
targetGrpcProxies.
createTagBinding
compute.
targetGrpcProxies.
delete
compute.
targetGrpcProxies.
deleteTagBinding
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.
targetGrpcProxies.
update
compute.targetGrpcProxies.use
compute.
targetHttpProxies.
create
compute.
targetHttpProxies.
createTagBinding
compute.
targetHttpProxies.
delete
compute.
targetHttpProxies.
deleteTagBinding
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.
targetHttpProxies.
setUrlMap
compute.
targetHttpProxies.
update
compute.targetHttpProxies.use
compute.
targetHttpsProxies.
create
compute.
targetHttpsProxies.
createTagBinding
compute.
targetHttpsProxies.
delete
compute.
targetHttpsProxies.
deleteTagBinding
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.
targetHttpsProxies.
setCertificateMap
compute.
targetHttpsProxies.
setQuicOverride
compute.
targetHttpsProxies.
setSslCertificates
compute.
targetHttpsProxies.
setSslPolicy
compute.
targetHttpsProxies.
setUrlMap
compute.
targetHttpsProxies.
update
compute.targetHttpsProxies.use
compute.targetInstances.create
compute.
targetInstances.
createTagBinding
compute.targetInstances.delete
compute.
targetInstances.
deleteTagBinding
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.
targetInstances.
setSecurityPolicy
compute.targetInstances.use
compute.
targetPools.
addHealthCheck
compute.
targetPools.
addInstance
compute.targetPools.create
compute.
targetPools.
createTagBinding
compute.targetPools.delete
compute.
targetPools.
deleteTagBinding
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.
targetPools.
removeHealthCheck
compute.
targetPools.
removeInstance
compute.
targetPools.
setSecurityPolicy
compute.targetPools.update
compute.targetPools.use
compute.
targetSslProxies.
create
compute.
targetSslProxies.
createTagBinding
compute.
targetSslProxies.
delete
compute.
targetSslProxies.
deleteTagBinding
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.
targetSslProxies.
setBackendService
compute.
targetSslProxies.
setCertificateMap
compute.
targetSslProxies.
setProxyHeader
compute.
targetSslProxies.
setSslCertificates
compute.
targetSslProxies.
setSslPolicy
compute.
targetSslProxies.
update
compute.targetSslProxies.use
compute.
targetTcpProxies.
create
compute.
targetTcpProxies.
createTagBinding
compute.
targetTcpProxies.
delete
compute.
targetTcpProxies.
deleteTagBinding
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.
targetTcpProxies.
update
compute.targetTcpProxies.use
compute.
targetVpnGateways.
create
compute.
targetVpnGateways.
createTagBinding
compute.
targetVpnGateways.
delete
compute.
targetVpnGateways.
deleteTagBinding
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.
targetVpnGateways.
setLabels
compute.targetVpnGateways.use
compute.urlMaps.create
compute.
urlMaps.
createTagBinding
compute.urlMaps.delete
compute.
urlMaps.
deleteTagBinding
compute.urlMaps.get
compute.
urlMaps.
invalidateCache
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.update
compute.urlMaps.use
compute.urlMaps.validate
compute.vpnGateways.create
compute.
vpnGateways.
createTagBinding
compute.vpnGateways.delete
compute.
vpnGateways.
deleteTagBinding
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnGateways.setLabels
compute.vpnGateways.use
compute.vpnTunnels.create
compute.
vpnTunnels.
createTagBinding
compute.vpnTunnels.delete
compute.
vpnTunnels.
deleteTagBinding
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.vpnTunnels.setLabels
compute.wireGroups.create
compute.wireGroups.delete
compute.wireGroups.get
compute.wireGroups.list
compute.wireGroups.update
compute.zoneOperations.delete
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.
zoneOperations.
setIamPolicy
compute.zones.get
compute.zones.list
notebooks.*
notebooks.environments.create
notebooks.environments.delete
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.
environments.
setIamPolicy
notebooks.executions.create
notebooks.executions.delete
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
executions.
setIamPolicy
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.delete
notebooks.instances.diagnose
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.instances.reset
notebooks.
instances.
setAccelerator
notebooks.
instances.
setIamPolicy
notebooks.instances.setLabels
notebooks.
instances.
setMachineType
notebooks.instances.start
notebooks.instances.stop
notebooks.instances.update
notebooks.
instances.
updateConfig
notebooks.
instances.
updateShieldInstanceConfig
notebooks.instances.upgrade
notebooks.instances.use
notebooks.locations.get
notebooks.locations.list
notebooks.operations.cancel
notebooks.operations.delete
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.delete
notebooks.runtimes.diagnose
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.runtimes.reset
notebooks.
runtimes.
setIamPolicy
notebooks.runtimes.start
notebooks.runtimes.stop
notebooks.runtimes.switch
notebooks.runtimes.update
notebooks.runtimes.upgrade
notebooks.schedules.create
notebooks.schedules.delete
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
notebooks.
schedules.
setIamPolicy
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Legacy Viewer
(
roles/
notebooks.legacyViewer
)
Read-only access to Notebooks all resources through compute API.
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
instances.
checkUpgradability
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.locations.*
notebooks.locations.get
notebooks.locations.list
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Runner
(
roles/
notebooks.runner
)
Restricted access for running scheduled Notebooks.
aiplatform.
notebookExecutionJobs.*
aiplatform.
notebookExecutionJobs.
create
aiplatform.
notebookExecutionJobs.
delete
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.operations.list
aiplatform.pipelineJobs.create
aiplatform.schedules.*
aiplatform.schedules.create
aiplatform.schedules.delete
aiplatform.schedules.get
aiplatform.schedules.list
aiplatform.schedules.update
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.executions.create
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.locations.*
notebooks.locations.get
notebooks.locations.list
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.schedules.create
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
AI Platform Notebooks Service Agent
(
roles/
notebooks.serviceAgent
)
Provide access for notebooks service agent to manage notebook instances in user projects
Warning:
Do not grant service agent roles to any principals except
service agents
.
aiplatform.customJobs.cancel
aiplatform.customJobs.create
aiplatform.customJobs.get
aiplatform.customJobs.list
aiplatform.
notebookExecutionJobs.*
aiplatform.
notebookExecutionJobs.
create
aiplatform.
notebookExecutionJobs.
delete
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.operations.list
aiplatform.pipelineJobs.create
aiplatform.schedules.*
aiplatform.schedules.create
aiplatform.schedules.delete
aiplatform.schedules.get
aiplatform.schedules.list
aiplatform.schedules.update
backupdr.
backupPlanAssociations.
createForComputeDisk
backupdr.
backupPlanAssociations.
createForComputeInstance
backupdr.
backupPlanAssociations.
deleteForComputeDisk
backupdr.
backupPlanAssociations.
deleteForComputeInstance
backupdr.
backupPlanAssociations.
fetchForComputeDisk
backupdr.
backupPlanAssociations.
getForComputeDisk
backupdr.
backupPlanAssociations.
list
backupdr.
backupPlanAssociations.
triggerBackupForComputeDisk
backupdr.
backupPlanAssociations.
triggerBackupForComputeInstance
backupdr.
backupPlanAssociations.
updateForComputeDisk
backupdr.
backupPlanAssociations.
updateForComputeInstance
backupdr.backupPlans.get
backupdr.backupPlans.list
backupdr.
backupPlans.
useForComputeDisk
backupdr.
backupPlans.
useForComputeInstance
backupdr.backupVaults.get
backupdr.backupVaults.list
backupdr.locations.list
backupdr.operations.get
backupdr.operations.list
backupdr.
serviceConfig.
initialize
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.
addresses.
createInternal
compute.
addresses.
deleteInternal
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.addresses.use
compute.addresses.useInternal
compute.autoscalers.*
compute.autoscalers.create
compute.autoscalers.delete
compute.autoscalers.get
compute.autoscalers.list
compute.autoscalers.update
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.*
compute.
disks.
addResourcePolicies
compute.disks.create
compute.disks.createSnapshot
compute.disks.createTagBinding
compute.disks.delete
compute.disks.deleteTagBinding
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
disks.
removeResourcePolicies
compute.disks.resize
compute.disks.setIamPolicy
compute.disks.setLabels
compute.
disks.
startAsyncReplication
compute.
disks.
stopAsyncReplication
compute.
disks.
stopGroupAsyncReplication
compute.disks.update
compute.disks.updateKmsKey
compute.disks.use
compute.disks.useReadOnly
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.globalAddresses.use
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.*
compute.
globalNetworkEndpointGroups.
attachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
create
compute.
globalNetworkEndpointGroups.
createTagBinding
compute.
globalNetworkEndpointGroups.
delete
compute.
globalNetworkEndpointGroups.
deleteTagBinding
compute.
globalNetworkEndpointGroups.
detachNetworkEndpoints
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.
globalNetworkEndpointGroups.
use
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.*
compute.images.create
compute.
images.
createTagBinding
compute.images.delete
compute.
images.
deleteTagBinding
compute.images.deprecate
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.images.setIamPolicy
compute.images.setLabels
compute.images.update
compute.images.useReadOnly
compute.
instanceGroupManagers.*
compute.
instanceGroupManagers.
create
compute.
instanceGroupManagers.
createTagBinding
compute.
instanceGroupManagers.
delete
compute.
instanceGroupManagers.
deleteTagBinding
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.
instanceGroupManagers.
update
compute.
instanceGroupManagers.
use
compute.instanceGroups.*
compute.instanceGroups.create
compute.
instanceGroups.
createTagBinding
compute.instanceGroups.delete
compute.
instanceGroups.
deleteTagBinding
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceGroups.update
compute.instanceGroups.use
compute.instanceSettings.*
compute.instanceSettings.get
compute.
instanceSettings.
update
compute.instanceTemplates.*
compute.
instanceTemplates.
create
compute.
instanceTemplates.
delete
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.
instanceTemplates.
setIamPolicy
compute.
instanceTemplates.
useReadOnly
compute.instances.*
compute.
instances.
addAccessConfig
compute.
instances.
addNetworkInterface
compute.
instances.
addResourcePolicies
compute.instances.attachDisk
compute.instances.create
compute.
instances.
createTagBinding
compute.instances.delete
compute.
instances.
deleteAccessConfig
compute.
instances.
deleteNetworkInterface
compute.
instances.
deleteTagBinding
compute.instances.detachDisk
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instances.osAdminLogin
compute.instances.osLogin
compute.
instances.
pscInterfaceCreate
compute.
instances.
removeResourcePolicies
compute.instances.reset
compute.instances.resume
compute.
instances.
sendDiagnosticInterrupt
compute.
instances.
setDeletionProtection
compute.
instances.
setDiskAutoDelete
compute.instances.setIamPolicy
compute.instances.setLabels
compute.
instances.
setMachineResources
compute.
instances.
setMachineType
compute.instances.setMetadata
compute.
instances.
setMinCpuPlatform
compute.instances.setName
compute.
instances.
setScheduling
compute.
instances.
setSecurityPolicy
compute.
instances.
setServiceAccount
compute.
instances.
setShieldedInstanceIntegrityPolicy
compute.
instances.
setShieldedVmIntegrityPolicy
compute.instances.setTags
compute.
instances.
simulateMaintenanceEvent
compute.instances.start
compute.
instances.
startWithEncryptionKey
compute.instances.stop
compute.instances.suspend
compute.instances.update
compute.
instances.
updateAccessConfig
compute.
instances.
updateDisplayDevice
compute.
instances.
updateNetworkInterface
compute.
instances.
updateSecurity
compute.
instances.
updateShieldedInstanceConfig
compute.
instances.
updateShieldedVmConfig
compute.instances.use
compute.instances.useReadOnly
compute.instantSnapshots.*
compute.
instantSnapshots.
create
compute.
instantSnapshots.
delete
compute.
instantSnapshots.
export
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
instantSnapshots.
setIamPolicy
compute.
instantSnapshots.
setLabels
compute.
instantSnapshots.
useReadOnly
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.*
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.
licenseCodes.
setIamPolicy
compute.licenses.*
compute.licenses.create
compute.licenses.delete
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.licenses.setIamPolicy
compute.licenses.update
compute.machineImages.*
compute.machineImages.create
compute.machineImages.delete
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.
machineImages.
setIamPolicy
compute.
machineImages.
setLabels
compute.
machineImages.
useReadOnly
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.*
compute.multiMig.create
compute.multiMig.delete
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.*
compute.
networkEndpointGroups.
attachNetworkEndpoints
compute.
networkEndpointGroups.
create
compute.
networkEndpointGroups.
createTagBinding
compute.
networkEndpointGroups.
delete
compute.
networkEndpointGroups.
deleteTagBinding
compute.
networkEndpointGroups.
detachNetworkEndpoints
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.
networkEndpointGroups.
use
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.networks.use
compute.networks.useExternalIp
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
projects.
setCommonInstanceMetadata
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.*
compute.
regionNetworkEndpointGroups.
attachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
create
compute.
regionNetworkEndpointGroups.
createTagBinding
compute.
regionNetworkEndpointGroups.
delete
compute.
regionNetworkEndpointGroups.
deleteTagBinding
compute.
regionNetworkEndpointGroups.
detachNetworkEndpoints
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNetworkEndpointGroups.
use
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.*
compute.
resourcePolicies.
create
compute.
resourcePolicies.
delete
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.
resourcePolicies.
setIamPolicy
compute.
resourcePolicies.
update
compute.resourcePolicies.use
compute.
resourcePolicies.
useReadOnly
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.*
compute.snapshots.create
compute.
snapshots.
createTagBinding
compute.snapshots.delete
compute.
snapshots.
deleteTagBinding
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.snapshots.setIamPolicy
compute.snapshots.setLabels
compute.snapshots.updateKmsKey
compute.snapshots.useReadOnly
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.storagePools.use
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.subnetworks.use
compute.
subnetworks.
useExternalIp
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
dataproc.clusters.get
dataproc.clusters.use
dataproc.jobs.cancel
dataproc.jobs.create
dataproc.jobs.delete
dataproc.jobs.get
dataproc.jobs.list
dataproc.jobs.update
iam.serviceAccounts.actAs
iam.serviceAccounts.get
iam.
serviceAccounts.
getAccessToken
iam.serviceAccounts.list
ml.jobs.create
ml.jobs.get
ml.jobs.list
notebooks.*
notebooks.environments.create
notebooks.environments.delete
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.
environments.
setIamPolicy
notebooks.executions.create
notebooks.executions.delete
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
executions.
setIamPolicy
notebooks.
instances.
checkUpgradability
notebooks.instances.create
notebooks.instances.delete
notebooks.instances.diagnose
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.instances.reset
notebooks.
instances.
setAccelerator
notebooks.
instances.
setIamPolicy
notebooks.instances.setLabels
notebooks.
instances.
setMachineType
notebooks.instances.start
notebooks.instances.stop
notebooks.instances.update
notebooks.
instances.
updateConfig
notebooks.
instances.
updateShieldInstanceConfig
notebooks.instances.upgrade
notebooks.instances.use
notebooks.locations.get
notebooks.locations.list
notebooks.operations.cancel
notebooks.operations.delete
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.create
notebooks.runtimes.delete
notebooks.runtimes.diagnose
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.runtimes.reset
notebooks.
runtimes.
setIamPolicy
notebooks.runtimes.start
notebooks.runtimes.stop
notebooks.runtimes.switch
notebooks.runtimes.update
notebooks.runtimes.upgrade
notebooks.schedules.create
notebooks.schedules.delete
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
notebooks.
schedules.
setIamPolicy
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Notebooks Viewer
(
roles/
notebooks.viewer
)
Read-only access to Notebooks, all resources.
Lowest-level resources where you can grant this role:
Instance
aiplatform.
notebookExecutionJobs.
get
aiplatform.
notebookExecutionJobs.
list
aiplatform.schedules.get
aiplatform.schedules.list
compute.acceleratorTypes.*
compute.acceleratorTypes.get
compute.acceleratorTypes.list
compute.addresses.get
compute.addresses.list
compute.
addresses.
listEffectiveTags
compute.
addresses.
listTagBindings
compute.autoscalers.get
compute.autoscalers.list
compute.backendBuckets.get
compute.
backendBuckets.
getIamPolicy
compute.backendBuckets.list
compute.
backendBuckets.
listEffectiveTags
compute.
backendBuckets.
listTagBindings
compute.backendServices.get
compute.
backendServices.
getIamPolicy
compute.backendServices.list
compute.
backendServices.
listEffectiveTags
compute.
backendServices.
listTagBindings
compute.commitments.get
compute.commitments.list
compute.crossSiteNetworks.get
compute.crossSiteNetworks.list
compute.diskSettings.get
compute.diskTypes.*
compute.diskTypes.get
compute.diskTypes.list
compute.disks.get
compute.disks.getIamPolicy
compute.disks.list
compute.
disks.
listEffectiveTags
compute.disks.listTagBindings
compute.
externalVpnGateways.
get
compute.
externalVpnGateways.
list
compute.
externalVpnGateways.
listEffectiveTags
compute.
externalVpnGateways.
listTagBindings
compute.firewallPolicies.get
compute.
firewallPolicies.
getIamPolicy
compute.firewallPolicies.list
compute.
firewallPolicies.
listEffectiveTags
compute.
firewallPolicies.
listTagBindings
compute.firewalls.get
compute.firewalls.list
compute.
firewalls.
listEffectiveTags
compute.
firewalls.
listTagBindings
compute.forwardingRules.get
compute.forwardingRules.list
compute.
forwardingRules.
listEffectiveTags
compute.
forwardingRules.
listTagBindings
compute.futureReservations.get
compute.
futureReservations.
getIamPolicy
compute.
futureReservations.
list
compute.globalAddresses.get
compute.globalAddresses.list
compute.
globalAddresses.
listEffectiveTags
compute.
globalAddresses.
listTagBindings
compute.
globalForwardingRules.
get
compute.
globalForwardingRules.
list
compute.
globalForwardingRules.
listEffectiveTags
compute.
globalForwardingRules.
listTagBindings
compute.
globalNetworkEndpointGroups.
get
compute.
globalNetworkEndpointGroups.
list
compute.
globalNetworkEndpointGroups.
listEffectiveTags
compute.
globalNetworkEndpointGroups.
listTagBindings
compute.globalOperations.get
compute.
globalOperations.
getIamPolicy
compute.globalOperations.list
compute.
globalPublicDelegatedPrefixes.
get
compute.
globalPublicDelegatedPrefixes.
list
compute.healthChecks.get
compute.healthChecks.list
compute.
healthChecks.
listEffectiveTags
compute.
healthChecks.
listTagBindings
compute.httpHealthChecks.get
compute.httpHealthChecks.list
compute.
httpHealthChecks.
listEffectiveTags
compute.
httpHealthChecks.
listTagBindings
compute.httpsHealthChecks.get
compute.httpsHealthChecks.list
compute.
httpsHealthChecks.
listEffectiveTags
compute.
httpsHealthChecks.
listTagBindings
compute.images.get
compute.images.getFromFamily
compute.images.getIamPolicy
compute.images.list
compute.
images.
listEffectiveTags
compute.images.listTagBindings
compute.
instanceGroupManagers.
get
compute.
instanceGroupManagers.
list
compute.
instanceGroupManagers.
listEffectiveTags
compute.
instanceGroupManagers.
listTagBindings
compute.instanceGroups.get
compute.instanceGroups.list
compute.
instanceGroups.
listEffectiveTags
compute.
instanceGroups.
listTagBindings
compute.instanceSettings.get
compute.instanceTemplates.get
compute.
instanceTemplates.
getIamPolicy
compute.instanceTemplates.list
compute.instances.get
compute.
instances.
getEffectiveFirewalls
compute.
instances.
getGuestAttributes
compute.instances.getIamPolicy
compute.
instances.
getScreenshot
compute.
instances.
getSerialPortOutput
compute.
instances.
getShieldedInstanceIdentity
compute.
instances.
getShieldedVmIdentity
compute.instances.list
compute.
instances.
listEffectiveTags
compute.
instances.
listReferrers
compute.
instances.
listTagBindings
compute.instantSnapshots.get
compute.
instantSnapshots.
getIamPolicy
compute.instantSnapshots.list
compute.
interconnectAttachmentGroups.
get
compute.
interconnectAttachmentGroups.
list
compute.
interconnectAttachments.
get
compute.
interconnectAttachments.
list
compute.
interconnectAttachments.
listEffectiveTags
compute.
interconnectAttachments.
listTagBindings
compute.interconnectGroups.get
compute.
interconnectGroups.
list
compute.
interconnectLocations.*
compute.
interconnectLocations.
get
compute.
interconnectLocations.
list
compute.
interconnectRemoteLocations.*
compute.
interconnectRemoteLocations.
get
compute.
interconnectRemoteLocations.
list
compute.interconnects.get
compute.interconnects.list
compute.
interconnects.
listEffectiveTags
compute.
interconnects.
listTagBindings
compute.licenseCodes.get
compute.
licenseCodes.
getIamPolicy
compute.licenseCodes.list
compute.licenses.get
compute.licenses.getIamPolicy
compute.licenses.list
compute.machineImages.get
compute.
machineImages.
getIamPolicy
compute.machineImages.list
compute.machineTypes.*
compute.machineTypes.get
compute.machineTypes.list
compute.multiMig.get
compute.multiMig.list
compute.multiMigMembers.*
compute.multiMigMembers.get
compute.multiMigMembers.list
compute.networkAttachments.get
compute.
networkAttachments.
getIamPolicy
compute.
networkAttachments.
list
compute.
networkAttachments.
listEffectiveTags
compute.
networkAttachments.
listTagBindings
compute.
networkEdgeSecurityServices.
get
compute.
networkEdgeSecurityServices.
list
compute.
networkEdgeSecurityServices.
listEffectiveTags
compute.
networkEdgeSecurityServices.
listTagBindings
compute.
networkEndpointGroups.
get
compute.
networkEndpointGroups.
list
compute.
networkEndpointGroups.
listEffectiveTags
compute.
networkEndpointGroups.
listTagBindings
compute.networkProfiles.*
compute.networkProfiles.get
compute.networkProfiles.list
compute.networks.get
compute.
networks.
getEffectiveFirewalls
compute.
networks.
getRegionEffectiveFirewalls
compute.networks.list
compute.
networks.
listEffectiveTags
compute.
networks.
listPeeringRoutes
compute.
networks.
listTagBindings
compute.nodeGroups.get
compute.
nodeGroups.
getIamPolicy
compute.nodeGroups.list
compute.nodeTemplates.get
compute.
nodeTemplates.
getIamPolicy
compute.nodeTemplates.list
compute.nodeTypes.*
compute.nodeTypes.get
compute.nodeTypes.list
compute.
organizations.
listAssociations
compute.packetMirrorings.get
compute.packetMirrorings.list
compute.
packetMirrorings.
listEffectiveTags
compute.
packetMirrorings.
listTagBindings
compute.previewFeatures.get
compute.previewFeatures.list
compute.projects.get
compute.
publicAdvertisedPrefixes.
get
compute.
publicAdvertisedPrefixes.
list
compute.
publicDelegatedPrefixes.
get
compute.
publicDelegatedPrefixes.
list
compute.
publicDelegatedPrefixes.
listEffectiveTags
compute.
publicDelegatedPrefixes.
listTagBindings
compute.
regionBackendBuckets.
get
compute.
regionBackendBuckets.
getIamPolicy
compute.
regionBackendBuckets.
list
compute.
regionBackendBuckets.
listEffectiveTags
compute.
regionBackendBuckets.
listTagBindings
compute.
regionBackendServices.
get
compute.
regionBackendServices.
getIamPolicy
compute.
regionBackendServices.
list
compute.
regionBackendServices.
listEffectiveTags
compute.
regionBackendServices.
listTagBindings
compute.
regionCompositeHealthChecks.
get
compute.
regionCompositeHealthChecks.
list
compute.
regionFirewallPolicies.
get
compute.
regionFirewallPolicies.
getIamPolicy
compute.
regionFirewallPolicies.
list
compute.
regionFirewallPolicies.
listEffectiveTags
compute.
regionFirewallPolicies.
listTagBindings
compute.
regionHealthAggregationPolicies.
get
compute.
regionHealthAggregationPolicies.
list
compute.
regionHealthCheckServices.
get
compute.
regionHealthCheckServices.
list
compute.regionHealthChecks.get
compute.
regionHealthChecks.
list
compute.
regionHealthChecks.
listEffectiveTags
compute.
regionHealthChecks.
listTagBindings
compute.
regionHealthSources.
get
compute.
regionHealthSources.
list
compute.
regionNetworkEndpointGroups.
get
compute.
regionNetworkEndpointGroups.
list
compute.
regionNetworkEndpointGroups.
listEffectiveTags
compute.
regionNetworkEndpointGroups.
listTagBindings
compute.
regionNotificationEndpoints.
get
compute.
regionNotificationEndpoints.
list
compute.regionOperations.get
compute.
regionOperations.
getIamPolicy
compute.regionOperations.list
compute.
regionSecurityPolicies.
get
compute.
regionSecurityPolicies.
list
compute.
regionSecurityPolicies.
listEffectiveTags
compute.
regionSecurityPolicies.
listTagBindings
compute.
regionSslCertificates.
get
compute.
regionSslCertificates.
list
compute.
regionSslCertificates.
listEffectiveTags
compute.
regionSslCertificates.
listTagBindings
compute.regionSslPolicies.get
compute.regionSslPolicies.list
compute.
regionSslPolicies.
listAvailableFeatures
compute.
regionSslPolicies.
listEffectiveTags
compute.
regionSslPolicies.
listTagBindings
compute.
regionTargetHttpProxies.
get
compute.
regionTargetHttpProxies.
list
compute.
regionTargetHttpProxies.
listEffectiveTags
compute.
regionTargetHttpProxies.
listTagBindings
compute.
regionTargetHttpsProxies.
get
compute.
regionTargetHttpsProxies.
list
compute.
regionTargetHttpsProxies.
listEffectiveTags
compute.
regionTargetHttpsProxies.
listTagBindings
compute.
regionTargetTcpProxies.
get
compute.
regionTargetTcpProxies.
list
compute.
regionTargetTcpProxies.
listEffectiveTags
compute.
regionTargetTcpProxies.
listTagBindings
compute.regionUrlMaps.get
compute.regionUrlMaps.list
compute.
regionUrlMaps.
listEffectiveTags
compute.
regionUrlMaps.
listTagBindings
compute.regionUrlMaps.validate
compute.regions.*
compute.regions.get
compute.regions.list
compute.reservationBlocks.get
compute.reservationBlocks.list
compute.
reservationSubBlocks.
get
compute.
reservationSubBlocks.
list
compute.reservations.get
compute.reservations.list
compute.resourcePolicies.get
compute.
resourcePolicies.
getIamPolicy
compute.resourcePolicies.list
compute.rolloutPlans.get
compute.rolloutPlans.list
compute.rollouts.get
compute.rollouts.list
compute.routers.get
compute.routers.getRoutePolicy
compute.routers.list
compute.routers.listBgpRoutes
compute.
routers.
listEffectiveTags
compute.
routers.
listRoutePolicies
compute.
routers.
listTagBindings
compute.routes.get
compute.routes.list
compute.
routes.
listEffectiveTags
compute.routes.listTagBindings
compute.securityPolicies.get
compute.securityPolicies.list
compute.
securityPolicies.
listEffectiveTags
compute.
securityPolicies.
listTagBindings
compute.serviceAttachments.get
compute.
serviceAttachments.
getIamPolicy
compute.
serviceAttachments.
list
compute.
serviceAttachments.
listEffectiveTags
compute.
serviceAttachments.
listTagBindings
compute.snapshotSettings.get
compute.snapshots.get
compute.snapshots.getIamPolicy
compute.snapshots.list
compute.
snapshots.
listEffectiveTags
compute.
snapshots.
listTagBindings
compute.spotAssistants.get
compute.sslCertificates.get
compute.sslCertificates.list
compute.
sslCertificates.
listEffectiveTags
compute.
sslCertificates.
listTagBindings
compute.sslPolicies.get
compute.sslPolicies.list
compute.
sslPolicies.
listAvailableFeatures
compute.
sslPolicies.
listEffectiveTags
compute.
sslPolicies.
listTagBindings
compute.storagePools.get
compute.
storagePools.
getIamPolicy
compute.storagePools.list
compute.subnetworks.get
compute.
subnetworks.
getIamPolicy
compute.subnetworks.list
compute.
subnetworks.
listEffectiveTags
compute.
subnetworks.
listTagBindings
compute.targetGrpcProxies.get
compute.targetGrpcProxies.list
compute.
targetGrpcProxies.
listEffectiveTags
compute.
targetGrpcProxies.
listTagBindings
compute.targetHttpProxies.get
compute.targetHttpProxies.list
compute.
targetHttpProxies.
listEffectiveTags
compute.
targetHttpProxies.
listTagBindings
compute.targetHttpsProxies.get
compute.
targetHttpsProxies.
list
compute.
targetHttpsProxies.
listEffectiveTags
compute.
targetHttpsProxies.
listTagBindings
compute.targetInstances.get
compute.targetInstances.list
compute.
targetInstances.
listEffectiveTags
compute.
targetInstances.
listTagBindings
compute.targetPools.get
compute.targetPools.list
compute.
targetPools.
listEffectiveTags
compute.
targetPools.
listTagBindings
compute.targetSslProxies.get
compute.targetSslProxies.list
compute.
targetSslProxies.
listEffectiveTags
compute.
targetSslProxies.
listTagBindings
compute.targetTcpProxies.get
compute.targetTcpProxies.list
compute.
targetTcpProxies.
listEffectiveTags
compute.
targetTcpProxies.
listTagBindings
compute.targetVpnGateways.get
compute.targetVpnGateways.list
compute.
targetVpnGateways.
listEffectiveTags
compute.
targetVpnGateways.
listTagBindings
compute.urlMaps.get
compute.urlMaps.list
compute.
urlMaps.
listEffectiveTags
compute.
urlMaps.
listTagBindings
compute.urlMaps.validate
compute.vpnGateways.get
compute.vpnGateways.list
compute.
vpnGateways.
listEffectiveTags
compute.
vpnGateways.
listTagBindings
compute.vpnTunnels.get
compute.vpnTunnels.list
compute.
vpnTunnels.
listEffectiveTags
compute.
vpnTunnels.
listTagBindings
compute.wireGroups.get
compute.wireGroups.list
compute.zoneOperations.get
compute.
zoneOperations.
getIamPolicy
compute.zoneOperations.list
compute.zones.*
compute.zones.get
compute.zones.list
notebooks.environments.get
notebooks.
environments.
getIamPolicy
notebooks.environments.list
notebooks.executions.get
notebooks.
executions.
getIamPolicy
notebooks.executions.list
notebooks.
instances.
checkUpgradability
notebooks.instances.get
notebooks.instances.getHealth
notebooks.
instances.
getIamPolicy
notebooks.instances.list
notebooks.locations.*
notebooks.locations.get
notebooks.locations.list
notebooks.operations.get
notebooks.operations.list
notebooks.runtimes.get
notebooks.
runtimes.
getIamPolicy
notebooks.runtimes.list
notebooks.schedules.get
notebooks.
schedules.
getIamPolicy
notebooks.schedules.list
resourcemanager.projects.get
resourcemanager.projects.list
serviceusage.quotas.get
serviceusage.services.get
serviceusage.services.list
Basic roles
The older Google Cloud
basic roles
are common to all Google Cloud services. These roles are Owner, Editor,
and Viewer.
The basic roles provide permissions across Google Cloud, not just for
Vertex AI Workbench. For this reason, you should
use Vertex AI Workbench roles whenever possible.
Custom roles
If the predefined IAM roles for Vertex AI Workbench
don't meet your needs, you can define custom roles. Custom roles enable you
to choose a specific set of permissions, create your own role with
those permissions, and grant the role to users in your organization.
For more information, see
Understanding
IAM custom roles
.
Project-level access versus resource-level policies
A resource inherits all policies from its
ancestry
.
A
policy
set at the resource level doesn't
affect project-level policies.  You can use project-level access and
resource-level policies to customize permissions.
For example, you can grant users
roles/notebooks.viewer
permissions
at the project level so that they can view all
Vertex AI Workbench resources in the project,
and then you can grant each user
roles/notebooks.admin
permissions
on a specific user-managed notebooks instance so that they
have all of the
admin
abilities to administer that instance.
Not all Vertex AI Workbench predefined roles and resources support
resource-level policies. To see which roles can be used on which resources,
view the descriptions
for each role.
Changes to the ability to access a resource take time to propagate. For more
information, see
Access change
propagation
.
What's next
Grant a principal access to
a user-managed notebooks instance.
Grant a principal access to
JupyterLab.
Learn more about
IAM
.
Learn how to
create and manage custom IAM
roles
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/images.txt
Choose a virtual machine image for your Vertex AI Workbench user-managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Choose a virtual machine image
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
User-managed notebooks instances are
Deep Learning VM Images
instances
with JupyterLab notebook environments that are enabled and ready to use.
Specific user-managed notebooks images are available
to suit your choice of
framework and processor. To find the image that you want,
see the following table.
Decide on an image family
To ensure that your instance uses a supported image family, create an instance
by referencing an image family with
-notebooks
in
the name. The following table lists the default versions of image families,
organized by framework type. If you need a specific framework version that
isn't shown here,
see
Supported framework versions
.
Framework
Processor
Image family names
Base
GPU
common-cu110-notebooks
common-cu113-notebooks
common-cu118-notebooks
common-cu121-notebooks
CPU
common-cpu-notebooks
TensorFlow Enterprise
GPU
tf-ent-2-13-cu113-notebooks
PyTorch
GPU
pytorch-2-2-cu121-notebooks
R
CPU (experimental)
r-4-1-cpu-experimental-notebooks
Choose an operating system
Debian 11 is the default OS for most frameworks. Ubuntu 22.04
images are available for some frameworks.
Ubuntu 22.04 images are denoted by the
-ubuntu-2204
suffixes in the image family name (see
List all available
versions
). Debian 10 and Debian 9 images are deprecated.
PyTorch and TensorFlow Enterprise
image families support A100 GPU accelerators.
TensorFlow Enterprise images
TensorFlow Enterprise image families provide a Google Cloud
optimized distribution of TensorFlow.
For more information about TensorFlow Enterprise including which versions
are supported, see
TensorFlow Enterprise overview
.
Experimental images
The
table of image families
shows the
user-managed notebooks image families that are experimental.
Experimental images are
supported on a best-effort basis and might not receive
refreshes on each new release of the framework.
Specify an image version
When you use an
image family name
to create a
user-managed notebooks instance, you get the most recent image
of that version of the framework.
For example, if you create a user-managed notebooks instance
based on the family name
tf-ent-2-13-cu113-notebooks
,
the specific image name might look like
tf-ent-2-13-cu113-notebooks-v20230716
.
To create multiple user-managed notebooks instances based on the
exact same image, use the
image name
instead of the image family name.
To determine the exact name of the most recent image, run the following command
by using the
Google Cloud CLI
in your preferred terminal or in
Cloud Shell
. Replace
IMAGE_FAMILY
with the image family name for which you want
the most recent version number.
gcloud compute images describe-from-family
IMAGE_FAMILY
\
    --project deeplearning-platform-release
In the output, look for the
name
field, and use that image name
when you create instances.
Supported framework versions
Vertex AI supports each framework version based on a schedule to
minimize security vulnerabilities. Review the
Vertex AI framework
support policy
to understand the
implications of the end-of-support and end-of-availability dates.
If you need a specific framework or CUDA version, see the following tables. To
find a specific
VERSION_DATE
for an image, see
Listing the available
versions
.
Base versions
ML framework version
Current patch version
Supported accelerators
End of patch and support date
End of availability date
Image family name
Base-CPU (Python 3.10 / Debian 11)
Not applicable (N/A)
CPU only
Jul 1, 2024
Jul 1, 2025
common-cpu-
VERSION_DATE
-debian-11
Base-cu122 (Python 3.10)
CUDA 12.2
GPU (CUDA 12.2)
Jan 8, 2025
Jan 8, 2026
common-cu122-
VERSION_DATE
-debian-11-py310
Base-cu121 (Python 3.10)
CUDA 12.1
GPU (CUDA 12.1)
Feb 28, 2024
Feb 28, 2025
common-cu121-
VERSION_DATE
-debian-11-py310
Base-cu118 (Python 3.10)
CUDA 11.8
GPU (CUDA 11.8)
Jul 1, 2024
Jul 1, 2025
common-cu118-
VERSION_DATE
-debian-11-py310
Base-cu113 (Python 3.10)
CUDA 11.3
GPU (CUDA 11.3)
Jan 1, 2024
Jan 1, 2025
common-cu113-
VERSION_DATE
-debian-11-py310
Base-cu113 (Python 3.7)
CUDA 11.3
GPU (CUDA 11.3)
Sep 18, 2023
Sep 18, 2024
common-cu113-
VERSION_DATE
-py37
Base-cu110 (Python 3.7)
CUDA 11.0
GPU (CUDA 11.0)
Sep 18, 2023
Sep 18, 2024
common-cu110-
VERSION_DATE
-py37
Base-CPU (Python 3.7)
Not applicable (N/A)
CPU only
Sep 18, 2023
Sep 18, 2024
common-cpu-
VERSION_DATE
-debian-10
TensorFlow versions
ML framework version
Current patch version
Supported accelerators
End of patch and support date
End of availability date
Image family name
2.17 (Python 3.10)
2.17.0
CPU only
Jul 11, 2025
Jul 11, 2026
tf-2-17-cpu-
VERSION_DATE
-py310
2.17 (Python 3.10)
2.17.0
GPU (CUDA 12.3)
Jul 11, 2025
Jul 11, 2026
tf-2-17-cu123-
VERSION_DATE
-py310
2.16 (Python 3.10)
2.16.2
CPU only
Jun 28, 2025
Jun 28, 2026
tf-2-16-cpu-
VERSION_DATE
-py310
2.16 (Python 3.10)
2.16.2
GPU (CUDA 12.3)
Jun 28, 2025
Jun 28, 2026
tf-2-16-cu123-
VERSION_DATE
-py310
2.15 (Python 3.10)
2.15.0
CPU only
Nov 14, 2024
Nov 14, 2025
tf-2-15-cpu-
VERSION_DATE
-py310
2.15 (Python 3.10)
2.15.0
GPU (CUDA 12.1)
Nov 14, 2024
Nov 14, 2025
tf-2-15-cu121-
VERSION_DATE
-py310
2.14 (Python 3.10)
2.14.0
CPU only
Sep 26, 2024
Sep 26, 2025
tf-2-14-cpu-
VERSION_DATE
-py310
2.14 (Python 3.10)
2.14.0
GPU (CUDA 11.8)
Sep 26, 2024
Sep 26, 2025
tf-2-14-cu118-
VERSION_DATE
-py310
2.13 (Python 3.10)
2.13.0
CPU only
Jul 5, 2024
Jul 5, 2025
tf-2-13-cpu-
VERSION_DATE
-py310
2.13 (Python 3.10)
2.13.0
GPU (CUDA 11.8)
Jul 5, 2024
Jul 5, 2025
tf-2-13-cu118-
VERSION_DATE
-py310
2.12 (Python 3.10)
2.12.0
CPU only
June 30, 2024
June 30, 2025
tf-2-12-cpu-
VERSION_DATE
-py310
2.12 (Python 3.10)
2.12.0
GPU (CUDA 11.8)
Jan 18, 2024
Jan 18, 2025
tf-2-12-cu113-
VERSION_DATE
-py310
2.11 (Python 3.10)
2.11.0
CPU only
Nov 15, 2023
Nov 15, 2024
tf-2-11-cpu-
VERSION_DATE
-py310
2.11 (Python 3.10)
2.11.0
GPU (CUDA 11.3)
Nov 15, 2023
Nov 15, 2024
tf-2-11-cu113-
VERSION_DATE
-py310
2.11
2.11.0
CPU only
Nov 15, 2023
Nov 15, 2024
tf-2-11-cpu-
VERSION_DATE
-py37
2.11
2.11.0
GPU (CUDA 11.3)
Nov 15, 2023
Nov 15, 2024
tf-2-11-cu113-
VERSION_DATE
-py37
2.10
2.10.1
CPU only
Nov 15, 2023
Nov 15, 2024
tf-2-10-cpu-
VERSION_DATE
-py37
2.10
2.10.1
GPU (CUDA 11.3)
Nov 15, 2023
Nov 15, 2024
tf-2-10-cu113-
VERSION_DATE
-py37
2.9
2.9.3
CPU only
Nov 15, 2023
Nov 15, 2024
tf-2-9-cpu-
VERSION_DATE
-py37
2.9
2.9.3
GPU (CUDA 11.3)
Nov 15, 2023
Nov 15, 2024
tf-2-9-cu113-
VERSION_DATE
-py37
2.8
2.8.4
CPU only
Nov 15, 2023
Nov 15, 2024
tf-2-8-cpu-
VERSION_DATE
-py37
2.8
2.8.4
GPU (CUDA 11.3)
Nov 15, 2023
Nov 15, 2024
tf-2-8-cu113-
VERSION_DATE
-py37
2.6 (py39)
2.6.5
CPU only
Sep 1, 2023
Sep 1, 2024
tf-2-6-cpu-
VERSION_DATE
-py39
2.6 (py39)
2.6.5
GPU (CUDA 11.3)
Sep 1, 2023
Sep 1, 2024
tf-2-6-cu110-
VERSION_DATE
-py39
2.6 (py37)
2.6.5
CPU only
Sep 18, 2023
Sep 18, 2024
tf-2-6-cpu-
VERSION_DATE
-py37
2.6 (py37)
2.6.5
GPU (CUDA 11.3)
Sep 18, 2023
Sep 18, 2024
tf-2-6-cu110-
VERSION_DATE
-py37
2.3
2.3.4
CPU only
Sep 1, 2023
Sep 1, 2024
tf-2-3-cpu
2.3
2.3.4
GPU (CUDA 11.3)
Sep 1, 2023
Sep 1, 2024
tf-2-3-cu110-
VERSION_DATE
PyTorch versions
ML framework version
Current patch version
Supported accelerators
End of patch and support date
End of availability date
Image family name
2.2 (Python 3.10)
2.2.0
CUDA 12.1
Jan 30, 2025
Jan 30, 2026
pytorch-2-2-
VERSION_DATE
-py310
2.1 (Python 3.10)
2.1.0
CUDA 12.1
Oct 4, 2024
Oct 4, 2025
pytorch-2-1-
VERSION_DATE
-py310
2.0 (Python 3.10)
2.0.0
CUDA 11.8
Mar 15, 2024
Mar 15, 2025
pytorch-2-0-
VERSION_DATE
-py310
1.13 (Python 3.10)
1.13.1
CUDA 11.3
Dec 8, 2023
Dec 8, 2024
pytorch-1-13-
VERSION_DATE
-py310
1.13
1.13.1
CUDA 11.3
Dec 8, 2023
Dec 8, 2024
pytorch-1-13-
VERSION_DATE
-py37
1.12
1.12.1
CUDA 11.3
Sep 1, 2023
Sep 1, 2024
pytorch-1-12-
VERSION_DATE
-py310
List all available versions using gcloud CLI
You can also list all available Vertex AI images using the
following
gcloud CLI
command:
gcloud compute images list \
    --project deeplearning-platform-release | grep notebooks
Image family names are listed in the following format:
FRAMEWORK
-
VERSION
-
CUDA_VERSION
(-experimental)-notebooks
FRAMEWORK
: the target library
VERSION
: the framework version
CUDA_VERSION
: the version of the CUDA stack,
if present.
For example, an image from the family
tf-ent-2-13-cu113-notebooks
has
TensorFlow Enterprise 2.13 and CUDA 11.3.
What's next
Learn more about
Deep Learning VM
instances
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/introduction.txt
Introduction to Vertex AI Workbench user-managed notebooks  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Introduction to user-managed notebooks
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
Vertex AI Workbench user-managed notebooks instances
let you create and manage deep learning virtual machine
(VM) instances that are prepackaged with
JupyterLab
.
User-managed notebooks instances have
a preinstalled suite of deep learning packages,
including support for the TensorFlow and PyTorch
frameworks. You can configure either CPU-only or GPU-enabled instances.
Your user-managed notebooks instances are protected
by Google Cloud
authentication and authorization and are available by using a
user-managed notebooks instance URL.
User-managed notebooks instances also integrate with
GitHub
and can sync with a GitHub repository.
User-managed notebooks instances save you
the difficulty of creating and
configuring a
Deep Learning virtual machine
by providing verified, optimized, and tested images
for your chosen framework.
Preinstalled software
You can configure a user-managed notebooks instance
to include the following:
JupyterLab (
see version details
)
Python 3, with key packages:
numpy
sklearn
scipy
pandas
nltk
pillow
fairness-indicators
for TensorFlow 2.3 and 2.4
user-managed notebooks instances
many others
R version 4.
x
, with key packages:
xgboost
ggplot2
caret
nnet
rpy2 (an R package for accessing R in Python notebooks)
randomForest
many others
Anaconda
Nvidia packages with the latest Nvidia driver for GPU-enabled instances:
CUDA 11.
x
and 12.
x
CuDNN 7.
x
NCCL 2.
x
JupyterLab version details
JupyterLab 3.
x
is preinstalled on
new user-managed notebooks instances
by default. For instances created before
the
M80 Deep Learning VM
release
,
JupyterLab 1.
x
was preinstalled.
To create an earlier version of a user-managed notebooks instance,
see
Create a specific version of a user-managed notebooks
instance
.
VPC Service Controls
VPC Service Controls provides additional security for your
user-managed notebooks instances.
For more information, see the
Overview of
VPC Service Controls
. To use
user-managed notebooks within a service perimeter, see
Use
a user-managed notebooks instance within a service
perimeter
.
Upgrades
You can upgrade your environment to use new capabilities and to benefit from
framework updates, package updates, and bug fixes. You can
upgrade environments manually or through an automatic update setting.
To learn more, see
Upgrade the environment of
a user-managed notebooks instance
.
User-managed notebooks and Dataproc Hub
Dataproc Hub is a customized
JupyterHub
server.
Administrators can create Dataproc Hub instances that can
spawn single-user
Dataproc
clusters to host
user-managed notebooks environments. For more information, see
Configure Dataproc Hub
.
User-managed notebooks and Dataflow
You can use user-managed notebooks within a pipeline,
and then run
the pipeline on
Dataflow
. For information about
how to create an
Apache Beam
user-managed notebooks instance that you can use with
Dataflow, see
Developing interactively with Apache Beam
notebooks
.
Limitations
Consider the following limitations of
user-managed notebooks when planning your project:
User-managed notebooks instances are highly
customizable and can be
ideal for users who need a lot of control over their environment.
Therefore, user-managed notebooks instances
can require more time to set up and manage than
managed notebooks instances.
Managed notebooks instances can be
more ideal for users who don't need a lot of control over their environment.
For more information, see
Introduction to
managed notebooks
.
Third party JupyterLab extensions are not supported.
The Dataproc JupyterLab plugin isn't supported for
user-managed notebooks, but you can use the plugin in
Vertex AI Workbench instances. See
Create a
Dataproc-enabled
instance
.
For Dataproc Hub user-managed notebooks instances,
disabling file downloading from the JupyterLab user interface
is not supported. User-managed notebooks instances
that use the Dataproc Hub framework permit file downloading even
if you don't select
Enable file downloading from JupyterLab UI
when you create the instance.
When you use
Access Context Manager
and
Chrome Enterprise Premium
to protect managed notebooks instances with
context-aware access controls, access is evaluated each time
the user authenticates to the instance. For example, access
is evaluated the first time the user accesses JupyterLab and
whenever they access it thereafter if their web browser's
cookie has expired.
Pricing
Learn more about Vertex AI Workbench
pricing
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/manage-access-jupyterlab.txt
Manage access to a Vertex AI Workbench user-managed notebooks instance's JupyterLab interface  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Manage access to a user-managed notebooks instance's JupyterLab interface
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
This page describes how to grant access to the JupyterLab interface
of a Vertex AI Workbench user-managed notebooks instance.
You control access to a user-managed notebooks instance's
JupyterLab interface through the instance's access mode.
You set a JupyterLab access mode when you create
a user-managed notebooks instance.
The access mode can't be changed after the notebook is created.
The JupyterLab access mode determines who can use
the instance's JupyterLab interface.
The access mode also determines which credentials are used when
your instance interacts with other Google Cloud services.
Access limitations
Granting a principal access to
a user-managed notebooks instance's JupyterLab interface
doesn't grant access to the instance itself. For example,
to start, stop, or reset an instance, you must grant the principal
access to perform those operations by setting an
IAM policy
on the instance.
To grant access to the user-managed notebooks instance,
see
Manage access to
a user-managed notebooks instance
.
JupyterLab access modes
User-managed notebooks instances support the
following access modes:
Single user only
: The
Single user only
access mode
grants access only to the user that you specify.
Service account
: The
Service account
access mode
grants access to a service account. You can grant access to one or more
users through this service account.
Note:
To grant access to the instance through the single user option
      or the service account, you must use an individual's
      user account email address. Group access is not supported.
Single user only
When you create a user-managed notebooks instance
with
Single user only
access, you specify a user account.
The specified user account is the only user with access to
the JupyterLab interface. If the specified user is not the creator of the
instance, you must grant the specified user the
Service Account User role
(
roles/iam.serviceAccountUser
) on the instance's service account. If the
instance needs to access other Google Cloud resources, this
service account
must also have access to those Google Cloud resources.
Note:
When you create a user-managed notebooks instance
with
Single user only
access, your instance completes the boot process
using the Compute Engine default service account.
Your specified user account can access the instance after the boot process
is finished.
Grant access to a single user
To grant access to a single user, complete the following steps.
Create
a user-managed notebooks instance
with the following specifications:
In the
Create instance
dialog, in
the
IAM and security
section, select the
Single user only
access mode.
In the
User email
field, enter the user account that you want
to grant access.
Complete the rest of the dialog, and then click
Create
.
Service account
When you create a user-managed notebooks instance
with
Service account
access, you specify a service account. If
the instance needs to access
other Google resources, this service account must have access to those
Google resources also.
When you specify a service account,
choose one of the following:
Select the Compute Engine default service account.
Specify a custom service account. The custom service account must be
in the same project as your user-managed notebooks instance.
To create the instance, you must have
the
iam.serviceAccounts.actAs
permission on the service account.
To grant access to users through a service account,
you grant the
iam.serviceAccounts.actAs
permission on
the specified service account for each user who needs
to access JupyterLab.
Grant access to multiple users through a service account
Create
a user-managed notebooks instance
with the following specifications:
In the
Create instance
dialog, in
the
IAM and security
section, select the
Service account
access mode.
Choose the Compute Engine default service account
or a
custom
service account
.
To use the Compute Engine default service account,
select
Use Compute Engine default service account
.
To use a custom service account, clear
Use Compute Engine default service account
, and then,
in the
Service account email
field, enter
your custom service account email address.
Complete the rest of the dialog, and then click
Create
.
For each user who needs to access JupyterLab,
grant the
iam.serviceAccounts.actAs
permission on your
service account
.
Access mode metadata
The access mode that you configure during
user-managed notebooks instance creation
is stored in the notebook metadata.
When you select the
Single user only
access mode,
Vertex AI Workbench stores a value for
proxy-mode
and
proxy-user-mail
.
The following are examples of single user access metadata entries:
proxy-mode=mail
proxy-user-mail=user@example.com
When you select the
Service account
access mode, Vertex AI Workbench
stores a
proxy-mode=service_account
metadata entry.
Caution:
Changing the access mode metadata is not supported and can make the
JupyterLab interface inaccessible.
What's next
Grant a principal access to
a user-managed notebooks instance.
To learn how to grant access to other Google resources, see
Manage access to
other resources
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/manage-access.txt
Manage access to a Vertex AI Workbench user-managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Manage access to a user-managed notebooks instance
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
This guide describes how you can grant access to
a specific Vertex AI Workbench user-managed notebooks instance.
To manage access to Vertex AI resources, see
the
Vertex AI page on access control
.
You grant access to a user-managed notebooks instance by setting an
Identity and Access Management (IAM) policy
on the instance.
The policy binds one or more principals, such as a user or a
service account, to one or more
roles
.
Each role contains a list of permissions that let the principal interact
with the instance.
You can grant access to an instance, instead of to a parent resource
such as a project, folder, or organization, to exercise the
principle of
least privilege
.
If you grant access to a
parent resource
(for example, to a project), you implicitly grant access to all its child
resources (for example, to all instances in that project). To limit access to
resources, set IAM policies on lower-level resources when
possible, instead of at the project level or above.
For general information about how to grant, change, and revoke access to
resources unrelated to Vertex AI Workbench, for example, to grant access to
a Google Cloud project, see the IAM documentation for
managing access to projects, folders, and
organizations
.
Access limitations
Access to an instance can include a broad range of abilities, depending
on the role you assign to the principal. For example,
you might grant a principal the ability to start, stop, upgrade, and
monitor the health status of an instance. For the complete list of
IAM permissions available, see
Predefined
user-managed notebooks IAM
roles
.
However, even granting a principal full access to
a user-managed notebooks instance doesn't grant
the ability to use the instance's JupyterLab interface.
To grant access to the JupyterLab interface, see
Manage access to a
user-managed notebooks instance's
JupyterLab interface
.
Grant access to user-managed notebooks instances
To grant users permission to access
a specific user-managed notebooks instance,
set an
IAM policy
on the instance.
gcloud
To grant a role to a principal on
a user-managed notebooks instance, use the
get-iam-policy
command to retrieve the current policy,
edit the current policy's access, and then use the
set-iam-policy
command to update the policy on the instance.
Retrieve the current policy
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: The name of your user-managed notebooks instance
PROJECT_ID
: Your Google Cloud project ID
ZONE
: The zone where your instance is located
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
get-iam-policy
INSTANCE_NAME
--project
=
PROJECT_ID
--location
=
ZONE
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
get-iam-policy
INSTANCE_NAME
--project
=
PROJECT_ID
--location
=
ZONE
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
get-iam-policy
INSTANCE_NAME
--project
=
PROJECT_ID
--location
=
ZONE
The response is the text of your instance's IAM policy.
See the following for an example.
{
  "bindings": [
    {
      "role": "roles/notebooks.viewer",
      "members": [
        "user:email@example.com"
      ]
    }
  ],
  "etag": "BwWWja0YfJA=",
  "version": 3
}
Edit the policy
Edit the policy with a text editor to add or remove principals and their
associated roles. For example, to grant the
notebooks.admin
role to
eve@example.com
, add the following new binding to the policy
in the
"bindings"
section:
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
After adding the new binding, the policy might look like the following:
{
"bindings"
:
[
{
"role": "roles/notebooks.viewer",
"members": [
"user:email@example.com"
]
}
,
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
]
,
"etag"
:
"BwWWja0YfJA="
,
"version"
:
3
}
Save the updated policy in a file named
request.json
.
Update the policy on the instance
In the body of the request, provide the updated IAM
policy from the previous step, nested inside a
"policy"
section.
Before using any of the command data below,
  make the following replacements:
INSTANCE_NAME
: The name of your user-managed notebooks instance
PROJECT_ID
: Your Google Cloud project ID
ZONE
: The zone where your instance is located
Save the following content in a file called
request.json
:
{
"policy"
:
{
"bindings"
:
[
{
"role"
:
"roles/notebooks.viewer"
,
"members"
:
[
"user:email@example.com"
]
},
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
],
"etag"
:
"BwWWja0YfJA="
,
"version"
:
3
}
}
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
set-iam-policy
INSTANCE_NAME
--project
=
PROJECT_ID
--location
=
ZONE
request.json
--format
=
json
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
set-iam-policy
INSTANCE_NAME
--project
=
PROJECT_ID
--location
=
ZONE
request.json
--format
=
json
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
set-iam-policy
INSTANCE_NAME
--project
=
PROJECT_ID
--location
=
ZONE
request.json
--format
=
json
Grant access to the JupyterLab interface
Granting a principal access to
a user-managed notebooks instance doesn't grant
the ability to use the instance's JupyterLab interface.
To grant access to the JupyterLab interface, see
Manage access to a
user-managed notebooks instance's
JupyterLab interface
.
API
To grant a role to a principal on
a user-managed notebooks instance, use the
getIamPolicy
method to retrieve the current policy,
edit the current policy's access, and then use the
setIamPolicy
method to update the policy on the instance.
Retrieve the current policy
Before using any of the request data,
  make the following replacements:
INSTANCE_NAME
: The name of your user-managed notebooks instance
HTTP method and URL:
GET https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:getIamPolicy
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Execute the following command:
curl -X GET \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:getIamPolicy"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method GET `
-Headers $headers `
-Uri "https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:getIamPolicy" | Select-Object -Expand Content
The response is the text of your instance's IAM policy.
See the following for an example.
{
  "bindings": [
    {
      "role": "roles/notebooks.viewer",
      "members": [
        "user:email@example.com"
      ]
    }
  ],
  "etag": "BwWWja0YfJA=",
  "version": 3
}
Edit the policy
Edit the policy with a text editor to add or remove principals and their
associated roles. For example, to grant the
notebooks.admin
role to
eve@example.com, add the following new binding to the policy
in the
"bindings"
section:
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
After adding the new binding, the policy might look like the following:
{
"bindings"
:
[
{
"role": "roles/notebooks.viewer",
"members": [
"user:email@example.com"
]
}
,
{
"role"
:
"roles/notebooks.admin"
,
"members"
:
[
"user:eve@example.com"
]
}
]
,
"etag"
:
"BwWWja0YfJA="
,
"version"
:
3
}
Update the policy on the instance
In the body of the request, provide the updated IAM
policy from the previous step, nested inside a
"policy"
section.
Before using any of the request data,
  make the following replacements:
INSTANCE_NAME
: The name of your user-managed notebooks instance
HTTP method and URL:
POST https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:setIamPolicy
Request JSON body:
{
  "policy": {
    "bindings": [
      {
        "role": "roles/notebooks.viewer",
        "members": [
          "user:email@example.com"
        ]
      },
      {
        "role": "roles/notebooks.admin",
        "members": [
          "user:eve@example.com"
        ]
      }
    ],
    "etag": "BwWWja0YfJA=",
    "version": 3
  }
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:setIamPolicy"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://notebooks.googleapis.com/v1/
INSTANCE_NAME
:setIamPolicy" | Select-Object -Expand Content
You should receive a successful status code (2xx) and an empty response.
Grant access to the JupyterLab interface
Granting a principal access to
a user-managed notebooks instance doesn't grant
the ability to use the instance's JupyterLab interface.
To grant access to the JupyterLab interface, see
Manage access to a
user-managed notebooks instance's
JupyterLab interface
.
What's next
Grant a principal access to
JupyterLab.
To learn about Identity and Access Management (IAM) and how
IAM roles can help grant and restrict access,
see the
IAM documentation
.
Learn about the
IAM roles available
to Vertex AI Workbench
user-managed notebooks
.
Learn how to create and manage
custom roles
.
To learn how to grant access to other Google resources, see
Manage access to
other resources
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/manage-hardware-accelerators.txt
Change machine type and configure GPUs of a Vertex AI Workbench user-managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Change machine type and configure GPUs of a user-managed notebooks instance
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
This page shows you how to change the machine type and
GPU configuration of a user-managed notebooks instance.
Overview
The machine type determines some specifications of
your user-managed notebooks instance,
such as the amount of memory, virtual cores, and persistent disk limits.
Changing the machine type can improve performance or help avoid errors
caused by high memory utilization.
GPUs provide hardware acceleration that can improve the performance
of your notebook code. You may want to add or increase the number of GPUs
for greater performance, or, to reduce the cost of running
your user-managed notebooks instance,
you may want to remove GPUs.
Before you change a machine type or GPU configuration
Consider the following before you make any changes to
your user-managed notebooks instance's machine type
or GPU configuration.
Billing implications
Each machine type and GPU configuration is billed at a different rate.
Make sure you understand
the
pricing
implications of making a change.
For example, an
e2-highmem-2
machine type costs more than an
e2-standard-2
machine type.
Changing a machine type might also affect sustained use discounts.
Sustained use discounts are calculated
separately for different
categories
in
the same region. If you change machine types so that the new machine type is
in a different category, the subsequent running time of
your instance's VM counts toward the
sustained use discount of the new category.
Moving to a smaller machine type
If you move from a machine type with more resources to a machine type with fewer
resources, such as moving from an
e2-standard-8
machine type to an
e2-standard-2
, you might run into hardware resource issues or performance
limitations because smaller machine types are less powerful than larger machine
types.
Make a backup
It's good practice to make regular backups of
your instance's persistent disk data using snapshots. Consider
taking a
snapshot
of your
persistent disk data before you change the machine type. If you want to make
sure the new machine type is able to support the data on the existing instance,
you can take a persistent disk snapshot and use it to start a second instance
with the new machine type to confirm that the instance starts up successfully.
Change the machine type and configure GPUs
Note:
To change the machine type or GPUs for
    a user-managed notebooks instance,
    you must
shut down the user-managed notebooks
    instance
.
To change the machine type or configure the GPUs on
a user-managed notebooks instance, complete the following steps.
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
In the
Notebook name
column, click the name of the
instance that you want to modify.
The
Notebook details
page opens.
Click the
Hardware
tab.
In the
Modify hardware configuration
section,
select the
Machine type
that you want to use.
If there are GPUs available for your instance's combination of
zone, environment, and machine type, you can configure the GPUs.
In the
GPUs
section, select the
GPU type
and
Number of GPUs
that you want to use.
Learn more about GPU regions and zone
availability
.
If you haven't already installed the required GPU drivers on your instance,
select
Install NVIDIA GPU driver automatically for me
to install the drivers automatically on next startup.
After Vertex AI Workbench has finished updating the machine type
and GPU configuration, you can start
your user-managed notebooks instance.
What's next
Learn more about the available
GPU platforms
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/migrate-to-instances.txt
Migrate from user-managed notebooks to Vertex AI Workbench instances  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Migrate from user-managed notebooks to Vertex AI Workbench instances
Vertex AI Workbench user-managed notebooks is
deprecated
.
    On April 14, 2025, support for
    user-managed notebooks will end and
    the ability to create user-managed notebooks instances
    will be removed. Existing instances will continue to function
    but patches, updates, and upgrades won't be available. To continue using
    Vertex AI Workbench, complete the steps on this page to
migrate
    your user-managed notebooks instances to
    Vertex AI Workbench instances
.
This page describes how to migrate from
a user-managed notebooks instance to a Vertex AI Workbench instance.
You can migrate by using the Vertex AI Workbench migration tool or
migrate your instance's data and files manually.
Overview of the migration tool
Vertex AI Workbench provides a migration tool for migrating from
a user-managed notebooks instance to a Vertex AI Workbench instance.
The migration tool creates a Vertex AI Workbench instance with a
configuration similar to the user-managed notebooks instance
that you want to migrate. For example,
the migration tool creates an instance that has the same or similar
machine type, network configuration, idle shutdown settings,
and other specifications. Then, the files on your
user-managed notebooks instance's data disk are copied to the
Vertex AI Workbench instance.
Vertex AI Workbench doesn't delete or change your
user-managed notebooks instance, so after migration you
can continue to use it. If you don't need the
user-managed notebooks instance
anymore,
delete it
to avoid further charges for that instance.
Billing
If your user-managed notebooks instance
uses Extreme Persistent Disks,
then your migration generates charges for I/O operations.
See "Extreme provisioned IOPS" in the
Persistent Disk and Hyperdisk pricing section of
Disk pricing
.
After migration, the user-managed notebooks instance still exists
and generates charges as before. If you don't need
the user-managed notebooks instance
anymore,
delete it
to avoid further charges for that instance.
Default migration tool behaviors
The Vertex AI Workbench migration tool attempts to
migrate your user-managed notebooks instance to
a Vertex AI Workbench instance with matching specifications.
When a specification in your user-managed notebooks instance
isn't available in Vertex AI Workbench instances, Vertex AI Workbench
uses a default specification when possible. When the migration tool
can't migrate a specification of
your user-managed notebooks instance,
it doesn't migrate the instance.
The following table lists some of the key default migration behaviors
for the migration tool.
Category
user-managed notebooks specification
Migration result
OS
Any Ubuntu version
Debian 11
Any Debian version
Debian 11
Framework
Any CUDA version
CUDA 11.3
Any Python version
Python 3.10
Any PyTorch version
PyTorch 1.13
Any TensorFlow version
TensorFlow 2.11
Any R version
Not migrated; see
Add
        a conda environment
Any local PySpark version
Not migrated; see
Add
        a conda environment
Any XGBoost version
Not migrated; see
Add
        a conda environment
Any Kaggle Python version
Not migrated; see
Add
        a conda environment
Any Jax version
Not migrated; see
Add
        a conda environment
Any Apache Beam version
Not migrated; see
Add
        a conda environment
Machine type
A supported machine type
Identical machine type
An unsupported machine type
e2-standard-4
Accelerators
Supported accelerators
Identical accelerators
Unsupported accelerators
Migration doesn't include accelerators
Setting
Idle shutdown
Migrated
Delete to trash
Migrated
nbconvert
Migrated
File downloading
Migrated
Terminal access
Migrated
Other
Identity and Access Management permissions
Migrated, though new permissions might be required to use
        the Vertex AI Workbench instance
Access mode
Migrated
Network
Migrated
Post-startup script
When using the Google Cloud console, the instance is migrated without
        the post-startup script; to migrate the instance with the
        post-startup script, use the Google Cloud CLI or REST API to
specify the
PostStartupScriptOption
option
Dataproc Hub
Not migrated; must
migrate manually
Specifying the post-startup script
User-managed notebooks instances that use a post-startup script
must be migrated to an instance with the
PostStartupScriptOption
option specified. Use this option to indicate whether you want to
skip or rerun the post-startup script in your new
Vertex AI Workbench instance.
Specifying the
PostStartupScriptOption
option isn't supported
in the Google Cloud console. To specify the
PostStartupScriptOption
option when you migrate
your user-managed notebooks instance, you must
use the Google Cloud CLI or REST API.
Before you begin
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
Required roles
To get the permissions that
      you need to migrate a user-managed notebooks instance to a Vertex AI Workbench instance,
    
      ask your administrator to grant you the
    
  
  Notebooks Runner (
roles/notebooks.runner
)
   IAM role on the project.
  

  

  
  
  For more information about granting roles, see
Manage access to projects, folders, and organizations
.
This predefined role contains
        
        the permissions required to migrate a user-managed notebooks instance to a Vertex AI Workbench instance. To see the exact permissions that are
        required, expand the
Required permissions
section:
Required permissions
The following permissions are required to migrate a user-managed notebooks instance to a Vertex AI Workbench instance:
notebooks.instances.create
notebooks.instances.get
You might also be able to get
          these permissions
        with
custom roles
or
        other
predefined roles
.
Pre-migration check
Before you migrate, check your user-managed notebooks instance's
migration eligibility by listing your instances and checking
the output for any migration warnings or errors.
List your instances
To list your user-managed notebooks instances that aren't
migrated yet, use the
projects.locations.instances.list
method with the filter
migrated:false
. You can list them by using the
gcloud CLI or REST API:
gcloud
Before using any of the command data below,
  make the following replacements:
PROJECT_ID
: Your project ID
LOCATION
: The region where your user-managed notebooks instance is located, or use
-
to list instances from all regions
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
list
--project
=
PROJECT_ID
\
--location
=
LOCATION
--filter
=
migrated:false
--format
=
default
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
list
--project
=
PROJECT_ID
`
--location
=
LOCATION
--filter
=
migrated:false
--format
=
default
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
list
--project
=
PROJECT_ID
^
--location
=
LOCATION
--filter
=
migrated:false
--format
=
default
REST
Before using any of the request data,
  make the following replacements:
PROJECT_ID
: Your project ID
LOCATION
: The region where your user-managed notebooks instance is located, or use
-
to list instances from all regions
HTTP method and URL:
GET https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/instances?filter=migrated:false
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Execute the following command:
curl -X GET \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
"https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/instances?filter=migrated:false"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method GET `
-Headers $headers `
-Uri "https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/instances?filter=migrated:false" | Select-Object -Expand Content
Check the output for warnings or errors
If migration warnings or errors are detected, the output of
the
projects.locations.instances.list
method includes this information.
Warnings appear when specific components in
your user-managed notebooks instance's configuration won't
migrate to the same specification in a Vertex AI Workbench instance.
For example, if your user-managed notebooks instance uses an
unsupported accelerator, a warning appears in the output. In this case,
the instance is migrated without any accelerators. You can attach
accelerators after migration. Review the warnings in
the output, consider the
migration tool's default behaviors
,
and assess whether the migration tool is acceptable for your migration.
One or more errors in the output means that you can't migrate
the user-managed notebooks instance by using the migration tool.
You must
migrate the instance manually
.
For more information about migration warnings and errors, see
warnings
and
errors
in the
InstanceMigrationEligibility
documentation.
Migrate by using the migration tool
You can migrate your user-managed notebooks instance by using
the Google Cloud console, the gcloud CLI, or REST API.
Console
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Click the
Migrate
button.
On the
Migrate user-managed notebooks to instances
page,
on the
Ready
tab, select the instance that you want to migrate.
Click
Migrate
.
After the migration is finished, go to the
Instances
page
to view your new Vertex AI Workbench instance.
Go to Instances
gcloud
Before using any of the command data below,
  make the following replacements:
PROJECT_ID
: Your project ID
LOCATION
: The region where your user-managed notebooks instance is located
INSTANCE_ID
: The ID of the user-managed notebooks instance
POST_STARTUP_SCRIPT_OPTION
: Optional: One of the
post-startup script options
Execute the
  
  following
  
  command:
Linux, macOS, or Cloud Shell
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
migrate
RUNTIME_ID
\
--project
=
PROJECT_ID
\
--location
=
LOCATION
\
--post-startup-script-option
=
POST_STARTUP_SCRIPT_OPTION
Windows (PowerShell)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
migrate
RUNTIME_ID
`
--project
=
PROJECT_ID
`
--location
=
LOCATION
`
--post-startup-script-option
=
POST_STARTUP_SCRIPT_OPTION
Windows (cmd.exe)
Note:
Ensure you have initialized the Google Cloud CLI with authentication and a project
  by running either
gcloud init
;
  or
gcloud auth login
and
gcloud config set project
.
gcloud
notebooks
instances
migrate
RUNTIME_ID
^
--project
=
PROJECT_ID
^
--location
=
LOCATION
^
--post-startup-script-option
=
POST_STARTUP_SCRIPT_OPTION
REST
Before using any of the request data,
  make the following replacements:
PROJECT_ID
: Your project ID
LOCATION
: The region where your user-managed notebooks instance is located
INSTANCE_ID
: The ID of the user-managed notebooks instance
POST_STARTUP_SCRIPT_OPTION
: Optional: One of the
post-startup script options
HTTP method and URL:
POST https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_ID
:migrate
Request JSON body:
{
  "postStartupScriptOption": (
POST_STARTUP_SCRIPT_OPTION
)
}
To send your request, choose one of these options:
curl
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
, or by using
Cloud Shell
,
            which automatically logs you into the
gcloud
CLI
            .
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json; charset=utf-8" \
-d @request.json \
"https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_ID
:migrate"
PowerShell
Note:
The following command assumes that you have logged in to
          the
gcloud
CLI with your user account by running
gcloud init
or
gcloud auth login
.
          You can check the currently active account by running
gcloud auth list
.
Save the request body in a file named
request.json
,
      and execute the following command:
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
-Method POST `
-Headers $headers `
-ContentType: "application/json; charset=utf-8" `
-InFile request.json `
-Uri "https://notebooks.googleapis.com/v1/projects/
PROJECT_ID
/locations/
LOCATION
/instances/
INSTANCE_ID
:migrate" | Select-Object -Expand Content
Migrate manually
To migrate your instance to a Vertex AI Workbench instance manually,
consider using the following methods:
Use Cloud Storage and the terminal
: Copy your data and files
to Cloud Storage and then to another instance by using the terminal.
Use GitHub
: Copy your data and files to a GitHub repository
by using the Git extension for JupyterLab.
This guide describes how to migrate data and files by using
Cloud Storage and the terminal.
Requirements
You must have terminal access to your
user-managed notebooks instance.
Terminal access is manually set when you create an instance. The
terminal access setting cannot be changed after the instance is created.
Migrate manually by using Cloud Storage and the terminal
To migrate data and files to a new Vertex AI Workbench instance
by using Cloud Storage and the terminal, do the following.
Create a Cloud Storage bucket
in the same project where your user-managed notebooks instance
is located.
In that same project,
Create
a Vertex AI Workbench instance
to migrate your data to.
When you create this instance:
Enable terminal access.
Specify the machine type, network,
and other characteristics to match what you need.
In your user-managed notebooks instance's
JupyterLab interface, select
File
>
New
>
Terminal
to open a terminal window.
Use the
gcloud CLI
to copy your user data
to a Cloud Storage bucket. The following example command
copies all of the files from your instance's
/home/jupyter/
directory
to a directory in a Cloud Storage bucket.
gcloud
storage
cp
/home/jupyter/*
gs://
BUCKET_NAME
PATH
--recursive
Replace the following:
BUCKET_NAME
: The name of your
Cloud Storage bucket
PATH
: The path to the directory
where you want to copy your files, for example:
/copy/jupyter/
In your new Vertex AI Workbench instance's
JupyterLab interface, select
File
>
New
>
Terminal
to open a terminal window.
Use the gcloud CLI to copy your data to the new instance.
The following example command copies all of
the files from a Cloud Storage directory to the
your new instance's
/home/jupyter/
directory.
gcloud
storage
cp
gs://
BUCKET_NAME
PATH
*
/home/jupyter/
--recursive
Confirm the migration
After the migration, the original user-managed notebooks instance
continues to work as before. Confirm that your migration was a success
before you delete the original instance.
Delete the user-managed notebooks instance
If you don't need the user-managed notebooks instance that
you migrated from, delete it to avoid further charges for that instance.
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Select the instance that you want to delete.
Click
delete
Delete
.
(Depending on the size of your window,
the
Delete
button might be in
the
more_vert
options menu.)
To confirm, click
Delete
.
Troubleshoot
To find methods for diagnosing and resolving migration issues, see
Troubleshooting
Vertex AI Workbench
.
What's next
Learn more about
Vertex AI Workbench instances
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/migrate.txt
Migrate data to a new Vertex AI Workbench user-managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Migrate data to a new user-managed notebooks instance
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
This page describes how to migrate data and files from
an existing user-managed notebooks instance to a new one.
When to migrate
You might need to migrate your data to
a new user-managed notebooks instance if
you can't upgrade the environment of your existing instance.
See the
requirements for upgrading the environment of
a user-managed notebooks instance
.
Migration options
To migrate data and files from one user-managed notebooks instance to another,
consider using the following methods:
Use GitHub
: Copy your data and files to a GitHub repository
by using the Git extension for JupyterLab.
To use this method,
see
Save a notebook to
GitHub
.
Use Cloud Storage and the terminal
: Copy your data and files
to Cloud Storage and then to another instance by using the terminal.
Use Cloud Storage within JupyterLab notebooks
:
Copy your data and files to Cloud Storage and then to another instance
by running commands within your respective instances' notebook cells.
This guide describes how to migrate data and files by using
Cloud Storage and the terminal.
Requirements
You must have terminal access to your user-managed notebooks instance.
Terminal access is manually set when you create an instance. The
terminal access setting cannot be changed after the instance is created.
Before you begin
Create a Cloud Storage bucket
in the same project where your user-managed notebooks instance
is located.
Migrate your data to a new user-managed notebooks instance
To migrate data and files to a new user-managed notebooks instance
by using Cloud Storage and the terminal, complete the following steps.
In your user-managed notebooks instance's
JupyterLab interface, select
File
>
New
>
Terminal
to open a terminal window.

Or connect to your instance's terminal
by
using SSH
.
Use the
gcloud CLI
to copy your user data
to a Cloud Storage bucket. The following example command
copies all of the files from your instance's
/home/jupyter/
directory
to a directory in a Cloud Storage bucket.
gcloud
storage
cp
/home/jupyter/*
gs://
BUCKET_NAME
PATH
--recursive
Replace the following:
BUCKET_NAME
: the name of your
Cloud Storage bucket
PATH
: the path to the directory
where you want to copy your files, for example:
/copy/jupyter/
Open your user-managed notebooks instance's JupyterLab interface.
In your user-managed notebooks instance's
JupyterLab interface, select
File
>
New
>
Terminal
to open a terminal window.
Use the gcloud CLI to restore your data on the new instance.
The following example command copies all of
the files from a Cloud Storage directory to the
your new instance's
/home/jupyter/
directory.
gcloud
storage
cp
gs://
BUCKET_NAME
PATH
*
/home/jupyter/
What's next
Learn how to
automatically and manually upgrade the environment of
user-managed notebooks instances
.
Learn more about
using
SSH access
to connect
to your user-managed notebooks instance.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/monitor-health.txt
Monitor health status  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Monitor health status
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
User-managed notebooks instances provide several methods
for monitoring the health
of your notebooks. This page describes how
to use each method.
Methods for monitoring health status
You can monitor the health of your user-managed notebooks
instances in a few different ways. This page describes how to
use the following methods:
Use guest attributes to report
system health
Report custom metrics to Cloud Monitoring
Report system and application metrics
to Monitoring by installing Monitoring
on a user-managed notebooks instance
Use the diagnostic tool
Set up the gcloud CLI
To complete some of the steps on this page, you need to use the
Google Cloud CLI.
Install
the Google Cloud CLI.
        
          After installation,
initialize
the Google Cloud CLI by running the following command:
gcloud
init
If you're using an external identity provider (IdP), you must first
sign in to the gcloud CLI with your federated identity
.
Note:
You can run the gcloud CLI in
      the Google Cloud console without installing the Google Cloud CLI. To run the
      gcloud CLI in the Google Cloud console,
use
        Cloud Shell
.
Use guest attributes to report system health
You can use guest attributes to report the system health of the following
core services:
Docker service
Docker reverse proxy agent
Jupyter service
Jupyter API
Guest attributes
are a specific type of custom metadata that
applications can write to while running on
your user-managed notebooks
instance. To learn more about guest attributes,
see
About VM metadata
.
How instances use guest attributes to report system health
The
notebooks-collection-agent
service runs a Python process
in the background that verifies the status of
the user-managed notebooks
instance's core services and updates the guest attributes as either
1
if no problems are detected or
-1
if a failure is detected.
To use the
notebooks-collection-agent
service to
report on your user-managed notebooks instance's health,
you must enable the following guest attributes
while
creating
a user-managed notebooks instance:
enable-guest-attributes=TRUE
: This enables guest
attributes on your user-managed notebooks instance. All new
instances enable this attribute by default.
report-system-health=TRUE
: This records system
health check results to your guest attributes.
The
notebooks-collection-agent
service doesn't need
any special permissions to write to the instance's guest attributes.
Note:
The
notebooks-collection-agent
service uses
approximately 50 MB of memory to run in the background.
Create a user-managed notebooks instance with system health guest attributes enabled
To use system health guest attributes to report
on your user-managed notebooks instance's health,
you must select the
Enable system health report
checkbox when you create
a user-managed notebooks instance.
You can enable the system health report by using either the Google Cloud console
or the Google Cloud CLI.
Before you begin
Before you can create a user-managed notebooks instance,
you must have a
Google Cloud project and enable the Notebooks API
for that project.
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
If you plan to use GPUs with your
  user-managed notebooks instance,
check the quotas page in the
  Google Cloud console
to ensure that you have enough GPUs available in your project. If GPUs
  are not listed on the quotas page, or you require additional GPU quota,
  you can request a quota increase. See
Requesting an increase in
  quota
on the
  Compute Engine
Resource quotas
page.
Required roles
If you created the project, you have the
  Owner (
roles/owner
) IAM role on the project,
  which includes all required permissions. Skip this section and
  start creating your user-managed notebooks instance. If you didn't
  create the project yourself, continue in this section.
To get the permissions that
      you need to create a Vertex AI Workbench user-managed notebooks instance,
    
      ask your administrator to grant you the
    following IAM roles on the project:
Notebooks Admin (
roles/notebooks.admin
)
Service Account User (
roles/iam.serviceAccountUser
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create the instance
Console
In the Google Cloud console, go to the
User-managed notebooks
page.
Or go to
notebook.new
(https://notebook.new) and skip the next step.
Go to User-managed notebooks
Click
add_box
New notebook
,
and then select
Customize
.
On the
Create a user-managed notebook
page,
in the
Details
section,
provide the following information for your new instance:
Name
: a name for your new instance
Region
and
Zone
: Select a region and zone for
the new instance. For best network performance,
select the region that is geographically closest to you.
See the available
user-managed notebooks
locations
.
Select the
System health
section.
In the
System health and reporting
section, select the
Enable system health report
checkbox.
Complete the rest of the instance creation dialog,
and then click
Create
.
gcloud
From
Cloud Shell
or
any environment where the
Google Cloud CLI
is installed,
enter the following
Google Cloud CLI
command:
gcloud
notebooks
instances
create
INSTANCE_NAME
\
--
vm
-
image
-
project
=
deeplearning
-
platform
-
release
\
--
vm
-
image
-
family
=
IMAGE_FAMILY
\
--
machine
-
type
=
MACHINE_TYPE
\
--
location
=
ZONE
\
--
metadata
=
enable
-
guest
-
attributes
=
TRUE
,
report
-
system
-
health
=
TRUE
Replace the following:
INSTANCE_NAME
: the name of your new
instance
IMAGE_FAMILY
: the
image family
name
that you want to use to create your instance
MACHINE_TYPE
: the
machine
type
of your instance's VM;
for example,
n1-standard-4
ZONE
: the zone where
you want your new instance to be located,
for example,
us-west1-a
Access your instance from
the
Google Cloud console
.
Monitor system health through guest attributes
For user-managed notebooks instances that
have
the related guest attributes enabled
,
you can retrieve the values of your system health guest attributes
by using either the Google Cloud console, the Google Cloud CLI with Compute Engine
commands, or the Google Cloud CLI with Vertex AI Workbench commands.
Console
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Click the instance name that
you want to view the system health status of.
On the
Notebook details
page, click the
Health
tab. Review
the status of your instance and its core services.
gcloud with Compute Engine
gcloud compute instances get-guest-attributes
INSTANCE_NAME
\
    --zone
ZONE
Replace the following:
INSTANCE_NAME
: the name of your instance
ZONE
: the zone where your instance is located
If your core services are healthy, the results look like the following.
A value of
1
means no failure was detected.
NAMESPACE   KEY                         VALUE
 notebooks   docker_proxy_agent_status   1
 notebooks   docker_status               1
 notebooks   jupyterlab_api_status       1
 notebooks   jupyterlab_status           1
 notebooks   system-health               1
 notebooks   updated                     2020-10-01 17:00:00.12345
If any of the four core services fail, system-health reports a
-1
value to indicate system failure. In most cases,
a system failure means that JupyterLab is not accessible.
An example of a failure result might look like the following.
NAMESPACE   KEY                         VALUE
 notebooks   docker_proxy_agent_status   -1
 notebooks   docker_status               -1
 notebooks   jupyterlab_api_status       1
 notebooks   jupyterlab_status           1
 notebooks   system-health               -1
 notebooks   updated                     2020-10-01 17:00:00.12345
gcloud with Vertex AI Workbench
To monitor your system health, you can use the
getInstanceHealth
method to retrieve
the values of your guest attributes.
The following example shows how to do this using
the gcloud CLI.
gcloud notebooks instances is-healthy example-instance \
    --location=
ZONE
Replace
ZONE
with the zone where your instance
is located, for example,
us-west1-a
.
If your core services are healthy, the results look like the following.
A value of
1
means no failure was detected.
{
          "health_state": HEALTHY,
          "docker-proxy-agent": 1,
          "docker-service": 1,
          "jupyter-service": 1,
          "jupyter-api": 1,
          "last-updated": "2020-10-01 17:00:30.12345"
  }
An example of a failure result might look like the following.
{
          "healthy": UNHEALTHY,
          "docker-proxy-agent": 1,
          "docker-service": 1,
          "jupyter-service": -1,
          "jupyter-api": -1,
          "last-updated": "2020-10-01 17:00:30.12345"
  }
Report custom metrics to Monitoring
User-managed notebooks instances let you
collect system status and JupyterLab metrics
and report them to Cloud Monitoring. These custom metrics
are different from the standard metrics that are reported when
you
install Monitoring on
your user-managed notebooks instance
.
The custom metrics reported to Monitoring include the following:
The system health of these user-managed notebooks core services:
Docker service
Docker reverse proxy agent
Jupyter service
Jupyter API
The following JupyterLab metrics:
Number of kernels
Number of terminals
Number of connections
Number of sessions
Maximum memory
High memory
Current memory
How instances report custom metrics to Monitoring
To report custom metrics to Monitoring, you must enable
the
report-notebook-metrics
metadata setting
while
creating
a user-managed notebooks instance.
You must also make sure that the user-managed notebooks
instance's service account
has Monitoring Metric Writer (
roles/monitoring.metricWriter
)
permissions. For more information, see
Manage access to projects, folders, and organizations
.
Create a user-managed notebooks instance that reports custom metrics to Monitoring
To report custom metrics to Monitoring, you must select
the
Report custom metrics to Cloud Monitoring
checkbox
when you create a user-managed notebooks instance.
You can enable reporting custom metrics to Cloud Monitoring by using either
the Google Cloud console or the Google Cloud CLI.
Before you begin
Before you can create a user-managed notebooks instance,
you must have a
Google Cloud project and enable the Notebooks API
for that project.
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
If you plan to use GPUs with your
  user-managed notebooks instance,
check the quotas page in the
  Google Cloud console
to ensure that you have enough GPUs available in your project. If GPUs
  are not listed on the quotas page, or you require additional GPU quota,
  you can request a quota increase. See
Requesting an increase in
  quota
on the
  Compute Engine
Resource quotas
page.
Required roles
If you created the project, you have the
  Owner (
roles/owner
) IAM role on the project,
  which includes all required permissions. Skip this section and
  start creating your user-managed notebooks instance. If you didn't
  create the project yourself, continue in this section.
To get the permissions that
      you need to create a Vertex AI Workbench user-managed notebooks instance,
    
      ask your administrator to grant you the
    following IAM roles on the project:
Notebooks Admin (
roles/notebooks.admin
)
Service Account User (
roles/iam.serviceAccountUser
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create the instance
Console
In the Google Cloud console, go to the
User-managed notebooks
page.
Or go to
notebook.new
(https://notebook.new) and skip the next step.
Go to User-managed notebooks
Click
add_box
New notebook
,
and then select
Customize
.
On the
Create a user-managed notebook
page,
in the
Details
section,
provide the following information for your new instance:
Name
: a name for your new instance
Region
and
Zone
: Select a region and zone for
the new instance. For best network performance,
select the region that is geographically closest to you.
See the available
user-managed notebooks
locations
.
Select the
System health
section.
In the
System health and reporting
section, select the
Report custom metrics to Cloud Monitoring
checkbox.
Complete the rest of the instance creation dialog,
and then click
Create
.
gcloud
From
Cloud Shell
or
any environment where the
Google Cloud CLI
is installed,
enter the following
Google Cloud CLI
command:
gcloud
notebooks
instances
create
INSTANCE_NAME
\
--
vm
-
image
-
project
=
deeplearning
-
platform
-
release
\
--
vm
-
image
-
family
=
IMAGE_FAMILY
\
--
machine
-
type
=
MACHINE_TYPE
\
--
location
=
ZONE
\
--
metadata
=
report
-
notebook
-
metrics
=
TRUE
Replace the following:
INSTANCE_NAME
: the name of your new
instance
IMAGE_FAMILY
: the
image family
name
that you want to use to create your instance
MACHINE_TYPE
: the
machine
type
of your instance's VM,
for example,
n1-standard-4
ZONE
: the zone where
you want your new instance to be located,
for example,
us-west1-a
Access your instance from
the
Google Cloud console
.
Grant Monitoring Metric Writer permissions to the service account
After you've created
your new user-managed notebooks instance,
grant Monitoring Metric Writer permissions
(
roles/monitoring.metricWriter
) to
the service account for
the user-managed notebooks instance.
For more information, see
Manage access to projects, folders, and organizations
.
Monitor custom metrics through Monitoring
For user-managed notebooks instances that
have
reporting custom metrics enabled
,
you can monitor your custom metrics
by using the Google Cloud console.
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Click the instance name that you want to view the custom metrics of.
On the
Notebook details
page, click the
Monitoring
tab. Review
the custom metrics for your instance.
Install Monitoring on an instance
This option automatically installs Monitoring.
The installation requires 256 MB of disk space. An internet
connection is required for the metrics to be reported to
Monitoring.
How instances report system and application metrics
To report system and application metrics by installing
Cloud Monitoring on your user-managed notebooks instance,
you must select the
Install Cloud Monitoring agent
checkbox when you create
a user-managed notebooks instance.
These metrics are different from the custom metrics that are reported when
you
enable the
report-notebook-metrics
metadata
setting
.
Create a user-managed notebooks instance that reports system and application metrics to Monitoring
To install Monitoring on your
user-managed notebooks instance, you can use either
the Google Cloud console or the Google Cloud CLI.
Before you begin
Before you can create a user-managed notebooks instance,
you must have a
Google Cloud project and enable the Notebooks API
for that project.
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
If you plan to use GPUs with your
  user-managed notebooks instance,
check the quotas page in the
  Google Cloud console
to ensure that you have enough GPUs available in your project. If GPUs
  are not listed on the quotas page, or you require additional GPU quota,
  you can request a quota increase. See
Requesting an increase in
  quota
on the
  Compute Engine
Resource quotas
page.
Required roles
If you created the project, you have the
  Owner (
roles/owner
) IAM role on the project,
  which includes all required permissions. Skip this section and
  start creating your user-managed notebooks instance. If you didn't
  create the project yourself, continue in this section.
To get the permissions that
      you need to create a Vertex AI Workbench user-managed notebooks instance,
    
      ask your administrator to grant you the
    following IAM roles on the project:
Notebooks Admin (
roles/notebooks.admin
)
Service Account User (
roles/iam.serviceAccountUser
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create the instance
Console
In the Google Cloud console, go to the
User-managed notebooks
page.
Or go to
notebook.new
(https://notebook.new) and skip the next step.
Go to User-managed notebooks
Click
add_box
New notebook
,
and then select
Customize
.
On the
Create a user-managed notebook
page,
in the
Details
section,
provide the following information for your new instance:
Name
: a name for your new instance
Region
and
Zone
: Select a region and zone for
the new instance. For best network performance,
select the region that is geographically closest to you.
See the available
user-managed notebooks
locations
.
Select the
System health
section.
In the
System health and reporting
section, select the
Install Cloud Monitoring agent
checkbox.
Complete the rest of the instance creation dialog,
and then click
Create
.
gcloud
From
Cloud Shell
or
any environment where the
Google Cloud CLI
is installed,
enter the following
Google Cloud CLI
command:
gcloud
notebooks
instances
create
INSTANCE_NAME
\
--
vm
-
image
-
project
=
deeplearning
-
platform
-
release
\
--
vm
-
image
-
family
=
IMAGE_FAMILY
\
--
machine
-
type
=
MACHINE_TYPE
\
--
location
=
ZONE
\
--
metadata
=
install
-
monitoring
-
agent
=
TRUE
Replace the following:
INSTANCE_NAME
: the name of your new
instance
IMAGE_FAMILY
: the
image family
name
that you want to use to create your instance
MACHINE_TYPE
: the
machine
type
of your instance's VM;
for example,
n1-standard-4
ZONE
: the zone where
you want your new instance to be located,
for example,
us-west1-a
Access your instance from
the
Google Cloud console
.
Monitor system and application metrics through Monitoring
For user-managed notebooks instances that
have
Monitoring installed
,
you can monitor your system and application metrics
by using the Google Cloud console:
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Click the instance name that
you want to view the system and application metrics of.
On the
Notebook details
page, click the
Monitoring
tab. Review
the system and application metrics for your instance. To learn how to
interpret these metrics, see
Review resource
metrics
.
Use the diagnostic tool to monitor system health
User-managed notebooks instances include
a built-in diagnostic tool
that can help you monitor the system health
of your instances.
Tasks performed by the diagnostic tool
The diagnostic tool performs the following tasks:
Verifies the status of the following user-managed notebooks
core services:
Docker service
Docker reverse proxy agent
Jupyter service
Jupyter API
Checks whether the disk space for boot and data disks is
used beyond an 85% threshold.
Installs
lsof
(internet connection required).
Collects the following instance logs:
Network information (
ifconfig
,
netstat
)
Logs in the
/var/log/
folder
Docker status information
lsof
(open files) data
Docker service status
Proxy reverse agent status
Jupyter service status
Jupyter API status
Proxy agent configuration file
Python processes
Runs the following commands and collects the results:
pip freeze
conda list
gcloud compute instances describe
INSTANCE_NAME
gcloud config list
Run the diagnostic tool
To run the diagnostic tool, complete the following steps:
Use ssh to connect to your user-managed notebooks
instance
.
In the SSH terminal, run the following commands:
sudo
-
i
cd
/
opt
/
deeplearning
/
bin
/
./
diagnostic_tool
.
sh
The diagnostic tool collects the logs,
compresses them in a
.tar.gz
file,
and places the file in the
/tmp/
folder.
Extract the file and then evaluate the contents.
The contents include:
log
folder
: Logs from
the
var/log/
folder
report.log
: Output for
all commands collected
proxy-agent-config.json
:
Proxy configuration information
Docker log
: A
-json.log
file
that includes Docker container logs
You can use the following options with the diagnostic tool.
Option
Description
-r
A repair option that tries to restore
        failed user-managed notebooks core services status
-s
Runs without a confirmation
-b
Uploads the
.tar.gz
file
        to a Cloud Storage bucket.
-v
A debug option for troubleshooting the tool in case of failures
-c
Captures 30 seconds of packet traffic into
        your user-managed notebooks instance, filtering SSH
-d
A destination folder in which to save the logs
-h
Help
What's next
Learn more about VM metadata
.
Learn more about
Monitoring
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/registering-legacy-notebooks.txt
Register a legacy Vertex AI Workbench user-managed notebooks instance with Notebooks API  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Register a legacy instance with Notebooks API
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
This page shows how to migrate and register a legacy
user-managed notebooks instance with the Notebooks API. Instances
created by using the Compute Engine API are called legacy instances. Legacy
instances don't have the latest updates to functionality and features.
To use the new functionality and features that are available with the
Notebooks API, you must
register
your legacy instances with
the Notebooks API. Before registering your legacy instances, check whether
they meet the
requirements
. You can
migrate
instances that don't meet the requirements.
If you enable the Notebooks API before registering your legacy instances, the
Notebooks API activation process attempts to register your existing legacy
instances automatically.
Requirements and limitations
Before registering your legacy instances with the Notebooks API, consider
the following requirements and limitations.
Source and destination zones must match and be a valid zone
for the Notebooks API.
For example, a legacy user-managed notebooks instance
in
us-west1-a
remains in
us-west1-a
when it is registered with
the Notebooks API. However, a legacy user-managed notebooks
instance in
us-central1-f
won't register with the Notebooks API because
us-central1-f
isn't a valid zone for the Notebooks API.
To get a list of the valid zones for the Notebooks API notebooks instances,
run the following command by using the
Google Cloud CLI
in
your preferred terminal or in
Cloud Shell
:
gcloud
notebooks
locations
list
If your legacy instance's zone is not a valid zone for the Notebooks API,
you can contact
support
or your account manager, or you can
migrate the legacy instance to a
new user-managed notebooks
instance
.
Only dual-disk instances can use all user-managed notebooks
features.
Single-disk legacy instances cannot use some
user-managed notebooks features, such as auto upgrade, even
after they are registered with the Notebooks API. To enable your
single-disk legacy instance to use all available features, you must
migrate your single-disk instance to a dual-disk
instance
.
You can migrate your single-disk instance to a dual-disk instance as part
of your migration from a legacy instance to an instance registered with the
Notebooks API. If you've already registered the legacy instance with the
Notebooks API, you can still migrate the instance to a
new
dual-disk
instance to resolve the issue.
To verify the number of disks, complete the following steps.
Console
In the Google Cloud console, go to the
VM instances
page.
Go to VM instances
Find your current legacy user-managed notebooks
instance.
Click the instance name to open the
VM instance details
page.
In the
boot disk
and
additional disks
sections, verify
how many disks are attached to the VM.
gcloud
In
Cloud Shell
or any environment where the
Google Cloud CLI
is installed, enter the following
Google Cloud CLI
command:
gcloud
compute
instances
describe
MY_INSTANCE_NAME
\
--zone
=
MY_ZONE
Replace the following:
MY_INSTANCE_NAME
: the name of your instance
MY_ZONE
: the zone of your instance
Review the information that follows
disks:
, and verify
how many disks are attached to the VM.
Migrate a legacy instance to a new user-managed notebooks instance
If your legacy user-managed notebooks instance is in a zone that
isn't a valid zone for the Notebooks API, or if you want to migrate from a
single-disk instance to a dual-disk instance, you must create a user-managed notebooks instance and copy your user data from your
legacy instance to the new instance.
To create a user-managed notebooks instance and copy your user
data from your legacy instance to the new instance, complete the following
steps:
To use
ssh
to connect to your legacy instance, in
Cloud Shell
or any environment where the
Google Cloud CLI
is installed, enter the following
Google Cloud CLI
command:
export
PROJECT_ID
=
"
MY_PROJECT_ID
"
export
ZONE
=
"
MY_ZONE
"
export
INSTANCE_NAME
=
"
MY_INSTANCE
"
gcloud
compute
ssh
\
--
project
$
PROJECT_ID
\
--
zone
$
ZONE
$
INSTANCE_NAME
\
--
-
L
8080
:
localhost
:
8080
Replace the following:
MY_PROJECT_ID
: the ID of your Google Cloud project
MY_ZONE
: the zone of your instance
MY_INSTANCE
: the name of your instance
with the relevant information.
To copy the contents of the legacy instance to a
Cloud Storage
bucket, use
gcloud storage
. The following example command copies all of
the notebook (
.ipynb
) files from the default directory
/home/jupyter/
to a Cloud Storage directory named
my-bucket/legacy-notebooks
.
gcloud
storage
cp
/home/jupyter/*.ipynb
gs://my-bucket/legacy-notebooks/
--recursive
Create a user-managed notebooks instance
with the same hardware
specifications as the legacy instance.
In this example, Vertex AI Workbench creates
a user-managed notebooks instance named
new-notebook
in the
example
project
using the latest TensorFlow 2 image, with an
n1-standard-1
machine type, in the
us-west1-a
zone.
In either
Cloud Shell
or any environment where the
Google Cloud CLI
is installed, enter the following
Google Cloud CLI
command:
gcloud
notebooks
instances
create
new-notebook
\
--vm-image-project
=
example
\
--vm-image-family
=
tf-latest-cpu
\
--machine-type
=
n1-standard-1
\
--location
=
us-west1-a
The new user-managed notebooks instance is dual-disk.
It has a boot disk and a data disk.
Use
ssh
to connect to the user-managed notebooks instance
that you just created.
To copy the contents of the legacy instance from the
Cloud Storage bucket to the new instance, use
gcloud storage
.
The following example command copies all of
the notebook (
.ipynb
) files from the Cloud Storage directory
to the new instance's
/home/jupyter/
directory.
gcloud
storage
cp
gs://my-bucket/legacy-notebooks/*.ipynb
/home/jupyter/
--recursive
In the new user-managed notebooks instance,
open JupyterLab and confirm
that the user data and assets have been successfully copied.
Optional: delete the legacy instance.
Register a legacy instance with the Notebooks API
To use the Notebooks API to manage your legacy instances, you must register
your legacy instances with the Notebooks API.
If you enable the Notebooks API before registering your legacy instances,
the Notebooks API activation process attempts to register your existing legacy
instances automatically. If you create legacy instances after the Notebooks API
is enabled, you must register them manually by using one of the following
methods.
To register your legacy instance with the Notebooks API, you can
use the Google Cloud console or the Notebooks API.
Console
To use the
Register all
option in the Google Cloud console,
complete the following steps:
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
If you have one or more legacy
user-managed notebooks instances,
a message appears indicating that you need to register them
with the Notebooks API. Next to this message, click
Register all
.
If you do not have any legacy user-managed notebooks
instances, but have not yet enabled the Notebooks API, click
Enable
Notebooks API
to ensure that new user-managed notebooks
instances are created by using the Notebooks API.
Notebooks API
To use the
register
method provided by the Notebooks API, in
Cloud Shell
or any environment where the
Google Cloud CLI
is installed, enter the following
Google Cloud CLI
command:
gcloud
notebooks
instances
register
MY_INSTANCE_NAME
\
--location
=
MY_ZONE
Replace the following:
MY_INSTANCE_NAME
: the name of your instance
MY_ZONE
: the zone of your instance
What's next
Create a user-managed notebooks instance by using a custom container
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/save-to-github.txt
Save a notebook to GitHub  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Save a notebook to GitHub
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
To back up your user-managed notebooks instance and make it available
to others, save the contents of your user-managed notebooks instance
to GitHub.
Create a GitHub repository
If you don't already have a
GitHub
repository, you must create one.
When you create your GitHub repository make sure that your GitHub repository
can be cloned by selecting the
Initialize this repository with a README
checkbox.
Clone your GitHub repository in your user-managed notebooks instance
To clone your GitHub repository in your user-managed notebooks
instance, complete the following steps:
In your GitHub repository, click the
Code
button,
and then click the
Local
tab.
Copy the
HTTPS
URL.
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Click
Open JupyterLab
to open
your user-managed notebooks instance.
In the JupyterLab
folder
File Browser
, select
the folder where you want to clone the GitHub repository. For example,
the home folder.
In JupyterLab, select
Git
>
Clone a Repository
.
If prompted, enter your credentials.
If you use a GitHub username and password, enter your
GitHub username and password.
If you use two-factor authentication with GitHub,
create and use a
personal access token
.
In the
Clone a repo
dialog, paste the HTTPS URL for your GitHub repository.
Click
Clone
.
Configure your user-managed notebooks instance with your GitHub user information
In JupyterLab, select
Git
>
Open Git Repository in Terminal
to open a Git terminal window.
In the Git terminal window, enter the following commands to configure
your Git username and email:
git config --global user.name "
YOUR_NAME
"
git config --global user.email "
YOUR_EMAIL
"
If your GitHub account requires SSH authentication, complete
the following steps to connect your account:
From your Git terminal in your user-managed notebooks
instance, follow GitHub's
instructions for generating a new SSH key
.
Follow the
instructions for adding that SSH key to your GitHub
account
.
Close the Git terminal window.
Add your committed files to your GitHub repository
Your user-managed notebooks instance shows your repository
as a new folder. If you don't
see your cloned GitHub repository as a folder, click the
Refresh File
List
button.
Double-click your repository folder to open it.
Add a new notebook to your user-managed notebooks instance.
To add a notebook file, you can use the menu or the Launcher.
Menu
To add a new notebook file from the menu, select
File
>
New
>
Notebook
.
In the
Select kernel
dialog, select the kernel for your new
notebook, for example,
Python 3
, and then click
Select
.
Your new notebook file opens.
Launcher
To add a new notebook file from the Launcher, select
File
>
New
>
Launcher
.
Click the tile for the kernel you want to use.
Your new notebook file opens.
Rename your new notebook file.
Menu
Select
File
>
Rename notebook
. The
Rename file
dialog opens.
In the
New name
field, change
Untitled.ipynb
to something
meaningful, such as
install.ipynb
.
Click
Rename
.
Launcher
Right-click the
Untitled.ipynb
tab and then click
Rename notebook
. The
Rename file
dialog opens.
In the
New name
field, change
Untitled.ipynb
to something
meaningful, such as
install.ipynb
.
Click
Rename
.
Select the
Git
tab. Your new notebook is listed in the
Untracked
grouping.
To add the new notebook as a file for your GitHub repository, right-click
the new notebook and select
Track
. On the
Git
tab, your notebook
is now added to the
Staged
grouping.
To commit your new notebook to your GitHub repository, on the
Git
tab,
add a commit comment and click
Commit
.
To open a Git terminal window, select
Git
>
Open Git repository in terminal
.
In the Git terminal window, enter the
git push
command.
If you use a GitHub username and password, when prompted, enter your
GitHub username and password.
If you use two-factor authentication with GitHub,
create a personal access token
to use.
When the
git push
command completes, your committed files are in
your GitHub repository.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/service-perimeter.txt
Use a Vertex AI Workbench user-managed notebooks instance within a service perimeter  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Use a user-managed notebooks instance within a service perimeter
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
This page describes how to use VPC Service Controls to set up
a user-managed notebooks instance within a service perimeter.
Before you begin
Read the
Overview of
VPC Service Controls
.
Create a service perimeter using
VPC Service Controls
.
This service perimeter protects the Google-managed resources of services
that you specify. While creating your service perimeter, do the following:
When it's time to add projects to your service perimeter, add the
project that contains your user-managed notebooks instance.
When it's time to add services to your service perimeter, add the
Notebooks API
.
If you have created your service perimeter without adding the
projects and services you need, see
Managing service
perimeters
to learn how to update your service perimeter.
Configure your DNS entries using Cloud DNS
Vertex AI Workbench user-managed notebooks instances use several domains that a
  Virtual Private Cloud network doesn't handle by default.
  To ensure that your VPC network correctly handles requests sent
  to those domains, use Cloud DNS to add DNS records. For more
  information about VPC routes, see
Routes
.
To create a
managed zone
for
  a domain, add a DNS entry that will route the request, and execute
  the transaction, complete the following steps.
  Repeat these steps for each of
several
  domains
that you need to handle requests for, starting
  with
*.notebooks.googleapis.com
.
In
Cloud Shell
or any environment where the
Google Cloud CLI
is installed, enter the following
Google Cloud CLI
commands.
To create a private managed zone
      for one of the domains that your
      VPC network needs to handle:
gcloud
dns
managed-zones
create
ZONE_NAME
\
--visibility
=
private
\
--networks
=
https://www.googleapis.com/compute/v1/projects/
PROJECT_ID
/global/networks/
NETWORK_NAME
\
--dns-name
=
DNS_NAME
\
--description
=
"Description of your managed zone"
Replace the following:
ZONE_NAME
: a name for the zone to create.
        You must use a separate zone for each domain. This zone name is used in
        each of the following steps.
PROJECT_ID
: the ID of the project that hosts your
        VPC network
NETWORK_NAME
: the name of the VPC
        network that you created earlier
DNS_NAME
: the part of the domain name that comes
        after the
*.
, with a period on the end.
        For example,
*.notebooks.googleapis.com
has a
DNS_NAME
of
notebooks.googleapis.com.
Start a transaction.
gcloud
dns
record-sets
transaction
start
--zone
=
ZONE_NAME
Add the following DNS A record. This reroutes traffic to
      Google's restricted IP addresses.
gcloud
dns
record-sets
transaction
add
\
--name
=
DNS_NAME
.
\
--type
=
A
199
.36.153.4
199
.36.153.5
199
.36.153.6
199
.36.153.7
\
--zone
=
ZONE_NAME
\
--ttl
=
300
Add the following DNS CNAME record to point to the A record
      that you just added. This redirects all traffic matching the
      domain to the IP addresses listed in the previous step.
gcloud
dns
record-sets
transaction
add
\
--name
=
\*
.
DNS_NAME
.
\
--type
=
CNAME
DNS_NAME
.
\
--zone
=
ZONE_NAME
\
--ttl
=
300
Execute the transaction.
gcloud
dns
record-sets
transaction
execute
--zone
=
ZONE_NAME
Repeat these steps for each of the following domains. For each
      repetition, change
ZONE_NAME
and
DNS_NAME
to the appropriate values for that
      domain. Keep
PROJECT_ID
and
NETWORK_NAME
the same each time. You already
      completed these steps for
*.notebooks.googleapis.com
.
*.notebooks.googleapis.com
*.notebooks.cloud.google.com
*.notebooks.googleusercontent.com
*.googleapis.com
to run code that interacts with other Google APIs
        and services
Configure the service perimeter
After
configuring the DNS records
, either
create a service
perimeter
or
update an existing
perimeter
to add your project to the service perimeter.
In the VPC network, add a route for the
199.36.153.4/30
range with a
next hop of
Default internet gateway
.
Note:
The
199.36.153.4/30
range is for
restricted.googleapis.com
to
      access APIs that are only VPC Service Controls compatible.
      If you aren't using VPC Service Controls, you can use the
199.36.153.8/30
range for
private.googleapis.com
. For more
      information about Private Google Access, see
Configure
      Private Google Access
.
Use Artifact Registry within your service perimeter
If you want to use Artifact Registry in your service perimeter,
see
Configure restricted access for GKE
private clusters
.
Use Shared VPC
If you are using
Shared VPC
,
you must add the host and the service projects to the service
perimeter. In the host project, you must also grant the
Compute Network User role
(
roles/compute.networkUser
) to the
Notebooks Service
Agent
from the service project. For more information, see
Managing
service perimeters
.
Limitations
Identity type for ingress and egress policies
When you specify an ingress or egress policy for a service perimeter,
you can't use
ANY_SERVICE_ACCOUNT
or
ANY_USER_ACCOUNT
as an identity type for
all
Vertex AI Workbench
operations.
Instead, use
ANY_IDENTITY
as the identity type.
Accessing the user-managed notebooks proxy from a workstation without internet
To access user-managed notebooks instances
from a workstation with limited internet access,
verify with your IT administrator that you can access the following domains:
*.accounts.google.com
*.accounts.youtube.com
*.googleusercontent.com
*.kernels.googleusercontent.com
*.gstatic.com
*.notebooks.cloud.google.com
*.notebooks.googleapis.com
You must have access to these domains for authentication to
Google Cloud. See the previous section,
Configure your DNS entries using Cloud DNS
,
for further configuration information.
What's next
Install dependencies
on your new
user-managed notebooks instance.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/shielded-vm.txt
Use a shielded virtual machine with Vertex AI Workbench user-managed notebooks  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Use a shielded virtual machine with user-managed notebooks
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
So you can be confident that your instances have not been compromised by
boot- or kernel-level malware or rootkits,
Shielded VM offers verifiable integrity of Compute Engine VM
instances.
Shielded VM's verifiable integrity is achieved through the
use of
Secure Boot
,
virtual trusted platform module
(vTPM)
-enabled
Measured
Boot
, and
integrity
monitoring
.
For more information, see
Shielded VM
.
Requirements and limitations
To use Shielded VM with user-managed notebooks,
you must create
a Deep Learning VM Images with a Debian 10 OS that
is
version
M51
or higher.
While using Vertex AI Workbench, you can't use
shielded VM user-managed notebooks instances
that use GPU accelerators.
Create a user-managed notebooks instance using a shielded VM
To create a shielded VM that you can use with
user-managed notebooks, complete the following steps:
Select the image family
that you want your instance to be based
on. Use the following
Google Cloud CLI
command to
list the available image families that are compatible
with user-managed notebooks
and Shielded VM. You can run the command in
Cloud Shell
or any environment where the
Google Cloud CLI
is installed.
gcloud
compute
images
list
\
--project
deeplearning-platform-release
\
--no-standard-images
|
grep
debian-10
Use the following command to create the Compute Engine instance.
gcloud
compute
instances
create
nb-legacy2
\
--image-project
=
deeplearning-platform-release
\
--image-family
=
MY_IMAGE_FAMILY
\
--metadata
=
"proxy-mode=service_account"
\
--scopes
=
https://www.googleapis.com/auth/cloud-platform
\
--shielded-secure-boot
\
--zone
=
MY_ZONE
Replace the following:
MY_IMAGE_FAMILY
: the image family name that you
want to use to create your VM
MY_ZONE
: the zone where you want your instance
to be located
Register your Compute Engine VM with the
Notebooks API
.
What's next
Learn more about
user-managed notebooks image
families
.
Learn more about
modifying Shielded VM
options
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/shut-down.txt
Shut down a Vertex AI Workbench user-managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Shut down a user-managed notebooks instance
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
To stop any processing for a user-managed notebooks instance,
you can shut down the instance.
You incur costs for running an instance, in addition to other potential costs
(see
Pricing
).
To stop incurring costs for running an
instance, shut down the instance. You continue to incur other potential costs.
To shut down a user-managed notebooks instance,
complete the following steps:
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Select the instance that you want to open.
Click
Open JupyterLab
.
It is
important to stop all running processes in case there are operations that
need to complete before you shut down
your user-managed notebooks instance, for example,
I/O processes that are writing to disk.
To stop all running processes:
To show all of the processes that are running, select the
Running
Terminals and Kernels
tab.
Next to each running process, click
Shutdown all
.
Close the browser tab or window for
your user-managed notebooks instance.
In the Google Cloud console, return to the
User-managed notebooks
page.
Go to User-managed notebooks
Select the instance that you want to shut down, and then
click
square
Stop
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/ssh-access.txt
Use SSH to access JupyterLab  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Use SSH to access JupyterLab
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
Whenever you don't have HTTPS access to your JupyterLab instance,
you must use SSH to establish a connection.
To set up
SSH port forwarding
,
complete the following steps, and then access your JupyterLab session through a
local browser:
Run the following command by using the
Google Cloud CLI
in
your preferred terminal or in
Cloud Shell
:
gcloud
compute
ssh
\
--project
PROJECT_ID
\
--zone
ZONE
\
INSTANCE_NAME
\
--
-L
8080
:localhost:8080
Replace the following:
PROJECT_ID
: your
Google Cloud project ID
ZONE
: the
zone
where your instance is located
INSTANCE_NAME
: the name of your
instance
Note:
If using Cloud Shell to run the command, add a
-4
to the
      SSH flags to use IPv4 to connect. Example:
-- -4 -L
LOCAL_PORT
:localhost:
REMOTE_PORT
Access your JupyterLab session through a local browser:
If you ran the command on your local machine, visit
https://localhost:8080
to access JupyterLab.
If you ran the command using
Cloud Shell
,
access JupyterLab through the
Web
Preview on port 8080.
Reasons why you might not have HTTPS access
To get HTTPS access to JupyterLab, your user-managed notebooks
instance must have access to a Google Cloud proxy service.
When the instance starts, it attempts to register itself with
the proxy service. If it fails to get proxy access,
your user-managed notebooks instance
prompts you to access JupyterLab through SSH.
The following are common reasons why you might not have HTTPS access to
JupyterLab:
Your JupyterLab instance's proxy-mode metadata setting
is incorrect.
Your network is configured to block internet access for the
virtual machines (VMs) running JupyterLab notebooks.
Your user-managed notebooks instance
doesn't have an external IP address.
Your
VPC Service Controls
settings
block access to
Artifact Registry
.
The following sections show how to resolve these issues.
For changes to take effect, you might need to restart the notebook's VM when
attempting to resolve these issues.
Your JupyterLab instance's proxy-mode metadata setting is incorrect
By default, when you use user-managed notebooks to create
a JupyterLab instance, Vertex AI Workbench adds
the proxy-mode metadata setting.
If you change or remove the proxy-mode metadata setting, then
the user-managed notebooks instance
can't connect to the proxy service.
To make sure your proxy-mode metadata setting is valid, complete
the following steps:
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Select the instance that you need to modify.
Next to
View VM details
, click
View in Compute Engine
.
On the VM details page, click
Edit
.
In the
Metadata
section, add or modify
the metadata to ensure there is a
proxy-mode entry
set
to the correct value, for example:
project_editors`.
Learn more about the possible values of the
proxy-mode
metadata
entry
.
Click
Save
.
The network is blocking internet access
Your JupyterLab instance accesses the proxy service through a public URL.
If your Virtual Private Cloud network settings block access to the public internet
or your firewall rules block egress traffic, you must use SSH to access
your user-managed notebooks instance.
If possible, you might want to work
with your network and firewall administrators to allow access to your
user-managed notebooks instance through the public internet.
Your user-managed notebooks instance doesn't have an external IP address
You might have created your user-managed notebooks instance
without an
external IP address. If you need to change this, complete the following
steps.
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Click the name of the instance that you need to modify.
Click
View VM details
.
Click
Edit
.
In the
Network interfaces
section, expand the network that
you want to have an external IP address.
Click the
External IP address
drop-down menu,
and select the option that you want.
To resolve this issue, you must not choose
None
.
In the
Network interfaces
section, click
Done
.
Click
Save
.
VPC Service Controls settings are blocking access to Artifact Registry
To connect to the proxy service,
your user-managed notebooks instance runs an
agent that it downloads from Artifact Registry. Without this agent
your instance cannot connect to the proxy service.
If your VPC Service Controls settings are blocking access to
Artifact Registry, you must add the Artifact Registry
service to the service perimeter of your VPC Service Controls.
Learn more about how service perimeters
work and what services VPC Service Controls can be used
to secure
.
Further troubleshooting
If you are still having trouble connecting, try reviewing the console
logs for your virtual machine. These logs might help you discover why
the user-managed notebooks instance is unable
to register with the proxy service.
To access these logs, complete the following steps:
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Select the instance that you want to troubleshoot.
In
Logs
, click
Serial port 1 (console)
.
What's next
For tips on resolving other issues,
see the
troubleshooting section on user-managed
notebooks
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/upgrade.txt
Upgrade the environment of a Vertex AI Workbench user-managed notebooks instance  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Upgrade the environment of a user-managed notebooks instance
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
Vertex AI Workbench user-managed notebooks instances are
Deep Learning VM Images
instances
with JupyterLab notebook environments enabled and ready for use.
This page describes how to upgrade the environment of
a user-managed notebooks instance.
Reasons to upgrade
You might want to upgrade the environment of
your user-managed notebooks instance
for any of the following reasons:
To use new capabilities that are only available in a newer version
of your environment.
To benefit from framework updates, package updates, and bug fixes
that have been implemented in a newer version of your environment.
Upgrade methods
There are two ways to upgrade a user-managed notebooks instance:
Automatic upgrade
: Enable auto upgrade when you create
a user-managed notebooks instance. During a recurring
time period that you specify, Vertex AI Workbench checks whether
your instance can be upgraded, and if so,
Vertex AI Workbench upgrades your instance.
Manual upgrade
: If
an existing user-managed notebooks instance
meets the
requirements
for upgrading, you can upgrade the instance manually.
Requirements and limitations
Backward compatibility with your user-managed notebooks isn't
guaranteed.
Make a copy of
your data
before upgrading a user-managed notebooks instance.
To determine whether you can upgrade
a specific user-managed notebooks instance,
see the following requirements and limitations:
The Notebooks API must be
enabled in the instance's
Google Cloud project
.
For more information, see
List enabled services
and
Enable an API
.
The user-managed notebooks instance's
environment version
must be M54 or higher.
The user-managed notebooks instance must have been created using
the Notebooks API or must have been
registered with the
Notebooks API
.
If your user-managed notebooks instance is container-based,
Vertex AI Workbench upgrades the OS. The image version depends on
the specific image pulled by your Dockerfile.
To help make sure the upgrade uses the most recent version
of the image, consider using the
latest
tag in
your Dockerfile.
If upgrading your instance is not an option for you,
consider
migrating your data to
a new user-managed notebooks instance
.
How the upgrade works
User-managed notebooks instances that can be upgraded
are dual-disk, with one boot disk and one data disk.
The upgrade process upgrades the boot disk
to a new image while preserving your data on the data disk.
Which components are upgraded or preserved?
The following table shows which components of
your user-managed notebooks
instance are upgraded and which are preserved.
Component
Upgrade result
Machine learning frameworks
Upgraded
Machine learning data
Preserved
Preinstalled dependencies
Upgraded
User-installed libraries
By default, must be reinstalled
        (see
User-installed libraries
)
Local files in the
/home/jupyter
directory
Preserved
Local files in any other
/home/
directory
Not preserved
Preinstalled operating system packages
Upgraded
User-installed operating system packages
Not preserved
GPU drivers
Upgraded
Notebooks
Preserved
User configurations
Preserved
User-installed libraries
By default, user-managed notebooks instances store
pip and Conda libraries in the boot disk, which is replaced during an upgrade.
When you install pip libraries, you can include the
--user
flag to
install them in the
/home/jupyter/
directory,
where they are preserved during an upgrade.
By default, if you install pip or Conda libraries in a kernel created from a
custom container, the libraries only persist while the kernel is running.
Each time the kernel is restarted, those libraries will need to be
reinstalled. To install persistent libraries in a custom container,
include the library installations in your Dockerfile. When installing
pip libraries in a kernel created from a custom container, you can include
the
--user
flag so that the libraries will persist until instance restart.
Environment versions
Vertex AI Workbench updates environments regularly (
see the
Deep Learning VM release
notes
), but with each
released version, not all of
the
environments
are updated.
Vertex AI Workbench only upgrades an instance if there is a newer
environment version for the VM image that your instance is based on.
For information about how to use a specific version to create
a user-managed notebooks instance, see
Create a specific version of a user-managed notebooks
instance
.
Before you upgrade
Before you upgrade, complete the following steps.
Check the
release notes
to learn about
updates to newer versions.
Make a copy of
your data
as a backup.
Automatic upgrade
Vertex AI Workbench can automatically upgrade instances that are
running. If your instance is stopped, it doesn't automatically upgrade your
instance, even if you enabled auto upgrade when you created it.
When you enable automatic environment upgrades, you specify
a recurring time period in which Vertex AI Workbench checks
whether the instance can be upgraded, and if it can be, upgrades the instance.
The time period you specify is stored as a
notebook-upgrade-schedule
metadata entry, in
unix-cron
format
, Greenwich Mean Time (GMT).
To check whether an instance can be upgraded,
Vertex AI Workbench uses the API method
isUpgradeable
.
This method checks for a newer version of the image on the instance's
boot disk.
If the instance can be upgraded, Vertex AI Workbench uses an
internal upgrade method to upgrade the instance.
Create a user-managed notebooks instance with auto upgrade enabled
To create a user-managed notebooks instance with auto upgrade
enabled, select the
Enable environment auto-upgrade
checkbox and set a
schedule when you create the instance.
You can specify auto-upgrade by using either the Google Cloud console or the
Google Cloud CLI.
Before you begin
Before you can create a user-managed notebooks instance,
you must have a
Google Cloud project and enable the Notebooks API
for that project.
Sign in to your Google Cloud account. If you're new to
        Google Cloud,
create an account
to evaluate how our products perform in
        real-world scenarios. New customers also get $300 in free credits to
        run, test, and deploy workloads.
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
In the Google Cloud console, on the project selector page,
        select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Note
: If you don't plan to keep the
    resources that you create in this procedure, create a project instead of
    selecting an existing project. After you finish these steps, you can
    delete the project, removing all resources associated with the project.
Go to project selector
Verify that billing is enabled for your Google Cloud project
.
Enable the Notebooks API.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the API
If you plan to use GPUs with your
  user-managed notebooks instance,
check the quotas page in the
  Google Cloud console
to ensure that you have enough GPUs available in your project. If GPUs
  are not listed on the quotas page, or you require additional GPU quota,
  you can request a quota increase. See
Requesting an increase in
  quota
on the
  Compute Engine
Resource quotas
page.
Required roles
If you created the project, you have the
  Owner (
roles/owner
) IAM role on the project,
  which includes all required permissions. Skip this section and
  start creating your user-managed notebooks instance. If you didn't
  create the project yourself, continue in this section.
To get the permissions that
      you need to create a Vertex AI Workbench user-managed notebooks instance,
    
      ask your administrator to grant you the
    following IAM roles on the project:
Notebooks Admin (
roles/notebooks.admin
)
Service Account User (
roles/iam.serviceAccountUser
)
For more information about granting roles, see
Manage access to projects, folders, and organizations
.
You might also be able to get
        the required permissions through
custom
        roles
or other
predefined
        roles
.
Create the instance
Console
In the Google Cloud console, go to the
User-managed notebooks
page.
Or go to
notebook.new
(https://notebook.new) and skip the next step.
Go to User-managed notebooks
Click
add_box
New notebook
,
and then select
Customize
.
On the
Create a user-managed notebook
page,
in the
Details
section,
provide the following information for your new instance:
Name
: a name for your new instance
Region
and
Zone
: Select a region and zone for
the new instance. For best network performance,
select the region that is geographically closest to you.
See the available
user-managed notebooks
locations
.
In the
System health
section, select
Environment auto-upgrade
.
Choose whether to upgrade your notebook
Weekly
or
Monthly
.
In the
Weekday
field, select the option that you want.
In the
Hour
field, choose an hour of the day.
Complete the rest of the instance creation dialog,
and then click
Create
.
gcloud
From
Cloud Shell
or
any environment where the
Google Cloud CLI
is installed,
enter the following
Google Cloud CLI
command:
gcloud
notebooks
instances
create
INSTANCE_NAME
--
metadata
=
notebook
-
upgrade
-
schedule
=
SCHEDULE
--
vm
-
image
-
project
=
deeplearning
-
platform
-
release
--
vm
-
image
-
family
=
VM_IMAGE_FAMILY
--
machine
-
type
=
MACHINE_TYPE
--
location
=
LOCATION
Replace the following:
INSTANCE_NAME
: the name of your new
instance
SCHEDULE
: the weekly or monthly schedule
that you set, in
unix-cron
format
;
for example, "00 19 * * MON" means weekly on Monday, at 1900 hours
Greenwich Mean Time (GMT)
VM_IMAGE_FAMILY
: the
image family
name
that you want to use to create your instance
MACHINE_TYPE
: the
machine
type
of your instance's VM
LOCATION
: the Google Cloud location where
you want your new instance to be
Access your instance from
the
Google Cloud console
.
Edit the auto upgrade schedule
To edit the auto upgrade schedule after you have created your
user-managed notebooks instance, complete the following steps:
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Click the instance name that needs the schedule change.
On the
Notebook details
page,
in the
Environment auto-upgrade
section, edit the schedule.
Click
Submit
to save your changes.
Manual upgrade
You can manually upgrade user-managed notebooks instances
that meet the
requirements
.
Check for a newer version of your instance's environment
To check whether a newer version of your instance's environment is available,
access your instance from the Google Cloud console.
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
If your instance isn't running,
start the
instance
.
Vertex AI Workbench can only upgrade instances when they're running.
Click the instance name that
you want to check for availability of a newer environment version.
On the
Notebook details
page, next to
VM details
, click
View in Compute Engine
.
If a newer version of the environment is available, a "This instance needs
to be upgraded" message appears.
Upgrade your instance's environment to a newer version
You can manually upgrade a user-managed notebooks instance in
the Google Cloud console or by using the Google Cloud CLI.
Note:
Upgrades can affect user operations that are running.
Console
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
If your instance isn't running,
start the
instance
.
Vertex AI Workbench can only upgrade instances when
they're running.
Click the instance name that you want to upgrade.
On the
Notebook details
page, click
Upgrade
.
Make sure you have
made a copy of the data on your
instance
before
continuing.
After your data is backed up, click
Upgrade
.
Vertex AI Workbench upgrades and starts your instance.
gcloud
To check whether an instance can be upgraded,
use the API method
isUpgradeable
.
This method checks for a newer version of the image on the instance's
boot disk.
gcloud notebooks instances is-upgradeable
INSTANCE_NAME
\
    --location=
LOCATION
Replace the following:
INSTANCE_NAME
: the name of your
instance
LOCATION
: the Google Cloud location where
your instance is located
If the instance is upgradable, the response is
true
. If the response
is
false
, the instance cannot be upgraded, but you can still
try to
migrate your data to a new
instance
.
If your instance isn't running,
start the
instance
.
Vertex AI Workbench can only upgrade instances when
they're running.
Make sure you have
made a copy of the data on your
instance
before
continuing.
If the instance is upgradable, use the API method
upgrade
to upgrade the instance:
gcloud notebooks instances upgrade
INSTANCE_NAME
\
    --location=
LOCATION
Vertex AI Workbench upgrades the instance.
Roll back an upgrade
To roll back an upgrade, complete the following steps:
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Click the instance name that you would like to roll back.
On the
Notebook details
page, under
Upgrade history
,
click
Rollback
.
Vertex AI Workbench rolls your instance back to the previous version.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español – América Latina
Français
Português – Brasil
中文 – 简体
日本語
한국어

# Source: vertex-ai/docs/workbench/user-managed/vpc-standalone.txt
Create a secure user-managed notebooks instance in a VPC network  |  Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Create a secure user-managed notebooks instance in a VPC network
Stay organized with collections
Save and categorize content based on your preferences.
Vertex AI Workbench user-managed notebooks is
deprecated
. On
    April 14, 2025, support for
    user-managed notebooks ended and the ability to create user-managed notebooks instances
    was removed. Existing instances will continue to function until
    March 30, 2026, but patches, updates, and upgrades
    won't be available. To continue using Vertex AI Workbench, we recommend that you
migrate
    your user-managed notebooks instances to Vertex AI Workbench instances
.
This tutorial is intended for enterprise data scientists, researchers, and
network administrators. It shows how to secure a
user-managed notebooks instance by creating it
in a Virtual Private Cloud (VPC) network.
A
VPC network
is a virtual version of
a physical network that is
implemented inside of Google's production network. It is a private network,
with its own private IP addresses, subnets, and network gateways.
In the enterprise, VPC networks are used to protect
data and instances by controlling access to
them from other networks and from the internet.
The VPC network in this tutorial is a standalone network.
However, you can share a VPC network from one project
(called a host project) to other projects in your Google Cloud organization.
To learn more about which type of VPC network to use, see
Single VPC network and Shared VPC
.
Following network security best practices,
the VPC
network in this tutorial uses a combination of
Cloud Router
,
Cloud NAT
, and
Private Google Access
to secure the instance in the following ways:
The user-managed notebooks instance doesn't have an external
IP address.
The instance has outbound internet access through a regional Cloud Router
and Cloud NAT gateway
so that you can install software packages or other dependencies.
Cloud NAT allows outbound connections and the inbound responses to those
connections. It does
not
permit unsolicited inbound requests from the
internet.
The instance uses Private Google Access to reach the external IP addresses
of Google APIs and services.
The tutorial also shows how to do the following:
Create a post-startup script to automatically clone a GitHub repo into the
newly created user-managed notebooks instance.
Use
Cloud Monitoring
to monitor the user-managed notebooks instance.
Use the
Compute Engine
API to start and stop the instance automatically to optimize costs.
Objectives
Create a VPC network and add a subnet that has Private Google Access
enabled.
Create a Cloud Router and Cloud NAT for the VPC network.
Create a user-managed notebooks instance in the subnet, using
a post-startup script that clones the
Google Cloud Generative AI
GitHub repository.
Enable Cloud Monitoring for the instance.
Create a VM instance schedule and attach it to the instance.
Costs
In this document, you use the following billable components of Google Cloud:
Cloud NAT
Compute Engine
Cloud Storage
Vertex AI
Virtual Private Cloud
To generate a cost estimate based on your projected usage,
      use the
pricing calculator
.
New Google Cloud users might be eligible for a
free trial
.
When you finish the tasks that are described in this document, you can avoid
   continued billing by deleting the resources that you created. For more information, see
Clean up
.
Before you begin
In the Google Cloud console, go to the project selector page.
Go to project selector
Select or create a Google Cloud project.
Roles required to select or create a project
Select a project
: Selecting a project doesn't require a specific
      IAM role—you can select any project that you've been
      granted a role on.
Create a project
: To create a project, you need the Project Creator
      (
roles/resourcemanager.projectCreator
), which contains the
resourcemanager.projects.create
permission.
Learn how to grant
      roles
.
Verify that billing is enabled for your Google Cloud project
.
Open
Cloud Shell
to execute the commands listed
    in this tutorial. Cloud Shell is an interactive shell environment
    for Google Cloud that lets you manage your projects and resources from
    your web browser.
Go to Cloud Shell
In the Cloud Shell, set the current project to your
  Google Cloud project ID and store the same
  project ID into the
projectid
shell variable:
projectid="
PROJECT_ID
"
gcloud
config
set
project
${
projectid
}
Replace
PROJECT_ID
with your project ID. If necessary, you can
  locate your project ID in the Google Cloud console. For more information, see
Find your project ID
.
Enable the IAM, Compute Engine, Notebooks, Cloud Storage, and Vertex AI APIs.
Roles required to enable APIs
To enable APIs, you need the Service Usage Admin IAM
          role (
roles/serviceusage.serviceUsageAdmin
), which
          contains the
serviceusage.services.enable
permission.
Learn how to grant
          roles
.
Enable the APIs
Make sure that you have the following role or roles on the project:
          
          roles/compute.networkAdmin,
  roles/compute.securityAdmin, roles/compute.instanceAdmin, roles/notebooks.admin,
  roles/resourcemanager.projectIamAdmin,
  roles/iam.serviceAccountAdmin, roles/iam.serviceAccountUser,
  roles/storage.Admin
Check for the roles
In the Google Cloud console, go to the
IAM
page.
Go to IAM
Select the project.
In the
Principal
column, find all rows that identify you or a group that
                you're included in. To learn which groups you're included in, contact your
                administrator.
For all rows that specify or include you, check the
Role
column to see whether
              the list of roles includes the required roles.
Grant the roles
In the Google Cloud console, go to the
IAM
page.
Go to IAM
Select the project.
Click
person_add
Grant access
.
In the
New principals
field, enter your user identifier.
              
              This is typically the email address for a Google Account.
In the
Select a role
list, select a role.
To grant additional roles, click
add
Add
              another role
and add each additional role.
Click
Save
.
Create and configure a standalone VPC
Create a VPC network named
securevertex-vpc
:
gcloud compute networks create securevertex-vpc --subnet-mode=custom
Create a subnet named
securevertex-subnet-a
, with a primary IPv4 range of
10.10.10.0/29
:
gcloud
compute
networks
subnets
create
securevertex
-
subnet
-
a
--
range
=
10.10.10.0
/
29
--
network
=
securevertex
-
vpc
--
region
=
us
-
central1
--
enable
-
private
-
ip
-
google
-
access
If desired, you can supply a different value for the
--range
parameter.
However, the minimum prefix length for a single notebook is 29.
For more information,
see
IPv4 subnet ranges
.
Create a regional Cloud Router named
cloud-router-us-central1
:
gcloud
compute
routers
create
cloud
-
router
-
us
-
central1
--
network
securevertex
-
vpc
--
region
us
-
central1
Create a regional Cloud NAT gateway named
cloud-nat-us-central1
:
gcloud
compute
routers
nats
create
cloud
-
nat
-
us
-
central1
--
router
=
cloud
-
router
-
us
-
central1
--
auto
-
allocate
-
nat
-
external
-
ips
--
nat
-
all
-
subnet
-
ip
-
ranges
--
region
us
-
central1
Create a Cloud Storage bucket
Create the Cloud Storage bucket:
gcloud storage buckets create --location=us-central1 --uniform-bucket-level-access gs://
BUCKET_NAME
Replace
BUCKET_NAME
with a unique bucket name.
Set the
BUCKET_NAME
shell variable and verify that it was
entered correctly:
BUCKET_NAME=
BUCKET_NAME
echo $BUCKET_NAME
Replace
BUCKET_NAME
with the bucket name.
Create and upload a post-startup script
To create the script, use a text editor such as
vi
or
nano
to create a file named
poststartup.sh
.
Paste the following shell script into the file:
#! /bin/bash
echo
"Current user: id"
>>
/tmp/notebook_config.log
2>&1
echo
"Changing dir to /home/jupyter"
>>
/tmp/notebook_config.log
2>&1
cd
/home/jupyter
echo
"Cloning generative-ai from github"
>>
/tmp/notebook_config.log
2>&1
su
-
jupyter
-c
"git clone https://github.com/GoogleCloudPlatform/generative-ai.git"
>>
/tmp/notebook_config.log
2>&1
echo
"Current user: id"
>>
/tmp/notebook_config.log
2>&1
echo
"Installing python packages"
>>
/tmp/notebook_config.log
2&1
su
-
jupyter
-c
"pip install --upgrade --no-warn-conflicts --no-warn-script-location --user \
google-cloud-bigquery \
google-cloud-pipeline-components \
google-cloud-aiplatform \
seaborn \
kfp"
>>
/tmp/notebook_config.log
2>&1
Save the file.
Upload the file to your Cloud Storage bucket:
gcloud storage cp poststartup.sh gs://$BUCKET_NAME
Create a custom service account
When you create a user-managed notebooks instance, we strongly
recommend that you clear the
Use Compute Engine default service account
checkbox and specify a custom service account. If your organization doesn't
enforce the
iam.automaticIamGrantsForDefaultServiceAccounts
organization policy constraint, the Compute Engine default service account
(and thus anyone you specify as an instance user) is granted the Editor role
(
roles/editor
) on your project. To turn off this behavior,
see
Disable automatic role grants to default service
accounts
.
Create a custom service account named
user-managed-notebook-sa
:
gcloud iam service-accounts create user-managed-notebook-sa \
--display-name="user-managed-notebook-sa"
Assign the Storage Object Viewer IAM role to the service
account:
gcloud projects add-iam-policy-binding $projectid --member="serviceAccount:user-managed-notebook-sa@$projectid.iam.gserviceaccount.com" --role="roles/storage.objectViewer"
Assign the Vertex AI User IAM role to the service
account:
gcloud projects add-iam-policy-binding $projectid --member="serviceAccount:user-managed-notebook-sa@$projectid.iam.gserviceaccount.com" --role="roles/aiplatform.user"
Create a user-managed notebooks instance
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Click
add_box
Create new
,
and then select
Advanced Options
.
The
Create instance
page opens.
On the
Create instance
page,
in the
Details
section, provide the following information for your new
instance and then click
Continue
:
Name
: Provide a name for your new instance, or accept the default.
Region
: Select
us-central1
.
Zone
: Select
us-central1-a
.
In the
Environment
section, provide the following and then click
Continue
:
Post-startup script
: Click
Browse
, double-click the
poststartup.sh
file, click it one more time, and
then click
Select
.
In the
Machine type
section, provide the following and then click
Continue
:
Shielded VM
: Select the following checkboxes:
Secure Boot
Virtual Trusted Platform Module (vTPM)
Integrity monitoring
In the
Disks
section, make sure that
Google-managed encryption key
is selected, and then click
Continue
:
In the
Networking
section, provide the following and then click
Continue
:
Networking
: Select
Network in this project
and complete the
following steps:
In the
Network
field, select
securevertex-vpc
.
In the
Subnetwork
field, select
securevertex-subnet-a
.
Clear the
Assign external IP address
checkbox. Not assigning
an external IP address prevents
the instance from receiving unsolicited communication from the
internet or other VPC networks.
Select the
Allow proxy access
checkbox.
In the
IAM and security
section, provide the following and then click
Continue
:
IAM and security
: To grant a single user access to the instance's
JupyterLab interface, complete the following steps:
Select
Single user
.
In the
User email
field, enter the email address for a single
user account. If you're creating the instance for someone else,
the following conditions apply:
You (the instance creator) don't have access to the instance's
JupyterLab interface. But you still control the instance, and
you can start, stop, or delete it.
After you create the instance, you need to grant the user the
Service Account User role
(
roles/iam.serviceAccountUser
) on the instance's service
account. See
Optional: Grant the Service Account User role to the instance user
.
Clear the
Use Compute Engine default service account
checkbox.
This step is important, because the Compute Engine default
service account (and thus the single user you just specified)
could have the Editor role (
roles/editor
) on your project.
In the
Service account email
field, enter
user-managed-notebook-sa@$projectid.iam.gserviceaccount.com
.
(This is the custom service
account email address that you created earlier.) This service
account has limited permissions.
To learn more about granting access, see
Manage access to a user-managed notebooks instance's JupyterLab interface
.
Security options
: Clear the following checkbox:
Root access to the instance
Select the following checkbox:
nbconvert
nbconvert
lets users export and download a notebook file as a different file
type, such as HTML, PDF, or LaTeX. This setting is required by some of
the notebooks in the
Google Cloud Generative AI
GitHub repo.
Clear the following checkbox:
File downloading
Select the following checkbox, unless you're in a production environment:
Terminal access
This enables terminal access to your instance from within the
JupyterLab user interface.
In the
System health
section, select
Environment auto-upgrade
and
provide the following:
In
Reporting
, select the following checkboxes:
Report system health
Report custom metrics to Cloud Monitoring
Install Cloud Monitoring
Report DNS status for required Google domains
Click
Create
.
Optional: Grant the Service Account User role to the instance user
If you're creating the user-managed notebooks
instance for another user,
you must grant them the
Service Account User role
(
roles/iam.serviceAccountUser
) on the
user-managed-notebook-sa
custom service account as follows:
gcloud
iam
service-accounts
add-iam-policy-binding
\
user-managed-notebook-sa@
PROJECT_ID
.iam.gserviceaccount.com
\
--member
=
"user:
USER_EMAIL
"
\
--role
=
"roles/iam.serviceAccountUser"
Replace the following values:
PROJECT_ID
: the project ID
USER_EMAIL
: the email address for the user
Verify that the user-managed notebooks instance was created
Vertex AI Workbench creates a user-managed notebooks
instance based on your specified properties and automatically starts the
instance.
When the instance is ready to use, Vertex AI Workbench
activates an
Open JupyterLab
link. This link is accessible only to
the single user that you specified at instance creation time.
Open the instance in JupyterLab and verify that the cloned
Google Cloud Generative AI
GitHub repo is present.
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
In the list of user-managed notebooks instances,
click the
Open JupyterLab
link for the instance you created.
In the folder list, you'll see a
generative-ai
folder. This
folder contains the cloned GitHub repo.
Monitor health status through Monitoring
You can monitor the system and application metrics for your
user-managed notebooks instances by using the
Google Cloud console. To learn more about instance monitoring and
about creating custom metrics, see
Monitor health status
.
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Click the name of the user-managed notebooks instance
that you want to view the metrics for.
On the
Notebook details
page, click the
Monitoring
tab. Review
the
CPU Utilization
and
Network Bytes
for your notebook instance.
To learn how to interpret these metrics, see
Review resource metrics
.
If you just created the instance, you won't see any data right away. Wait a
few minutes and refresh the console tab.
Create a VM instance schedule for your user-managed notebooks instance
Because a user-managed notebooks instance
is a Compute Engine VM instance, you can use Compute Engine APIs
to create a VM instance schedule for it.
Use a VM instance schedule to start and stop your
user-managed notebooks instance. During the hours when the
instance is stopped, you pay only for Cloud Storage costs.
You can attach an instance schedule to any VM instance that's
in the same region, so you can use the same instance
schedule to control all your user-managed notebooks instances
in the region.
To learn more about VM instance schedules, see
Scheduling a VM instance to start and stop
.
Create a custom IAM role
As a security best practice, we recommend creating a custom IAM
role that has only the following permissions and
assigning it to the Compute Engine default service account:
compute.instances.start
compute.instances.stop
Inside Cloud Shell, create a custom role named
Vm_Scheduler
and include
the necessary permissions:
Go to Cloud Shell
gcloud iam roles create Vm_Scheduler --project=$projectid \
--title=vm-scheduler-notebooks \
--permissions="compute.instances.start,compute.instances.stop" --stage=ga
Describe the custom role:
gcloud iam roles describe Vm_Scheduler --project=$projectid
Assign the role to the Compute Engine default service account
To give the Compute Engine default service
account permission to start and stop your
user-managed notebooks instances, 
you need to assign the
Vm_Scheduler
custom role to it.
The Compute Engine default service account for your project
has the following
email address:
PROJECT_NUMBER-compute@developer.gserviceaccount.com
,
where
PROJECT_NUMBER
is your project number.
Identify your project number and store it in the
project_number
shell
variable:
project_number=$(gcloud projects describe $projectid --format 'get(projectNumber)')
echo $project_number
Assign the custom role to the default service account:
gcloud
projects
add
-
iam
-
policy
-
binding
$projectid
--
member
=
"serviceAccount:$project_number-compute@developer.gserviceaccount.com"
--
role
=
"projects/$projectid/roles/Vm_Scheduler"
Create and attach the schedule
To create an instance schedule that starts your
user-managed notebooks instance at 7 AM and
stops them at 6 PM:
Create a start and stop schedule named
optimize-notebooks
:
gcloud
compute
resource
-
policies
create
instance
-
schedule
optimize
-
notebooks
\
--
region
=
us
-
central1
\
--
vm
-
start
-
schedule
=
'0
7
*
*
*
'
\
--
vm
-
stop
-
schedule
=
'0
18
*
*
*
'
\
--
timezone
=
TIME_ZONE
Replace
TIME_ZONE
with the location-based
IANA time zone for this instance schedule, for example,
America/Chicago
. If omitted, the default value
UTC
is used. For more information, see
time zone
.
Identify the name of your user-managed notebooks instance
by running the following
command and noting the
NAME
value that it returns:
gcloud compute instances list
Store the name in the
notebook_vm
shell variable:
notebook_vm=
NOTEBOOK_VM_NAME
echo $notebook_vm
Replace
NOTEBOOK_VM_NAME
with your
user-managed notebooks instance name.
Attach the instance schedule to your
user-managed notebooks instance:
gcloud compute instances add-resource-policies $notebook_vm \
  --resource-policies=optimize-notebooks \
  --zone=us-central1-a
Describe the instance schedule:
gcloud
compute
resource
-
policies
describe
optimize
-
notebooks
\
--
region
=
us
-
central1
You can verify if the instance schedule runs successfully by checking the
Compute Engine audit logs
for
the instance schedule resource policy and the attached VM instance.
You might need to wait for up to 15 minutes after the scheduled time
for each operation.
Clean up
To avoid incurring charges to your Google Cloud account for the resources used
in this tutorial, either
delete the project
that contains the resources, or keep the project and delete the individual
resources.
You can delete the individual resources in the project as follows:
In the Google Cloud console, go to the
User-managed notebooks
page.
Go to User-managed notebooks
Select your user-managed notebook instance.
Click
Delete
.
In the Cloud Shell, delete the remaining individual resources by
executing the following commands.
Go to Cloud Shell
gcloud
compute
routers
delete
cloud
-
router
-
us
-
central1
--
region
=
us
-
central1
--
quiet
gcloud
compute
routers
nats
delete
cloud
-
nat
-
us
-
central1
--
region
=
us
-
central1
--
router
=
cloud
-
router
-
us
-
central1
--
quiet
gcloud
compute
instances
remove
-
resource
-
policies
$notebook_vm
\
--
resource
-
policies
=
optimize
-
notebooks
\
--
zone
=
us
-
central1
-
a
--
quiet
gcloud
compute
resource
-
policies
delete
optimize
-
notebooks
--
region
=
us
-
central1
--
quiet
gcloud
compute
instances
delete
$notebook_vm
--
zone
=
us
-
central1
-
a
--
quiet
gcloud
compute
networks
subnets
delete
securevertex
-
subnet
-
a
--
region
=
us
-
central1
--
quiet
gcloud
iam
service
-
accounts
delete
user
-
managed
-
notebook
-
sa
@
$projectid
.
iam.gserviceaccount.com
--
quiet
gcloud
projects
remove
-
iam
-
policy
-
binding
$projectid
--
member
=
"serviceAccount:$project_number-compute@developer.gserviceaccount.com"
--
role
=
"projects/$projectid/roles/Vm_Scheduler"
gcloud
iam
roles
delete
Vm_Scheduler
--
project
=
$projectid
gcloud
compute
networks
delete
securevertex
-
vpc
--
quiet
What's next
Work through the
Vertex AI create a secure user-managed notebook
codelab.
Learn how to
use VPC Service Controls to set up a user-managed notebooks
instance within a service perimeter
.
Learn about
Best practices and reference architectures for VPC design
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
中文 – 繁體
日本語
한국어

# Source: vertex-ai/docs/workbench.txt
Introduction to Vertex AI Workbench  |  Google Cloud Documentation
Skip to main content
Technology areas
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Cross-product tools
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
/
Console
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
日本語
한국어
Sign in
Vertex AI Workbench
Start free
Guides
Reference
Samples
Support
Resources
Technology areas
More
Guides
Reference
Samples
Support
Resources
Cross-product tools
More
Console
Discover
Overview
Introduction to Vertex AI
MLOps on Vertex AI
Interfaces for Vertex AI
Vertex AI beginner's guides
Train an AutoML model
Train a custom model
Get inferences from a custom model
Train a model using Vertex AI and the Python SDK
Introduction
Prerequisites
Create a notebook
Create a dataset
Create a training script
Train a model
Make an inference
Integrated ML frameworks
PyTorch
TensorFlow
Vertex AI for BigQuery users
Glossary
Get started
Set up a project and a development environment
Install the Vertex AI SDK for Python
Choose a training method
Try a tutorial
Tutorials overview
AutoML tutorials
Hello image data
Overview
Set up your project and environment
Create a dataset and import images
Train an AutoML image classification model
Evaluate and analyze model performance
Deploy a model to an endpoint and make an inference
Clean up your project
Hello tabular data
Overview
Set up your project and environment
Create a dataset and train an AutoML classification model
Deploy a model and request an inference
Clean up your project
Custom training tutorials
Train a custom tabular model
Train a TensorFlow Keras image classification model
Overview
Set up your project and environment
Train a custom image classification model
Serve predictions from a custom image classification model
Clean up your project
Fine-tune an image classification model with custom data
Custom training notebook tutorials
Use Generative AI and LLMs
About Generative AI
Use Vertex AI development tools
Development tools overview
Use the Vertex AI SDK
Overview
Introduction to the Vertex AI SDK for Python
Vertex AI SDK for Python classes
Vertex AI SDK classes overview
Data classes
Training classes
Model classes
Prediction classes
Tracking classes
Use Vertex AI in notebooks
Choose a notebook solution
Colab Enterprise
Quickstart:Create a notebook by using the console
Connect to a runtime
Manage runtimes and runtime templates
Create a runtime template
Create a runtime
Vertex AI Workbench
Introduction
Notebook tutorials
Get started
Create an instance by using the Console
Schedule a notebook run
Set up an instance
Create an instance
Create a specific version of an instance
Create an instance with user credential access
Create an instance with Confidential Computing
Add a conda environment
Idle shutdown
Create an instance using a custom container
Create a Dataproc-enabled instance
Create an instance with third party credentials
Manage features through metadata
Use reservations
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Explore and visualize data in BigQuery
Maintain
Manage your conda environment
Back up and restore
Save a notebook to GitHub
Use a snapshot
Use Cloud Storage
Shut down an instance
Upgrade the environment of an instance
Access JupyterLab by using SSH
Migrate data to a new instance
Change machine type and configure GPUs
Provision resources using Terraform
Monitor
Monitor health status
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Use an instance within a service perimeter
Troubleshoot Vertex AI Workbench
Vertex AI Workbench release notes
Managed notebooks
Introduction to managed notebooks
Get started
Create a managed notebooks instance by using the Cloud console
Schedule a managed notebooks run
Set up a managed notebooks instance
Create an instance with a custom container
Run a managed notebooks instance on a Dataproc cluster
Use Dataproc Serverless Spark with managed notebooks
Idle shutdown
Managed notebooks versions
Connect to data
Query data in BigQuery from within JupyterLab
Access Cloud Storage buckets and files in JupyterLab
Explore and visualize data
Overview
Explore and visualize data in BigQuery
Develop a model
Model development in a managed notebooks instance
Deploy
Run notebook files with the executor
Run notebook executions with parameters
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Change machine type and configure GPUs of a managed notebooks instance
Upgrade the environment of a managed notebooks instance
Migrate data to a new managed notebooks instance
Monitor
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Use customer-managed encryption keys
Set up a network
Use a managed notebooks instance within a service perimeter
Troubleshoot managed notebooks
User-managed notebooks
Introduction to user-managed notebooks
Get started
Create a user-managed notebooks instance by using the Cloud console
Set up a user-managed notebooks instance
Create a specific version of an instance
Install dependencies
Choose a virtual machine image
Create an instance with a custom container
Explore data
Data science with R on Google Cloud: Exploratory data analysis tutorial
Monitor
Monitor health status
Audit logging
Control access
Access control
Manage access to an instance
Manage access to an instance's JupyterLab interface
Customer-managed encryption keys
Use a user-managed notebooks instance within a service perimeter
Use a shielded virtual machine with user-managed notebooks
Tutorial: Create a notebooks instance in a VPC network
Maintain
Migrate to a Vertex AI Workbench instance
Save a notebook to GitHub
Back up your data by using a snapshot
Shut down a user-managed notebooks instance
Change machine type and configure GPUs of a user-managed notebooks instance
Upgrade the environment of a user-managed notebooks instance
Migrate data to a new user-managed notebooks instance
Register a legacy instance with Notebooks API
Access JupyterLab by using SSH
Troubleshoot user-managed notebooks
Terraform support for Vertex AI
Managed Training on reserved clusters
Managed Training overview
Get started with Managed Training
Predictive AI model development
Overview
AutoML model development
AutoML training overview
Image data
Classification
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Object detection
Prepare data
Create dataset
Train model
Evaluate model
Get predictions
Interpret results
Encode image data using Base64
Export an AutoML Edge model
Tabular data
Overview
Introduction to tabular data
Tabular Workflows
Overview
Feature engineering
End-to-End AutoML
Overview
Train a model
Get online inferences
Get batch inferences
TabNet
Overview
Train a model
Get online inferences
Get batch inferences
Wide & Deep
Overview
Train a model
Get online inferences
Get batch inferences
Forecasting
Overview
Train a model
Get online inferences
Get batch inferences
Pricing
Service accounts
Manage quotas
Perform classification and regression with AutoML
Overview
Quickstart: AutoML Classification (Cloud Console)
Prepare training data
Create a dataset
Train a model
Evaluate model
View model architecture
Get online inferences
Get batch inferences
Export model
Perform forecasting with AutoML
Overview
Prepare training data
Create a dataset
Train a model
Evaluate model
Get inferences
Hierarchical forecasting
Perform forecasting with ARIMA+
Perform forecasting with Prophet
Perform entity reconciliation
Feature attributions for classification and regression
Feature attributions for forecasting
Data types and transformations for tabular AutoML data
Training parameters for forecasting
Data splits for tabular data
Best practices for creating tabular training data
Forecast with Timeseries Insights
Train an AutoML Edge model
Using the Console
Using the API
AutoML Text
Migrate from AutoML text to Gemini
Gemini for AutoML text users
Vertex AI Training
Overview of custom training in Vertex AI
Load and prepare data
Data preparation overview
Use Cloud Storage as a mounted file system
Mount an NFS share for custom training
Use managed datasets
Prepare training application
Understand the custom training service
Prepare training code
Use prebuilt containers
Create a Python training application for a prebuilt container
Prebuilt containers for custom training
Use custom containers
Custom containers for training
Create a custom container
Containerize and run training code locally
Use Deep Learning VM Images and Containers
Train on a persistent resource
Overview
Create persistent resource
Run training jobs on a persistent resource
Get persistent resource information
Reboot a persistent resource
Delete a persistent resource
Configure training job
Choose a custom training method
Configure container settings for training
Configure compute resources for training
Use reservations with training
Use Spot VMs with training
Submit training job
Create custom jobs
Hyperparameter tuning
Hyperparameter tuning overview
Use hyperparameter tuning
Create training pipelines
Schedule jobs based on resource availability
Use distributed training
Training with Cloud TPU VMs
Use private IP for custom training
Use Private Service Connect interface for training (recommended)
Perform Neural Architecture Search
Overview
Set up environment
Beginner tutorials
Best practices and workflow
Proxy task design
Optimize training speed for PyTorch
Use prebuilt training containers and search spaces
Monitor and debug
Monitor and debug training using an interactive shell
Profile model training performance
Optimize using Vertex AI Vizier
Overview of Vertex AI Vizier
Create Vertex AI Vizier studies
Vertex AI Vizier notebook tutorials
Get inferences
Tutorial: Build a pipeline for continuous training
Create custom organization policy constraints
Ray on Vertex AI
Ray on Vertex AI overview
Set up for Ray on Vertex AI
Create a Ray cluster on Vertex AI
Monitor Ray clusters on Vertex AI
Scale a Ray cluster on Vertex AI
Develop a Ray application on Vertex AI
Run Spark on Ray cluster on Vertex AI
Use Ray on Vertex AI with BigQuery
Deploy a model and get inferences
Delete a Ray cluster
Ray on Vertex AI notebook tutorials
Generative AI model development
Overview
Create and manage datasets
Overview
Data splits for AutoML models
Create an annotation set
Delete an annotation set
Add labels (console)
Export metadata and annotations from a dataset
Manage dataset versions
Use Data Catalog to search for model and dataset resources
Get inferences
Overview
Configure models for inference
Export model artifacts for inference
Prebuilt containers for inference
Custom container requirements for inference
Use a custom container for inference
Use arbitrary custom routes
Use the optimized TensorFlow runtime
Serve inferences with NVIDIA Triton
Custom inference routines
Get online inferences
Create an endpoint
Choose an endpoint type
Create a public endpoint
Use dedicated public endpoints (recommended)
Use dedicated private endpoints based on Private Service Connect (recommended)
Use private services access endpoints
Deploy a model to an endpoint
Overview of model deployment
Compute resources for inference
Deploy a model by using the Google Cloud console
Deploy a model by using the gcloud CLI or Vertex AI API
Use autoscaling for inference
Use a rolling deployment to replace a deployed model
Undeploy a model and delete the endpoint
Use Cloud TPUs for online inference
Use reservations with online inference
Use Flex-start VMs with inference
Use Spot VMs with inference
Get an online inference
View online inference metrics
View endpoint metrics
View DCGM metrics
Share resources across deployments
Use online inference logging
Get batch inferences
Get batch inferences from a custom model
Use reservations with batch inference
Get batch prediction from a self-deployed Model Garden model
Serve generative AI models
Deploy generative AI models
Serve Gemma open models using Cloud TPUs with Saxml
Serve Llama 3 open models using multi-host Cloud TPUs with Saxml
Serve a DeepSeek-V3 model using multi-host GPU deployment
Custom organization policies
Vertex AI inference notebook tutorials
Perform vector similarity searches
Vector Search overview
Try it
Get started
Vector Search quickstart
Before you begin
Notebook tutorials
About hybrid search
Create and manage index
Input data format and structure
Create and manage your index
Storage-optimized indexes
Index configuration parameters
Update and rebuild index
Filter vector matches
Import index data from BigQuery
Embeddings with metadata
Deploy and query an index
Private Service Connect (recommended)
Set up Vector Search with Private Service Connect
Query
JSON Web Token authentication
Public endpoint
Deploy
Query
Private services access
Set up a VPC network peering connection
Deploy
Query
JSON Web Token authentication
Monitor a deployed index
Use custom organization policies
Get support
Machine learning operations (MLOps)
Manage features
Feature management in Vertex AI
Vertex AI Feature Store
About Vertex AI Feature Store
Set up features
Prepare data source
Create a feature group
Create a feature
Set up online serving
Online serving types
Create an online store instance
Create a feature view instance
Control access
Control access to resources
Sync online store
Start a data sync
List sync operations
Update features in a feature view
Serve features
Serve features from online store
Serve historical feature values
Monitor
Monitor features
Manage feature resources
List feature groups
List features
Update a feature group
Update a feature
Delete a feature group
Delete a feature
Manage online store resources
List online stores
List feature views
Update an online store
Update a feature view
Delete an online store
Delete a feature view
Feature metadata
Update labels
Search for resources
Search for resources
Search for resource metadata in Data Catalog
Manage embeddings
Search using embeddings
Notebook tutorials
Vertex AI Feature Store tutorial notebooks
Vertex AI Feature Store (Legacy)
About Vertex AI Feature Store (Legacy)
Data model and resources
Source data requirements
Setup
Best practices
Use Vertex AI Feature Store (Legacy)
Manage featurestores
Manage entity types
Manage and find features
Batch import
Streaming import
Online serving
Fetch training data
Export feature values
Delete feature values
Monitoring
Control access to resources
Manage models
Introduction to Vertex AI Model Registry
Versioning in Model Registry
Import models to Model Registry
Copy models in Model Registry
Delete a model
Integrate with BigQuery ML
Use model aliases
Use model labels
Use Data Catalog to search for model and dataset resources
Evaluate models
Model evaluation in Vertex AI
Perform model evaluation in Vertex AI
Model evaluation for fairness
Introduction to model evaluation for fairness
Data bias metrics for Vertex AI
Model bias metrics for Vertex AI
Model evaluation notebook tutorials
Orchestrate ML workflows using pipelines
Introduction
Interfaces
Configure your project
Build a pipeline
Run a pipeline
Use pipeline templates
Create, upload, and use a pipeline template
Use a prebuilt template from the Template Gallery
Configure your pipeline
Configure execution caching
Configure failure policy
Configure retries for a pipeline task
Specify machine types for a pipeline step
Request Google Cloud machine resources with Vertex AI Pipelines
Configure Private Service Connect interface (recommended)
Configure secrets with Secret Manager
Configure a pipeline run on a persistent resource
Schedule and trigger pipeline runs
Schedule a pipeline run with scheduler API
Trigger a pipeline run with Pub/Sub
Cancel or delete pipeline runs
Cancel pipeline runs
Delete pipeline runs
Rerun a pipeline
Monitor pipeline execution
View pipeline metrics
View pipeline job logs
Route logs to a Cloud Pub/Sub sink
Configure email notifications
Visualize results
Visualize and analyze pipeline results
Track the lineage of pipeline artifacts
Output HTML and Markdown
Resource labeling by Vertex AI Pipelines
Understand pipeline run costs
Migrate from Kubeflow Pipelines to Vertex AI Pipelines
Use custom constraints
Google Cloud Pipeline Components
Quickstart
Introduction to Google Cloud Pipeline Components
Google Cloud Pipeline Component list
Use Google Cloud Pipeline Components
Build your own pipeline components
Vertex AI Pipelines tutorials
Tutorial notebooks
Track and analyze your ML metadata
Introduction to Vertex ML Metadata
Data model and resources
Configure your project's metadata store
Use Vertex ML Metadata
Track Vertex ML Metadata
Analyze Vertex ML Metadata
Manage Vertex ML Metadata
System schemas
Create and use custom schemas
Use custom constraints with metadata stores
Vertex ML Metadata notebook tutorials
Understand model behavior
Introduction to Explainable AI
Configure example-based explanations for custom training
Configure feature-based explanations for custom training
Configure explanations
Configure visualization settings
Improve explanations
Configure feature-based explanations for AutoML image classification
Configure visualization settings
Improve explanations
Use TensorFlow for explanations
Get explanations
Limitations of Explainable AI
Explainable AI notebook tutorials
Monitor model quality
Introduction to Model Monitoring
Model Monitoring v2
Set up model monitoring
Run monitoring jobs
Manage model monitors
Model Monitoring v1
Provide schemas to Model Monitoring
Monitor feature skew and drift
Monitor feature attribution skew and drift
Model Monitoring for batch predictions
Track Experiments
Introduction to Vertex AI Experiments
Set up for Vertex AI Experiments
Create an experiment
Create and manage experiment runs
Log data
Autolog data to an experiment run
Manually log data to an experiment run
Log models to an experiment run
Track executions and artifacts
Add pipeline run to experiment
Run training job with experiment tracking
Compare and analyze runs
Use Vertex AI TensorBoard
Introduction to Vertex AI TensorBoard
Set up Vertex AI TensorBoard
Configure training script
Use Vertex AI TensorBoard with custom training
Use Vertex AI TensorBoard with Vertex AI Pipelines
Manually log TensorBoard data
Upload existing logs
View Vertex AI TensorBoard
Notebook tutorials
Get started with Vertex AI Experiments
Compare pipeline runs
Model training
Compare models
Autologging
Custom training autologging
Track parameters and metrics for custom training
Delete outdated Vertex AI TensorBoard experiments
Vertex AI TensorBoard custom training with custom container
Vertex AI TensorBoard custom training with prebuilt container
Vertex AI TensorBoard hyperparameter tuning with HParams dashboard
Profile model training performance using Cloud Profiler
Profile model training performance using Cloud Profiler in custom training with prebuilt container
Vertex AI TensorBoard integration with Vertex AI Pipelines
Administer
Access control
Access control with IAM
IAM permissions
Set up a project for a team
Control access to Vertex AI endpoints
Use a custom service account
Use customer-managed encryption keys
Access Transparency
Monitor Vertex AI resources
Cloud Monitoring metrics
Audit logging information
Networking
Networking access overview
Accessing the Vertex AI API
Accessing Vertex AI services through private services access
Accessing Vertex AI services through PSC endpoints
Accessing Vertex AI services through PSC interfaces
Set up VPC Network Peering
Set up connectivity to other networks
Set up a Private Service Connect interface
Tutorial: Access training pipelines privately from on-premises
Tutorial: Access a Vector Search index privately from on-premises
Tutorial: Access the Generative AI API from on-premises
Tutorial: Access batch predictions privately from on-premises
Tutorial: Create a Vertex AI Workbench instance in a VPC network
Security
VPC Service Controls
Allow public endpoint access to protected resources from outside a perimeter
Allow multicloud access to protected resources from outside a perimeter
Allow access to protected resources from inside a perimeter
Name resources
Samples and tutorials
Notebook tutorials
Code samples
All Vertex AI code samples
Code samples for all products
AI and ML
Application development
Application hosting
Compute
Data analytics and pipelines
Databases
Distributed, hybrid, and multicloud
Generative AI
Industry solutions
Networking
Observability and monitoring
Security
Storage
Access and resources management
Costs and usage management
Infrastructure as code
Migration
SDK, languages, frameworks, and tools
Home
Documentation
AI and ML
Vertex AI Workbench
Guides
Send feedback
Stay organized with collections
Save and categorize content based on your preferences.
Introduction to Vertex AI Workbench
Vertex AI Workbench instances are Jupyter notebook-based development environments
for the entire data science workflow. You can interact with
Vertex AI and other Google Cloud services from within
a Vertex AI Workbench instance's Jupyter notebook.
Vertex AI Workbench integrations and features can make it easier
to access your data, process data faster, schedule notebook runs, and more.
Vertex AI Workbench instances are prepackaged with
JupyterLab
and have a preinstalled suite of deep learning packages,
including support for the TensorFlow and PyTorch
frameworks. You can configure either CPU-only or GPU-enabled instances.
Vertex AI Workbench instances support the ability to sync with a
GitHub
repository.
Vertex AI Workbench instances are protected
by Google Cloud authentication and authorization.
Access to data
You can access your data without leaving the JupyterLab user interface.
In JupyterLab's navigation menu on a Vertex AI Workbench instance,
you can use the Cloud Storage integration
to browse data and other files that you have access to.
See
Access Cloud Storage buckets and files
from within JupyterLab
.
You can also use the BigQuery integration
to browse tables that you have access to, write queries, preview results,
and load data into your notebook. See
Query data in BigQuery
tables from within JupyterLab
.
Execute notebook runs
Use the executor to run a notebook file as a one-time execution or
on a schedule. Choose the specific environment and hardware that you want
your execution to run on. Your notebook's code will run on
Vertex AI custom training, which can make it easier
to do distributed training, optimize hyperparameters, or
schedule continuous training jobs.
You can use parameters in your execution to make specific changes to each run.
For example, you might specify a different dataset to use,
change the learning rate on your model, or change the version of the model.
You can also set a notebook to run on a recurring schedule. Even while your
instance is shut down, Vertex AI Workbench will run your notebook file and
save the results for you to look at and share with others. See
Schedule a notebook run
.
Share insights
Executed notebook runs are stored in a Cloud Storage bucket,
so you can share your insights with others by granting access
to the results. See the
previous section on executing
notebook runs
.
Secure your instance
The following sections describe supported capabilities that can help you
secure your Vertex AI Workbench instance.
VPC
You can deploy your Vertex AI Workbench instance
with the default Google-managed network,
which uses a default VPC network and subnet.
Instead of the default network, you can specify a
VPC network to use with your instance.
To use Vertex AI Workbench within a service perimeter, see
Use a Vertex AI Workbench instance within a service
perimeter
.
Customer-managed encryption keys (CMEK)
By default, Google Cloud automatically
encrypts data when it is at
rest
using encryption keys
managed by Google. If you have specific compliance or regulatory requirements
related to the keys that protect your data, you can use customer-managed
encryption keys (CMEK) with your Vertex AI Workbench instances.
For more information,
see
Customer-managed encryption keys
.
Confidential Computing
Preview
This feature is
        
        subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the
Service Specific
        Terms
.
        
        Pre-GA features are available "as is" and might have limited support.
      
      For more information, see the
launch stage descriptions
.
You can encrypt your data-in-use by using Confidential Computing. To use
Confidential Computing, you enable the Confidential VM service
when you create a Vertex AI Workbench instance. To get started,
see
Create an instance with
Confidential Computing
.
Automated shutdown for idle instances
To help manage costs, Vertex AI Workbench instances shut down after
being idle for a specific time period by default. You can change the
amount of time or turn this feature off. For more information,
see
Idle shutdown
.
Add conda environments
Vertex AI Workbench instances use
kernels
based on conda environments. You can add a conda environment to
your Vertex AI Workbench instance, and the environment appears as
a kernel in your instance's JupyterLab interface.
Adding conda environments lets you use kernels that aren't available in the
default Vertex AI Workbench instance.
For example, you can add conda environments for R and Apache Beam. Or you
can add conda environments for specific earlier versions of the available
frameworks, such as TensorFlow, PyTorch, or Python.
For more information, see
Add a conda environment
.
Custom containers
You can create a Vertex AI Workbench instance based on a custom container.
Start with a Google-provided base container image, and modify it for
your needs. Then create an instance based on your custom container.
For more information, see
Create an instance using a
custom container
.
Dataproc integration
You can process data quickly by running a notebook on
a Dataproc cluster. After your cluster is set up, you can run
a notebook file on it without leaving the JupyterLab user interface.
For more information, see
Create a Dataproc-enabled
instance
.
Reserve VM resources
Use
Compute Engine reservations
to gain a high level of assurance that your Vertex AI Workbench instances
have enough virtual machine (VM) resources to run.
Reservations are a Compute Engine feature. They help make sure that
you have the resources available to create VMs with the same hardware
(memory and vCPUs) and optional resources (GPUs and Local SSD disks)
whenever you need them.
For more information, see
Use reservations
.
Create instances with third party credentials
You can create and manage Vertex AI Workbench instances with
third party credentials provided by Workforce Identity Federation.
Workforce Identity Federation uses your external identity provider (IdP)
to grant a group of users access to Vertex AI Workbench instances
through a proxy.
Access to a Vertex AI Workbench instance is granted by assigning a
workforce pool principal
to the Vertex AI Workbench instance's service account.
For more information, see
Create an instance with
third party credentials
.
Tags for Vertex AI Workbench instances
The underlying VM of a Vertex AI Workbench instance is a
Compute Engine VM. You can add and manage resource tags to your
Vertex AI Workbench instance through its Compute Engine VM.
When you create a Vertex AI Workbench instance,
Vertex AI Workbench attaches the Compute Engine resource tag
vertex-workbench-instances:prod=READ_ONLY
. This resource tag is
only used for internal purposes.
To learn more about managing tags for Compute Engine instances,
see
Manage tags for resources
.
Limitations
Consider the following limitations of
Vertex AI Workbench instances when planning your project:
Third party JupyterLab extensions aren't supported.
When you use
Access Context Manager
and
Chrome Enterprise Premium
to protect Vertex AI Workbench instances with context-aware
access controls, access is evaluated each time the user authenticates
to the instance. For example, access is evaluated the first time
the user accesses JupyterLab and whenever they access it thereafter
if their web browser's cookie has expired.
Using a custom container that isn't derived from the
Google-provided base container
(
gcr.io/deeplearning-platform-release/workbench-container:latest
)
increases the risks of compatibility issues with our services and
isn't supported. Instead, modify the base container to create a
custom container that meets your needs, and then
create an instance using
the custom container
.
Vertex AI Workbench instances expect images from the
cloud-notebooks-managed
project. The list of image names is available at
the creation page in the Google Cloud console. Although the use of custom
virtual machine (VM) images or
Deep Learning VM
images with Vertex AI Workbench instances can be possible,
Vertex AI Workbench doesn't provide any support for unexpected
behaviors or malfunctions when using those images.
The use of a user-managed notebooks image or
managed notebooks image to create a
Vertex AI Workbench instance isn't supported.
You can't edit the underlying VM of a Vertex AI Workbench instance
by using the Google Cloud console or the Compute Engine API. To edit a
Vertex AI Workbench instance's underlying VM, use the
projects.locations.instances.patch
method in the Notebooks API or the
gcloud workbench instances update
command in the Google Cloud SDK.
In instances that use VPC Service Controls, use of the
executor
isn't
supported.
To use accelerators with Vertex AI Workbench instances,
the accelerator type that you want must be available in your instance's
zone. To learn about accelerator availability by zone, see
GPU regions and zones availability
.
What's next
Create a Vertex AI Workbench instance
.
Compare Vertex AI's
notebook solutions
.
Send feedback
Except as otherwise noted, the content of this page is licensed under the
Creative Commons Attribution 4.0 License
, and code samples are licensed under the
Apache 2.0 License
. For details, see the
Google Developers Site Policies
. Java is a registered trademark of Oracle and/or its affiliates.
Last updated 2025-10-28 UTC.
Products and pricing
See all products
Google Cloud pricing
Google Cloud Marketplace
Contact sales
Support
Google Cloud Community
Support
Release Notes
System status
Resources
GitHub
Getting Started with Google Cloud
Code samples
Cloud Architecture Center
Training and Certification
Engage
Blog
Events
X (Twitter)
Google Cloud on YouTube
Google Cloud Tech on YouTube
About Google
Privacy
Site terms
Google Cloud terms
Manage cookies
Our third decade of climate action: join us
Sign up for the Google Cloud newsletter
Subscribe
English
Deutsch
Español
Español – América Latina
Français
Indonesia
Italiano
Português
Português – Brasil
中文 – 简体
日本語
한국어

